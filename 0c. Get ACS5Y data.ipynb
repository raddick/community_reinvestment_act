{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install xlrd\n",
    "debug = 2\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import geopandas\n",
    "import os\n",
    "import re\n",
    "from shutil import copyfile\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "\n",
    "output_basedir = '/home/idies/workspace/Temporary/raddick/cra_scratch_final/'\n",
    "acs5_dir = output_basedir + 'acs5/'\n",
    "outdir = acs5_dir\n",
    "\n",
    "census_basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/acs5/'\n",
    "\n",
    "extrasdir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "baltimore_dir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/baltimore/'\n",
    "\n",
    "g = 0\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined variables we need in 0.00 seconds!\n"
     ]
    }
   ],
   "source": [
    "# IV: population by race; owner-occupied units; MFI; hs grad pct (age 25 and older); \n",
    "### female hoh pct; unemployment pct (age 18 and older); poverty pct; median home value;\n",
    "### median home year built\n",
    "s = time.time()\n",
    "\n",
    "varlist = ['STUSAB','LOGRECNO']    # Lookup variables (we always need these): Seq 1 in 2017\n",
    "varlist += ['B01001_001']   # Total population: Seq 2 in 2017\n",
    "varlist += ['B11001_001']  # Total households: Seq 36 in 2017\n",
    "varlist += ['B02001_002', 'B02001_003'] # White-only and black-only population: Seq 4 in 2017\n",
    "\n",
    "# WHITE-ONLY AND BLACK-ONLY HOUSEHOLDER\n",
    "varlist += ['B11001A_001', 'B11001B_001']\n",
    "\n",
    "varlist += ['B25003_002'] # Owner-occupied housing units: Seq 103 in 2017\n",
    "varlist += ['B19113_001'] # Median family income: Seq 64 in 2017\n",
    "\n",
    "# EDUCATION HIGH SCHOOL OR ABOVE\n",
    "varlist += ['B15002_011', 'B15002_012', 'B15002_013', 'B15002_014', 'B15002_015']\n",
    "varlist += ['B15002_016', 'B15002_017', 'B15002_018']\n",
    "varlist += ['B15002_028', 'B15002_029', 'B15002_030', 'B15002_031', 'B15002_032']\n",
    "varlist += ['B15002_033', 'B15002_034', 'B15002_035']\n",
    "\n",
    "varlist += ['B11001_006'] # Female householder no husband present: Seq 36 in 2017\n",
    "\n",
    "varlist += ['B23025_005'] # Unemployed people age 16 or over: Seq 79 in 2017\n",
    "varlist += ['B17001_002'] # Income in the past 12 months below poverty level: Seq 48 in 2017\n",
    "varlist += ['B25077_001'] # MEDIAN VALUE (DOLLARS)% Owner-occupied housing units%Median value (dollars): Seq 106 in 2017\n",
    "varlist += ['B25035_001'] # MEDIAN YEAR STRUCTURE BUILT% Housing units%Median year structure built: Seq 105 in 2017\n",
    "\n",
    "possible_ind_vars = varlist\n",
    "# ANOTHER OPTION: UNEMPLOYED PEOPLE AGES 22 AND OLDER: Seq 76 in 2017\n",
    "#varlist += metadata_df['variable'][\n",
    "#    (metadata_df['description'].apply(lambda x: 'unemplo' in x.lower()))\n",
    "#    & (metadata_df['description'].apply(lambda x: x[0:31] == 'SEX BY AGE BY EMPLOYMENT STATUS'))\n",
    "#    & (metadata_df['variable'].apply(lambda x: re.search(\"[A-Z]_\", x) == None))  # note: case-sensitive search for varnames\n",
    "#    #& (metadata_df['sequence_number'].apply(lambda x: ((x >= 76) & (x <= 76))))    \n",
    "#].tolist()\n",
    "\n",
    "# Cross-tab variables to come back to later\n",
    "#varlist += ['B11001A_006', 'B11001B_006', 'B17001A_002', 'B17001B_002']\n",
    "#varlist += ['C15002A_004', 'C15002A_009', 'C15002B_004', 'C15002B_009']\n",
    "#varlist += ['B19113A_001', 'B19113B_001', 'B25003A_002', 'B25003B_002']\n",
    "\n",
    "# CHECK!\n",
    "#metadata_df[metadata_df['variable'].apply(lambda x: x in varlist)]\n",
    "### need to calculate percentages: population 25 and older...\n",
    "##### population age 16 and older in labor force, population for whom poverty status determined\n",
    "comparison_vars = []\n",
    "# POPULATION AGE 25 OR OVER\n",
    "comparison_vars += ['B01001_011', 'B01001_012', 'B01001_013', 'B01001_014', 'B01001_015']\n",
    "comparison_vars += ['B01001_016', 'B01001_017', 'B01001_018', 'B01001_019', 'B01001_020']\n",
    "comparison_vars += ['B01001_021', 'B01001_022', 'B01001_023', 'B01001_024', 'B01001_025']\n",
    "comparison_vars += ['B01001_035', 'B01001_036', 'B01001_037', 'B01001_038', 'B01001_039']\n",
    "comparison_vars += ['B01001_040', 'B01001_041', 'B01001_042', 'B01001_043', 'B01001_044']\n",
    "comparison_vars += ['B01001_045', 'B01001_046', 'B01001_047', 'B01001_048', 'B01001_049']\n",
    "\n",
    "comparison_vars += ['B11001_002', 'B11001_007'] # Family and nonfamily households\n",
    "\n",
    "#if (thisyear != 2010):\n",
    "comparison_vars += ['B23025_002'] # EMPLOYMENT STATUS FOR THE POPULATION 16 YEARS AND OVER%Population 16 years and over%Total%In labor force: Seq 79 in 2017\n",
    "comparison_vars += ['B17001_001'] # POVERTY STATUS IN THE PAST 12 MONTHS BY SEX BY AGE% Population for whom poverty status is determined%Total: Seq 48 in 2017\n",
    "\n",
    "# CHECK!\n",
    "#metadata_df[metadata_df['variable'].apply(lambda x: x in comparison_vars)]\n",
    "\n",
    "if (len(varlist) < len(possible_ind_vars) + len(comparison_vars)):\n",
    "    varlist += comparison_vars\n",
    "\n",
    "# CHECK!\n",
    "#metadata_df[metadata_df['variable'].apply(lambda x: x in varlist)]\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Defined variables we need in {0:,.2f} seconds!'.format(e-s))\n",
    "#for x in varlist:\n",
    "#    print(metadata_df['variable'][metadata_df['variable'] == x].values[0])#, ',\"',\n",
    "#          metadata_df['description'][metadata_df['variable'] == x].values[0],'\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 2010...\n",
      "Parsed 2010 metadata in 8 seconds!\n",
      "ut: Wrote 2,278 geographies and 130 variables in 1 minutes 34 seconds!\n",
      "va: Wrote 7,239 geographies and 130 variables in 3 minutes 50 seconds!\n",
      "vt: Wrote 706 geographies and 130 variables in 0 minutes 54 seconds!\n",
      "wa: Wrote 6,241 geographies and 130 variables in 3 minutes 21 seconds!\n",
      "wi: Wrote 5,898 geographies and 130 variables in 3 minutes 25 seconds!\n",
      "wv: Wrote 2,076 geographies and 130 variables in 1 minutes 43 seconds!\n",
      "wy: Wrote 542 geographies and 130 variables in 0 minutes 53 seconds!\n",
      "\n",
      "\n",
      "Grand total: 15 minutes 48 seconds!\n"
     ]
    }
   ],
   "source": [
    "g = 0\n",
    "state_codes_df = pandas.read_csv(extrasdir+'statecodes.csv')\n",
    "state_codes_df = state_codes_df.set_index('STUSAB')\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = sorted([x.lower() for x in states if x not in ('MD', 'AS', 'GU', 'MP', 'PR', 'UM', 'VI')])\n",
    "#states = states[-7:]\n",
    "#states = ['md']\n",
    "\n",
    "#years = np.arange(2017, 2009, -1)\n",
    "years = np.arange(2010, 2009, -1)\n",
    "\n",
    "for thisyear in years:\n",
    "    s = time.time()\n",
    "    print('Parsing {0:.0f}...'.format(thisyear))    \n",
    "    metaoutfile = outdir + 'acs5_metadata_{0:.0f}.csv'.format(thisyear)\n",
    "    census_yeardir = census_basedir + str(thisyear) + '/'\n",
    "    census_geodir = census_yeardir + 'geography/'\n",
    "    \n",
    "    if (thisyear == 2017):\n",
    "        census_metadir = census_yeardir + 'metadata/xls_temp/'\n",
    "    elif (thisyear == 2016):\n",
    "        census_metadir = census_yeardir + 'metadata/templates/'\n",
    "    elif (thisyear == 2014):\n",
    "        census_metadir = census_yeardir + 'metadata/seq/'\n",
    "    else:\n",
    "        census_metadir = census_yeardir + 'metadata/'\n",
    "    if (thisyear == 2017):\n",
    "        census_rawdatadir = census_yeardir + 'rawdata/'\n",
    "    elif (thisyear == 2016):\n",
    "        census_rawdatadir = census_yeardir + 'rawdata/data/tab4/sumfile/prod/{0:.0f}thru{1:.0f}/group2/'.format(thisyear-4, thisyear)\n",
    "    elif (thisyear == 2014 or thisyear == 2012 or thisyear == 2010):\n",
    "        census_rawdatadir = census_yeardir + 'rawdata/tab4/sumfile/prod/{0:.0f}thru{1:.0f}/group2/'.format(thisyear-4, thisyear)\n",
    "    else:\n",
    "        census_rawdatadir = census_yeardir + 'rawdata/group2/'.format(thisyear-4, thisyear)\n",
    "\n",
    "    themetafiles = [x for x in os.listdir(census_metadir) if 'seq' in x.lower()]\n",
    "    lastfilenum = len(themetafiles)\n",
    "    metadata_df = pandas.DataFrame()\n",
    "\n",
    "    i = 1\n",
    "    if (thisyear == 2017):\n",
    "        this_seq_metadata_cols_df = pandas.read_excel(census_metadir+'seq{0:.0f}.xlsx'.format(i))\n",
    "    elif (thisyear == 2011):\n",
    "        this_seq_metadata_cols_df = pandas.read_excel(census_metadir+'Seq{0:04d}.xls'.format(i))\n",
    "    else:\n",
    "        this_seq_metadata_cols_df = pandas.read_excel(census_metadir+'Seq{0:.0f}.xls'.format(i))\n",
    "    this_seq_metadata_df = this_seq_metadata_cols_df.T\n",
    "    this_seq_metadata_df.index.name = 'variable'\n",
    "    this_seq_metadata_df = this_seq_metadata_df.rename(columns={0: 'description'})\n",
    "    this_seq_metadata_df = this_seq_metadata_df.assign(sequence_number = i)\n",
    "    this_seq_metadata_df = this_seq_metadata_df.reset_index()\n",
    "    this_seq_metadata_df.index.name = 'varnum'\n",
    "    #metadata_df = pandas.concat((metadata_df, this_seq_metadata_df))\n",
    "\n",
    "    for i in range(1,lastfilenum+1):\n",
    "        #if ((np.mod(i,50) == 0) | (i == lastfilenum)):\n",
    "#            print('Parsing metadata for sequence {0:.0f}...'.format(i))\n",
    "        if (thisyear == 2017):\n",
    "            this_seq_metadata_cols_df = pandas.read_excel(census_metadir+'seq{0:.0f}.xlsx'.format(i))\n",
    "        elif (thisyear == 2011):\n",
    "            this_seq_metadata_cols_df = pandas.read_excel(census_metadir+'Seq{0:04d}.xls'.format(i))\n",
    "        else:\n",
    "            this_seq_metadata_cols_df = pandas.read_excel(census_metadir+'Seq{0:.0f}.xls'.format(i))\n",
    "        this_seq_metadata_df = this_seq_metadata_cols_df.T\n",
    "        this_seq_metadata_df = this_seq_metadata_df.assign(sequence_number = i)\n",
    "        this_seq_metadata_df = this_seq_metadata_df.drop(['FILEID', 'FILETYPE', 'STUSAB', 'CHARITER', 'SEQUENCE', 'LOGRECNO'], axis=0)\n",
    "        this_seq_metadata_df.index.name = 'variable'\n",
    "        this_seq_metadata_df = this_seq_metadata_df.rename(columns={0: 'description'})\n",
    "        this_seq_metadata_df = this_seq_metadata_df.reset_index()\n",
    "        metadata_df = pandas.concat((metadata_df, this_seq_metadata_df))\n",
    "    metadata_df = metadata_df.reset_index(drop=True)\n",
    "    metadata_df.index.name = 'varnum'\n",
    "     \n",
    "    #print('writing out metadata...')\n",
    "    #metadata_df.to_csv(metaoutfile, encoding='utf-8')\n",
    "    \n",
    "    e = time.time()\n",
    "    g = g + (e-s)\n",
    "    print('Parsed {0:.0f} metadata in {1:,.0f} seconds!'.format(thisyear, e-s))\n",
    "\n",
    "    #print('Reading {0:.0f} estimates and margins of error...'.format(thisyear))\n",
    "    \n",
    "    for onestate in states:\n",
    "        s = time.time()\n",
    "        outfile = outdir + 'acs5_tracts_for_cra_{0:.0f}{1:}.csv'.format(thisyear, onestate)\n",
    "        \n",
    "        #print('Parsing estimates and margins of error for {0:}...'.format(onestate))\n",
    "        estimates_df = pandas.DataFrame()\n",
    "        margins_of_error_df = pandas.DataFrame()\n",
    "        for i in range(1, lastfilenum+1):\n",
    "            #if ((i == 1) | (np.mod(i,25) == 0) | (i == lastfilenum)):\n",
    "#                print('Processing sequence {0:,.0f}...'.format(i))\n",
    "            this_seq_estimates_df = pandas.DataFrame()\n",
    "            this_seq_margins_of_error_df = pandas.DataFrame()\n",
    "            estimates_statefilename = census_rawdatadir + 'e{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i)        \n",
    "            margins_of_error_statefilename = census_rawdatadir + 'm{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i)\n",
    "            this_seq_estimates_df = pandas.read_csv(estimates_statefilename, header=None, sep=',', encoding='utf-8', low_memory=False)\n",
    "            this_seq_margins_of_error_df = pandas.read_csv(margins_of_error_statefilename, header=None, sep=',', encoding='utf-8', low_memory=False)            \n",
    "            column_names = metadata_df[metadata_df['sequence_number'] == i].set_index('variable').T.columns.tolist()\n",
    "            column_names = ['FILEID','FILETYPE','STUSAB','CHARITER','SEQUENCE','LOGRECNO'] + column_names\n",
    "\n",
    "            this_seq_estimates_df.columns = column_names\n",
    "            this_seq_margins_of_error_df.columns = column_names\n",
    "            if (i == 1):\n",
    "                estimates_df = this_seq_estimates_df\n",
    "                margins_of_error_df = this_seq_margins_of_error_df\n",
    "            else:\n",
    "                this_seq_estimates_df = this_seq_estimates_df.drop(['FILEID', 'FILETYPE', 'CHARITER', 'SEQUENCE'], axis=1)\n",
    "                this_seq_margins_of_error_df = this_seq_margins_of_error_df.drop(['FILEID', 'FILETYPE', 'CHARITER', 'SEQUENCE'], axis=1)\n",
    "                estimates_df = estimates_df.merge(this_seq_estimates_df, how='left', on=['STUSAB','LOGRECNO'], sort=False)\n",
    "                margins_of_error_df = margins_of_error_df.merge(this_seq_margins_of_error_df, how='left', on=['STUSAB','LOGRECNO'], sort=False)\n",
    "        \n",
    "        # e = time.time()\n",
    "        #print('Parsed {0:.0f} data for {1:} in {2:.0f} seconds...'.format(thisyear, onestate, e-s))\n",
    "        # s = time.time()\n",
    "        #print('Getting only the variables we need...')\n",
    "        if (thisyear == 2010):\n",
    "            estimates_df = estimates_df.assign(B23025_002 = np.nan)\n",
    "            estimates_df = estimates_df.assign(B23025_005 = np.nan)\n",
    "            margins_of_error_df = margins_of_error_df.assign(B23025_002 = np.nan)\n",
    "            margins_of_error_df = margins_of_error_df.assign(B23025_005 = np.nan)\n",
    "        #print('Originally {0:,.0f} variables in estimates and {1:,.0f} in margins of error...'.format(estimates_df.shape[1], margins_of_error_df.shape[1]))\n",
    "        estimates_df = estimates_df[varlist]\n",
    "        margins_of_error_df = margins_of_error_df[varlist]\n",
    "        #print('Now {0:,.0f} variables in estimates and {1:,.0f} in margins of error...'.format(estimates_df.shape[1], margins_of_error_df.shape[1]))\n",
    "        \n",
    "        estimates_df_bk = estimates_df\n",
    "        margins_of_error_df_bk = margins_of_error_df\n",
    "        #print('Joining estimates and margins of error...')\n",
    "        for x in margins_of_error_df.columns[2:]:\n",
    "            margins_of_error_df = margins_of_error_df.rename(columns={x: x+'_err'})\n",
    "        acs5_df = estimates_df.merge(margins_of_error_df, how='left', on=['STUSAB', 'LOGRECNO'])\n",
    "\n",
    "        #margins_of_error_df = margins_of_error_df_bk\n",
    "        #error_columns = ['LOGRECNO'] + [x+'_err' for x in margins_of_error_df.columns[6:]]\n",
    "\n",
    "        #print('Joining geography...')\n",
    "        if (thisyear >= 2016):\n",
    "            filename = census_geodir+'{0:}.xlsx'.format(onestate)\n",
    "        else:\n",
    "            filename = census_geodir+'{0:}.xls'.format(onestate)\n",
    "        geo_df = pandas.read_excel(filename)\n",
    "        if (thisyear == 2017):\n",
    "            geo_df = geo_df.assign(STUSAB = geo_df['State'].apply(lambda x: x.lower()))\n",
    "        else:\n",
    "            geo_df = geo_df.assign(STUSAB = geo_df['STATE'].apply(lambda x: x.lower()))\n",
    "        geo_df = geo_df.rename(columns={'Geography ID': 'GEOID'})\n",
    "#if (debug >= 1):\n",
    "#    print('Retaining only tract-level geographies...')\n",
    "##    print('Retaining only tract- and block-group-level geographies...')\n",
    "#geo_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '140'))]\n",
    "##geo_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '140')) | (geo_df['GEOID'].apply(lambda x: x[0:3] == '150'))]\n",
    "\n",
    "##geo_df = geo_df.set_index('GEOID')  #We'll set GEOID as index colum AFTER the merge\n",
    "        if (thisyear == 2017):\n",
    "            acs5_df = acs5_df.merge(geo_df, how='inner', left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "        else:\n",
    "            acs5_df = acs5_df.merge(geo_df, how='inner', left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "        acs5_df = acs5_df.set_index('GEOID')\n",
    "\n",
    "        #print('Fixing column names for easy joining later...')\n",
    "        if (thisyear == 2017):\n",
    "            acs5_df = acs5_df.drop('Logical Record Number', axis=1)\n",
    "        if (acs5_df.columns[-2] != 'STATE'):\n",
    "            acs5_df = acs5_df.rename(columns={acs5_df.columns[-2]: 'STATE'})\n",
    "        if (acs5_df.columns[-1] != 'Geography Name'):\n",
    "            acs5_df = acs5_df.rename(columns={acs5_df.columns[-1]: 'Geography Name'})\n",
    "\n",
    "        #print('Writing out data...')\n",
    "        acs5_df.to_csv(outfile, encoding='utf-8')\n",
    "        metadata_df[metadata_df['variable'].apply(lambda x: x in varlist)].to_csv(metaoutfile, encoding='utf-8')\n",
    "\n",
    "        #print('\\nData written out to:\\n{0:}\\nMetadata written out to:\\n{1:}'.format(outfile, metaoutfile))\n",
    "\n",
    "        e = time.time()\n",
    "        g = g + (e-s)\n",
    "\n",
    "        print('{0:}: Wrote {1:,.0f} geographies and {2:,.0f} variables in {3:,.0f} minutes {4:.0f} seconds!'.format(onestate, acs5_df.shape[0], acs5_df.shape[1], np.floor((e-s)/60), (e-s) % 60))\n",
    "        #print('\\n')\n",
    "        \n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "#            \n",
    "#            this_seq_estimates_df = this_seq_estimates_df.append(estimates_onestate_df, sort=False)\n",
    "#            this_seq_margins_of_error_df = this_seq_margins_of_error_df.append(margins_of_error_onestate_df, sort=False)\n",
    "\n",
    "\n",
    "#    for this_state in states:\n",
    "#        print('{0:}{1:.0f}'.format(this_state, thisyear))\n",
    "\n",
    "#print('backing up...')\n",
    "#metadata_df_bk = metadata_df\n",
    "\n",
    "print('Grand total: {0:,.0f} minutes {1:,.0f} seconds!'.format(np.floor(g/60), g%60))\n",
    "#metadata_df[varlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out metadata files for Baltimore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metafiles = [x for x in os.listdir(acs5_dir) if ('metadata' in x)]\n",
    "for thismeta in metafiles:\n",
    "    copyfile(acs5_dir+thismeta, baltimore_dir+'acs5_metadata/'+thismeta)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Baltimore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "md_acs5_df = pandas.DataFrame()\n",
    "datafiles = [x for x in os.listdir(acs5_dir) if (('tracts' in x) and ('md' in x))]\n",
    "for thisfile in datafiles:\n",
    "    xdf = pandas.read_csv(acs5_dir+thisfile, encoding='utf-8', low_memory=False)\n",
    "    xdf = xdf.assign(year = int(thisfile[-10:-6]))\n",
    "    md_acs5_df = md_acs5_df.append(xdf)\n",
    "    \n",
    "column_order = md_acs5_df.columns[0:1].tolist()\n",
    "column_order += md_acs5_df.columns[-1:].tolist()\n",
    "column_order += md_acs5_df.columns[1:-1].tolist()\n",
    "#column_order += md_acs5_df.columns[-3:].tolist()\n",
    "\n",
    "md_acs5_df[column_order][md_acs5_df['GEOID'].apply(lambda x: 'US24510' in x)].to_csv(baltimore_dir+'acs5_2010_2017.csv', encoding='utf-8')\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQUENCE GUIDE FOR ACS 5-yr in 2017\n",
    "\n",
    "Seq 1: unweighted count\n",
    "\n",
    "Seq 2-3: age-sex\n",
    "\n",
    "Seq 4: race\n",
    "\n",
    "Seq 5: hispanic origin\n",
    "\n",
    "Seq 6-7: ancestry\n",
    "\n",
    "Seq 8-12: foreign birth\n",
    "\n",
    "Seq 13-15: Place of Birth - Native\n",
    "\n",
    "Seq 16-22: Residence Last Year - Migration\n",
    "\n",
    "Seq 23-33: Journey to Work\n",
    "\n",
    "Seq 34: Children - Relationship\n",
    "\n",
    "Seq 35: Grand(Persons) - Age of HH Members\n",
    "\n",
    "Seq 36-37: Households - Families\n",
    "\n",
    "Seq 38-39: Marital Status\n",
    "\n",
    "Seq 40: Fertility\n",
    "\n",
    "Seq 41-42: School Enrollment\n",
    "\n",
    "Seq 43-44: Educational Attainment\n",
    "\n",
    "Seq 45-47: Language\n",
    "\n",
    "Seq 48-56: Poverty\n",
    "\n",
    "Seq 57-58: Disability\n",
    "\n",
    "Seq 59-66: Income\n",
    "\n",
    "Seq 67-72: Earnings\n",
    "\n",
    "Seq 73-74: Veteran Status\n",
    "\n",
    "Seq 75: Transfer Programs\n",
    "\n",
    "Seq 76-79: Employment Status\n",
    "\n",
    "Seq 80-102: Industry-Occupation-Class of Worker\n",
    "\n",
    "Seq 103-112: Housing\n",
    "\n",
    "Seq 113-123: Group Quarters\n",
    "\n",
    "Seq 124-127: Health Insurance\n",
    "\n",
    "Seq 128 Computer and Internet Usage\n",
    "\n",
    "Seq 129: Quality Measures\n",
    "\n",
    "Seq 130-133: Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = pandas.read_csv('/home/idies/workspace/Temporary/raddick/cra_scratch_final/acs5/acs5_tracts_for_cra_2017md.csv', encoding='utf-8', low_memory=False, index_col='GEOID')\n",
    "#z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thisyear = 2017\n",
    "s = time.time()\n",
    "\n",
    "basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/acs5/'\n",
    "yeardir = basedir + str(thisyear) + '/'\n",
    "if (thisyear == 2017):\n",
    "    rawdatadir = yeardir + 'rawdata/'\n",
    "    metadir = yeardir + 'metadata/xls_temp/'\n",
    "elif (thisyear == 2016):\n",
    "    rawdatadir = yeardir + 'rawdata/data/tab4/sumfile/prod/2012thru2016/group2/'\n",
    "    metadir = yeardir + 'metadata/templates/'\n",
    "elif (thisyear == 2015):\n",
    "    rawdatadir = yeardir + 'rawdata/group2/'\n",
    "    metadir = yeardir + 'metadata/'\n",
    "elif (thisyear == 2014):\n",
    "    rawdatadir = yeardir + 'rawdata/tab4/sumfile/prod/2010thru2014/group2/'\n",
    "    metadir = yeardir + 'metadata/seq/'\n",
    "elif (thisyear == 2013):\n",
    "    rawdatadir = yeardir + 'rawdata/group2/'\n",
    "    metadir = yeardir + 'metadata/'\n",
    "elif (thisyear == 2012):\n",
    "    rawdatadir = yeardir + 'rawdata/tab4/sumfile/prod/2008thru2012/group2/'\n",
    "    metadir = yeardir + 'metadata/'\n",
    "elif (thisyear == 2011):\n",
    "    rawdatadir = yeardir + 'rawdata/group2/'\n",
    "    metadir = yeardir + 'metadata/'\n",
    "elif (thisyear == 2010):\n",
    "    rawdatadir = yeardir + 'rawdata/tab4/sumfile/prod/2006thru2010/group2/'\n",
    "    metadir = yeardir + 'metadata/'\n",
    "    \n",
    "estimates_dir = yeardir + 'estimates/'\n",
    "margin_of_error_dir = yeardir + 'margin_of_error/'\n",
    "vardir = yeardir + 'variables/'\n",
    "\n",
    "geodir = yeardir + 'geography/'\n",
    "\n",
    "extrasdir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "\n",
    "outdir = acs5_dir\n",
    "#outdir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/datasets/acs5/'\n",
    "\n",
    "for thisdir in [estimates_dir, margin_of_error_dir, vardir, geodir]:#[datadir, errordir, vardir, geodir]:\n",
    "    #print(thisdir)\n",
    "    if not(os.path.exists(thisdir)):\n",
    "        os.makedirs(thisdir)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Done in {0:,.0f} seconds!'.format(e-s))\n",
    "#os.chdir(rawdatadir)\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
