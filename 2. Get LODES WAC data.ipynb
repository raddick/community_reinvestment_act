{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "#import io\n",
    "data_dir = '/home/idies/workspace/Temporary/raddick/cra_scratch_final/'\n",
    "jobs_dir = data_dir + 'lodes_wac/'\n",
    "baltimore_dir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/baltimore/'\n",
    "g = 0\n",
    "print('Done!')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.002 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "y = os.listdir(data_dir)\n",
    "hasdir = [x for x in y if x=='lodes_wac']\n",
    "if (len(hasdir) == 0):\n",
    "    os.mkdir(jobs_dir)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done in {0:,.3f} seconds!'.format(e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get WAC data from census.gov and combine into one file per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping data...\n",
      "Scraping from: https://lehd.ces.census.gov/data/lodes/LODES7/vt/wac/\n",
      "Parsing state 57 of 60... Getting 760 files...\n",
      "Scraped data to create 760 files in 2 minutes 55 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "this_state = 'vt'\n",
    "\n",
    "theurl = 'https://lehd.ces.census.gov/data/lodes/LODES7/'\n",
    "print('scraping data...')\n",
    "page = request.urlopen(theurl)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "page.close()\n",
    "\n",
    "bigtable = soup.find('table')\n",
    "allstaterows = bigtable.findAll('tr')\n",
    "\n",
    "#for i in range(8,9):# Alaska: downloaded 761 files in 2 minutes 46 seconds!\n",
    "#for i in range(9,10):# Alabama: downloaded 2,162 files in 14 minutes 23 seconds!\n",
    "#for i in range(10,11):# Arkansas: 128 minutes 4 seconds!\n",
    "#for i in range(15,16): # DC Wrote 715,448 rows to jobs_data_dc.csv in 2 minutes 59 seconds!\n",
    "#for i in range(11,12):# Arizona: Wrote 13,889,448 rows to jobs_data_az.csv in 146 minutes 10 seconds!\n",
    "\n",
    "#for i in range(12,13): # California 1: Done: wrote 61,672,840 rows to jobs_data_ca_1.csv in 140 minutes 45 seconds!\n",
    "#for i in range(12,13): # California 2: Done: Done: wrote 42,640,110 rows to jobs_data_ca.csv in 295 minutes 15 seconds!\n",
    "\n",
    "#for i in range(13,14):# Colorado: Wrote 19,412,406 rows to jobs_data_co.csv in 229 minutes 1 seconds!\n",
    "#for i in range(14,15):# Connecticut: Done: wrote 10,771,624 rows to jobs_data_ct.csv in 133 minutes 53 seconds!\n",
    "#for i in range(16,17): # Delaware: Done: wrote 2,789,900 rows to jobs_data_de.csv in 37 minutes 39 seconds!\n",
    "\n",
    "#for i in range(17,18):  # Florida 1:  wrote 32,549,290 rows to jobs_data_fl.csv in 104 minutes 32 seconds!\n",
    "#for i in range(17,18):  # Florida 2:  wrote 23,955,264 rows to jobs_data_fl.csv in 145 minutes 49 seconds!\n",
    "\n",
    "\n",
    "#for i in range(18,19):  # Georgia: wrote 27,619,956 rows to jobs_data_ga.csv in 197 minutes 40 seconds!\n",
    "#for i in range(19,20): # Hawaii: wrote 2,718,016 rows to jobs_data_hi.csv in 37 minutes 48 seconds!\n",
    "#for i in range(20,21): # Iowa: Done: wrote 15,456,674 rows to jobs_data_ia.csv in 219 minutes 49 seconds!\n",
    "#for i in range(21,22): # Idaho: Done: wrote 7,900,456 rows to jobs_data_id.csv in 112 minutes 48 seconds!\n",
    "\n",
    "#for i in range(22,23):  # Illinois 1: wrote 24,616,876 rows to jobs_data_il.csv in 66 minutes 20 seconds!\n",
    "#for i in range(22,23):  # Illinois 2: Wrote 17,587,534 rows to jobs_data_il.csv in 93 minutes 59 seconds!\n",
    "#for i in range(23,24):  # Indiana: wrote 23,527,492 rows to jobs_data_in.csv in 310 minutes 28 seconds!\n",
    "    \n",
    "#for i in range(24,25):  # Kansas: wrote 12,459,210 rows to jobs_data_ks.csv in 161 minutes 28 seconds!\n",
    "#for i in range(25,26):  # Kentucky:  wrote 13,852,412 rows to jobs_data_ky.csv in 177 minutes 22 seconds!\n",
    "#for i in range(26,27):   # Louisiana: wrote 16,495,578 rows to jobs_data_la.csv in 249 minutes 30 seconds!\n",
    "#for i in range(27,28):  # Massachusetts: wrote 9,156,418 rows to jobs_data_ma.csv in 36 minutes 47 seconds!\n",
    "#for i in range(28,29):  # Maryland: Wrote 18,121,632 rows to jobs_data_md.csv in 127 minutes 37 seconds!\n",
    "#for i in range(29,30):  # Maine:wrote 5,778,216 rows to jobs_data_me.csv in 79 minutes 36 seconds!\n",
    "#for i in range(30,31):  # Michigan: Wrote 28,618,778 rows to jobs_data_mi.csv in 350 minutes 26 seconds!\n",
    "#for i in range(31,32):   # Minnesota: wrote 19,976,072 rows to jobs_data_mn.csv in 144 minutes 39 seconds!\n",
    "#for i in range(32,33):  # Missouri: Wrote 24,473,990 rows to jobs_data_mo.csv in 301 minutes 16 seconds!\n",
    "#for i in range(33,34):  # Mississippi: Done: wrote 8,900,694 rows to jobs_data_ms.csv in 107 minutes 59 seconds!\n",
    "#for i in range(34,35): #  Montana: wrote 5,890,396 rows to jobs_data_mt.csv in 84 minutes 10 seconds!\n",
    "#for i in range(35,36):  # North Carolina\n",
    "#for i in range(36,37):  # North Dakota: Done: wrote 4,821,752 rows to jobs_data_nd.csv in 62 minutes 17 seconds!\n",
    "#for i in range(37,38):  # Nebraska: Done: wrote 10,367,456 rows to jobs_data_ne.csv in 128 minutes 51 seconds!\n",
    "#for i in range(38,39):   # New Hampshire: Done: wrote 5,050,412 rows to jobs_data_nh.csv in 61 minutes 53 seconds\n",
    "#for i in range(39,40):   # New Jersey: wrote 26,288,036 rows to jobs_data_nj.csv in 210 minutes 53 seconds!\n",
    "#for i in range(40,41):   # New Mexico: Wrote 6,851,906 rows to jobs_data_nm.csv in 85 minutes 46 seconds!\n",
    "#for i in range(41,42):  # Nevada: wrote 5,781,806 rows to jobs_data_nv.csv in 75 minutes 6 seconds!\n",
    "\n",
    "#for i in range(42,43):   # New York 1: wrote 28,180,428 rows to jobs_data_ny.csv in 83 minutes 16 seconds\n",
    "#for i in range(42,43):    # New York 2: wrote 20,749,578 rows to jobs_data_ny.csv in 127 minutes 36 seconds!\n",
    "\n",
    "#for i in range(43,44):  # Ohio: Wrote 35,651,764 rows to jobs_data_oh.csv in 431 minutes 40 seconds!\n",
    "#for i in range(44,45):    # Oklahoma: Done: wrote 13,922,816 rows to jobs_data_ok.csv in 181 minutes 52 seconds!\n",
    "#for i in range(45,46):   # Oregon: wrote 16,777,870 rows to jobs_data_or.csv in 211 minutes 15 seconds!\n",
    "#for i in range(46,47):  # Pennsylvania: Wrote 46,173,402 rows to jobs_data_pa.csv in 681 minutes 46 seconds!\n",
    "# 47 is Puerto Rico, skipping\n",
    "#for i in range(48,49):    # Rhode Island: wrote 4,053,362 rows to jobs_data_ri.csv in 52 minutes 13 seconds!\n",
    "#for i in range(49,50):   # South Carolina: wrote 15,517,110 rows to jobs_data_sc.csv in 348 minutes 47 seconds!\n",
    "#for i in range(50,51):     # South Dakota: Wrote 4,129,246 rows to jobs_data_sd.csv in 66 minutes 57 seconds!\n",
    "#for i in range(51,52): # Tennessee: wrote 19,170,058 rows to jobs_data_tn.csv in 237 minutes 48 seconds!\n",
    "\n",
    "#for i in range(52,53):  # Texas 1: wrote 42,962,025 rows to jobs_data_tx.csv in 113 minutes 48 seconds!\n",
    "#for i in range(52,53):    # Texas 2: wrote 31,434,722 rows to jobs_data_tx.csv in 103 minutes 56 seconds!\n",
    "# 53 is U.S., skipping\n",
    "\n",
    "#for i in range(54,55):  # Utah: wrote 9,261,700 rows to jobs_data_ut.csv in 115 minutes 51 seconds!\n",
    "#for i in range(57,58):  # Vermont: wrote 3,022,422 rows to jobs_data_vt.csv in 39 minutes 39 seconds!\n",
    "\n",
    "#for i in range(55,56):  # Virginia 1: wrote 15,585,370 rows to jobs_data_va.csv in 31 minutes 8 seconds!\n",
    "#for i in range(55,56):  # Virginia 2: wrote 11,482,902 rows to jobs_data_va.csv in 81 minutes 1 seconds!\n",
    "\n",
    "for i in range(57,58):  # Vermont: \n",
    "#for i in range(58,59):    # Washington (State):  wrote 24,854,106 rows to jobs_data_wa.csv in 301 minutes 7 seconds!\n",
    "\n",
    "#for i in range(59,60):     # Wisconsin: Wrote 19,565,890 rows to jobs_data_wi.csv in 271 minutes 2 seconds!\n",
    "\n",
    "#for i in range(60,61):   # West Virginia: wrote 7,111,648 rows to jobs_data_wv.csv in 87 minutes 52 seconds!\n",
    "#for i in range(61,62):   # Wyoming: wrote 3,668,138 rows to jobs_data_wy.csv in 47 minutes 1 seconds!\n",
    "    \n",
    "    allstatecells = allstaterows[i].findAll('td')\n",
    "    this_state_url = theurl + allstatecells[1].find('a').get('href')+'wac/'\n",
    "    print('Scraping from: {0:}'.format(this_state_url))\n",
    "    #print(this_state_url)\n",
    "    statepage = request.urlopen(this_state_url)\n",
    "    statesoup = BeautifulSoup(statepage, 'html.parser')\n",
    "    statepage.close()\n",
    "    statetable = statesoup.find('table')\n",
    "    filerows = statetable.findAll('tr')\n",
    "    print('Parsing state {0:,.0f} of {1:,.0f}... Getting {2:,.0f} files...'.format(i, 60, len(filerows)-4))\n",
    "    for j in range(3, len(filerows)-1):\n",
    "        filecells = filerows[j].findAll('td')\n",
    "        this_file_url = this_state_url + filecells[1].find('a').get('href')\n",
    "        this_file_name = jobs_dir + filecells[1].text\n",
    "        with open(this_file_name, 'wb') as f:\n",
    "            r = request.urlopen(this_file_url)\n",
    "            f.write(r.read())\n",
    "            r.close()\n",
    "nFiles = len([x for x in os.listdir(jobs_dir) if '.gz' in x])\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Scraped data to create {0:,.0f} files in {1:,.0f} minutes {2:,.0f} seconds!'.format(nFiles, np.floor((e-s)/60), (e-s)%60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping files...\n",
      "Found 760 gzip files...\n",
      "\tUn-gzipping file 0 of 759...\n",
      "\tUn-gzipping file 100 of 759...\n",
      "\tUn-gzipping file 200 of 759...\n",
      "\tUn-gzipping file 300 of 759...\n",
      "\tUn-gzipping file 400 of 759...\n",
      "\tUn-gzipping file 500 of 759...\n",
      "\tUn-gzipping file 600 of 759...\n",
      "\tUn-gzipping file 700 of 759...\n",
      "\tUn-gzipping file 759 of 759...\n",
      "Files unzipped in 20 seconds...\n",
      "Deleting zipfiles...\n",
      "Deleted all .gz files in 3 seconds...\n",
      "Done in 198 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "\n",
    "print('Unzipping files...')\n",
    "gzipfiles = [jobs_dir+x for x in os.listdir(jobs_dir) if ('{0:}_wac'.format(this_state) in x) and (x[-3:] == '.gz')]\n",
    "print('Found {0:,.0f} gzip files...'.format(len(gzipfiles)))\n",
    "\n",
    "for i in range(0, len(gzipfiles)):\n",
    "    if ((np.mod(i, 100) == 0) or (i == len(gzipfiles) - 1)):\n",
    "        print('\\tUn-gzipping file {0:.0f} of {1:.0f}...'.format(i, len(gzipfiles)-1))\n",
    "    csvfilename = gzipfiles[i][:-3]\n",
    "    with gzip.open(gzipfiles[i], 'rb') as f:\n",
    "        file_content = f.read()\n",
    "    with open(csvfilename, 'wb') as f:\n",
    "        f.write(file_content)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Files unzipped in {0:,.0f} seconds...'.format(e-s))\n",
    "\n",
    "s = time.time()\n",
    "print('Deleting zipfiles...')\n",
    "#os.listdir(jobs_dir)\n",
    "gzfiles = [jobs_dir+x for x in os.listdir(jobs_dir) if x[-3:] == '.gz']\n",
    "for y in gzfiles:\n",
    "    os.remove(y)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Deleted all .gz files in {0:,.0f} seconds...'.format(e-s))\n",
    "print('Done in {0:,.0f} seconds!'.format(g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining multiple files into a single CSV for vt...\n",
      "\tImporting file 400 of 759...\n",
      "\tImporting file 425 of 759...\n",
      "\tImporting file 450 of 759...\n",
      "\tImporting file 475 of 759...\n",
      "\tImporting file 500 of 759...\n",
      "\tImporting file 525 of 759...\n",
      "\tImporting file 550 of 759...\n",
      "\tImporting file 575 of 759...\n",
      "\tImporting file 600 of 759...\n",
      "\tImporting file 625 of 759...\n",
      "\tImporting file 650 of 759...\n",
      "\tImporting file 675 of 759...\n",
      "\tImporting file 700 of 759...\n",
      "\tImporting file 725 of 759...\n",
      "\tImporting file 750 of 759...\n",
      "\tImporting file 759 of 759...\n",
      "writing outfile...\n",
      "Wrote 1,289,090 rows to jobs_data_vt.csv in 7 minutes 11 seconds!\n",
      "\n",
      "Deleting component files that we combined...\n",
      "Found 58 state jobs files!\n",
      "Will delete 760 files...\n",
      "Deleting...\n",
      "Deleted files in 0 minutes 3 seconds!\n",
      "\n",
      "Done: wrote 1,289,090 rows to jobs_data_vt.csv in 10 minutes 32 seconds!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "print('Combining multiple files into a single CSV for {0:}...'.format(this_state))\n",
    "jobs_df = pandas.DataFrame()\n",
    "csvfiles = [jobs_dir+x for x in os.listdir(jobs_dir) if ('{0:}_wac'.format(this_state) in x) and (x[-4:] == '.csv')]\n",
    "\n",
    "#startfile = 0\n",
    "startfile = int(np.floor(len(csvfiles)/2))\n",
    "\n",
    "if (this_state in ['ca',  'fl', 'il', 'ny', 'tx', 'va']):\n",
    "    #last_to_read = int(np.floor(len(csvfiles)/2))+1\n",
    "    last_to_read = int(len(csvfiles))\n",
    "    print('reading {0:} from {1:,.0f} to {2:,.0f}'.format(this_state,startfile,last_to_read))\n",
    "else:\n",
    "    last_to_read = len(csvfiles)\n",
    "\n",
    "for i in range(startfile, last_to_read):\n",
    "    if ((np.mod(i, 25) == 0) or (i == len(csvfiles) - 1)):\n",
    "        print('\\tImporting file {0:,.0f} of {1:,.0f}...'.format(i, last_to_read-1))\n",
    "    thisyear = int(csvfiles[i][-8:-4])\n",
    "    #print(thisyear)\n",
    "    xdf = pandas.read_csv(csvfiles[i], low_memory=False)\n",
    "    xdf = xdf.assign(year = thisyear)     \n",
    "    jobs_df = jobs_df.append(xdf)\n",
    "jobs_df = jobs_df.reset_index(drop=True)\n",
    "jobs_df.index.name = 'rownumber'\n",
    "#print(jobs_df.shape)\n",
    "\n",
    "print('writing outfile...')\n",
    "\n",
    "if (this_state in ['ca','fl', 'il', 'ny', 'tx', 'va']):\n",
    "    #jobs_df.to_csv(jobs_dir+'jobs_data_{0:}_1.csv'.format(this_state), encoding='utf-8')\n",
    "    jobs_df.to_csv(jobs_dir+'jobs_data_{0:}_2.csv'.format(this_state), encoding='utf-8')\n",
    "else:\n",
    "    jobs_df.to_csv(jobs_dir+'jobs_data_{0:}.csv'.format(this_state), encoding='utf-8')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Wrote {0:,.0f} rows to jobs_data_{1:}.csv in {2:,.0f} minutes {3:,.0f} seconds!\\n'.format(len(jobs_df), this_state, np.floor((e-s)/60), (e-s) % 60))\n",
    "\n",
    "\n",
    "print('Deleting component files that we combined...')\n",
    "s = time.time()\n",
    "jobs_files_list = sorted([x[-6:-4] for x in os.listdir(jobs_dir) if 'jobs_' in x])\n",
    "print('Found {0:.0f} state jobs files!'.format(len(jobs_files_list)))\n",
    "#pprint(jobs_files_list)\n",
    "files_to_delete = sorted([jobs_dir+y for y in os.listdir(jobs_dir) if ('.csv' in y) and ('jobs_' not in y)])\n",
    "print('Will delete {0:,.0f} files...'.format(len(files_to_delete)))\n",
    "print('Deleting...')\n",
    "for z in files_to_delete:\n",
    "    os.remove(z)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Deleted files in {0:,.0f} minutes {1:,.0f} seconds!\\n'.format(np.floor((e-s)/60), (e-s) % 60))\n",
    "\n",
    "print('Done: wrote {0:,.0f} rows to jobs_data_{1:}.csv in {2:,.0f} minutes {3:,.0f} seconds!\\n'.format(len(jobs_df), this_state, np.floor((g)/60), (g) % 60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pandas.read_csv(jobs_dir+'jobs_data_ga.csv')\n",
    "# df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Baltimore data and write it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = time.time()\n",
    "df = pandas.read_csv(jobs_dir+'jobs_data_{0:}.csv'.format(this_state), encoding='utf-8', low_memory=False, index_col='rownumber')\n",
    "#df = pandas.read_csv(jobs_dir+'jobs_data_{0:}.csv'.format(this_state), encoding='utf-8', low_memory=False, index_col='rownumber')\n",
    "#baltimore_df = df[df['w_geocode'].apply(lambda x: str(x)[0:5] == '24510')]\n",
    "#baltimore_df.to_csv(baltimore_dir+'wac_jobs_df.csv', encoding='utf-8')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Read {0:,.0f} rows in {1:,.0f} seconds!'.format(len(df), e-s))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "s = time.time()\n",
    "jobs_meta_df = pandas.read_csv(data_dir+'lodes_wac_codebook.csv', encoding='utf-8', low_memory=False, index_col='varnum')\n",
    "jobs_meta_df.to_csv(baltimore_dir+'wac_jobs_metadata.csv', encoding='utf-8')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Wrote metadata for {0:,.0f} variables in {1:,.0f} seconds!'.format(len(jobs_meta_df), e-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
