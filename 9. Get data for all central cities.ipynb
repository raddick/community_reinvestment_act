{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n",
      "Now in directory: /home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "debug = 2\n",
    "g = 0\n",
    "\n",
    "thisyear = 2018\n",
    "\n",
    "print('Importing packages...')\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import time\n",
    "#import zipfile\n",
    "import geopandas\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Point\n",
    "\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "# Directories to look in\n",
    "thisdir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/'\n",
    "data_dir = '/home/idies/workspace/Temporary/raddick/cra_scratch_final/'\n",
    "jobs_dir = data_dir + 'lodes_wac/'\n",
    "\n",
    "#census_dir = data_dir + 'acs5/'\n",
    "\n",
    "output_data_dir = thisdir + 'final_data/'\n",
    "#baltimore_dir = thisdir + 'baltimore/'\n",
    "\n",
    "census_shapefile_tiger_basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/shapefiles/'\n",
    "cities_tracts_dir = '/home/idies/workspace/Temporary/raddick/census_scratch/cbsa_central_cities/v2/'\n",
    "\n",
    "\n",
    "\n",
    "shapefile_dir = census_shapefile_tiger_basedir + '{0:.0f}/TRACT/'.format(thisyear)\n",
    "\n",
    "acs5_basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/acs5/'\n",
    "\n",
    "code_lookup_dir = thisdir + 'code_guide_lookups/'\n",
    "inflation_dir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/datasets/inflation/'\n",
    "extrasdir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "\n",
    "#city_data_dir = thisdir + 'city_data/'\n",
    "scale = 1\n",
    "\n",
    "\n",
    "os.chdir(thisdir)\n",
    "g = 0 # global time\n",
    "\n",
    "print('Now in directory: {0:}'.format(os.getcwd()))\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "years = np.arange(2018, 2009, -1)\n",
    "\n",
    "filenames_df = pandas.DataFrame(data=[x for x in os.listdir(cities_tracts_dir) if ('central_city_tracts' in x) and (x[-4:] == '.shp')], columns=['filename'])\n",
    "filenames_df = filenames_df.assign(state_number = filenames_df['filename'].apply(lambda x: int(x[-6:-4])))\n",
    "filenames_df = filenames_df.sort_values(by = 'state_number')\n",
    "filenames_df = filenames_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "all_tracts_gdf = geopandas.GeoDataFrame()\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('reading central city tract shapefiles state by state...')\n",
    "\n",
    "for ix, thisrow in filenames_df.iterrows():\n",
    "    if (debug >= 2):\n",
    "        print('reading state number {0:.0f}...'.format(thisrow['state_number']))\n",
    "    this_state_gdf = geopandas.read_file(cities_tracts_dir+thisrow['filename'])\n",
    "    all_tracts_gdf = pandas.concat((all_tracts_gdf, this_state_gdf), axis=0, sort=False)\n",
    "\n",
    "column_names_df = pandas.read_csv(cities_tracts_dir+'column_names.csv')\n",
    "\n",
    "for i in range(0, len(all_tracts_gdf.columns)):\n",
    "    all_tracts_gdf = all_tracts_gdf.rename(columns = {all_tracts_gdf.columns[i]: column_names_df.columns[i]})\n",
    "\n",
    "print('fixing GEOIDs and setting them as index...')\n",
    "all_tracts_gdf.loc[:, 'GEOID'] = all_tracts_gdf['GEOID'].apply(lambda x: '14000US'+x)\n",
    "\n",
    "all_tracts_gdf = all_tracts_gdf.set_index('GEOID')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Read {0:,.0f} census tracts in {1:,.0f} seconds!'.format(len(all_tracts_gdf), e-s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get geo-aggregated loans for all census tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "print('reading nationwide data...')\n",
    "data_df = pandas.read_csv(data_dir+'agg_loans.csv', encoding='utf-8', low_memory=False, index_col='rownumber')\n",
    "\n",
    "print('Read {0:,.0f} tract-years...'.format(len(data_df)))\n",
    "\n",
    "# Keep only business loans\n",
    "print('keeping only business loans...')\n",
    "data_df = data_df[data_df['loan_type'] == 4]\n",
    "print('\\tKept {0:,.0f} tract-years...'.format(len(data_df)))\n",
    "\n",
    "# Keep only loan originations\n",
    "print('keeping only originated loans...')\n",
    "data_df = data_df[data_df['action_taken_type'] == 1]\n",
    "print('\\tKept {0:,.0f} tract-years...'.format(len(data_df)))\n",
    "\n",
    "print('keeping only loans 2010 or later...')\n",
    "data_df = data_df[data_df['activity_year'] >= 2010]\n",
    "print('\\tKept {0:,.0f} tract-years...'.format(len(data_df)))\n",
    "\n",
    "print('creating GEOIDs in loans dataframe...')\n",
    "data_df = data_df.assign(GEOID = data_df.apply(lambda row: '14000US{0:02d}{1:03d}{2:06d}'.format(row['state'], row['county'], int(100*row['census_tract'])), axis=1))\n",
    "\n",
    "print('keeping only GEOIDs in center city tracts...')\n",
    "center_city_tract_geoid_list = all_tracts_gdf.index.drop_duplicates().sort_values().tolist()\n",
    "data_df = data_df[data_df['GEOID'].isin(center_city_tract_geoid_list)]\n",
    "\n",
    "# print('backing up...')\n",
    "# data_df_bk = data_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Kept {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(data_df), e-s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get total loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "s = time.time()\n",
    "\n",
    "print('converting columns to numeric...')\n",
    "numeric_columns = []\n",
    "numeric_columns += ['nLoans1', 'amtLoans1', 'nLoans100k', 'amtLoans100k']\n",
    "numeric_columns += ['nLoans250k', 'amtLoans250k', 'nLoansToSmallest', 'amtLoansToSmallest']\n",
    "\n",
    "print('calculating total loans...')\n",
    "data_df = data_df.assign(nLoans = data_df['nLoans1'] + data_df['nLoans100k'] + data_df['nLoans250k'])\n",
    "data_df = data_df.assign(amtLoans = data_df['amtLoans1'] + data_df['amtLoans100k'] + data_df['amtLoans250k'])\n",
    "\n",
    "numeric_columns += ['nLoans', 'amtLoans']\n",
    "for thiscol in numeric_columns:\n",
    "    data_df.loc[:, thiscol] = data_df[thiscol].fillna(0)\n",
    "\n",
    "# print('re-ordering columns to move geometry to the end...')\n",
    "# new_column_order = [x for x in data_df.columns.tolist() if x != 'geometry'] + ['geometry']\n",
    "# data_df = data_df[new_column_order]\n",
    "\n",
    "print('setting index...')\n",
    "data_df = data_df.set_index(['GEOID', 'activity_year'])\n",
    "\n",
    "# print('backing up...')\n",
    "# data_df_bk = data_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Got total loans for {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(data_df), e-s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add income groups, CRA levels, working loans for each tract-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "\n",
    "print('looking up income group names from income_group_total...')\n",
    "\n",
    "data_df = data_df.rename(columns = {'income_group_total': 'income_group_code'})\n",
    "data_df = data_df.assign(income_group = np.nan)\n",
    "\n",
    "data_df.loc[data_df['income_group_code'] == 1, 'income_group'] = '< 10% of Median Family Income (MFI)'\n",
    "data_df.loc[data_df['income_group_code'] == 2, 'income_group'] = '10% to 20% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 3, 'income_group'] = '20% to 30% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 4, 'income_group'] = '30% to 40% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 5, 'income_group'] = '40% to 50% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 6, 'income_group'] = '50% to 60% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 7, 'income_group'] = '60% to 70% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 8, 'income_group'] = '70% to 80% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 9, 'income_group'] = '80% to 90% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 10, 'income_group'] = '90% to 100% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 11, 'income_group'] = '100% to 110% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 12, 'income_group'] = '110% to 120% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 13, 'income_group'] = '> 120% of MFI'\n",
    "data_df.loc[data_df['income_group_code'] == 14, 'income_group'] = 'unknown'\n",
    "\n",
    "print('Adding CRA income levels (low/moderate/middle/upper/unknown)...')\n",
    "# Get levels (low, moderate, middle, upper)\n",
    "data_df = data_df.assign(cra_level = np.nan)\n",
    "data_df.loc[(data_df['income_group_code'] >= 1) & (data_df['income_group_code'] <= 5), 'cra_level'] = 'low'\n",
    "data_df.loc[(data_df['income_group_code'] >= 6) & (data_df['income_group_code'] <= 8), 'cra_level'] = 'moderate'\n",
    "data_df.loc[(data_df['income_group_code'] >= 9) & (data_df['income_group_code'] <= 12), 'cra_level'] = 'middle'\n",
    "data_df.loc[(data_df['income_group_code'] == 13), 'cra_level'] = 'upper'\n",
    "data_df.loc[(data_df['income_group_code'] == 14), 'cra_level'] = 'unknown'\n",
    "\n",
    "print('Getting CRA income levels for tracts where only CRA level was reported...')\n",
    "data_df.loc[data_df['income_group_code'] == 101, 'cra_level'] = 'low'\n",
    "data_df.loc[data_df['income_group_code'] == 102, 'cra_level'] = 'moderate'\n",
    "data_df.loc[data_df['income_group_code'] == 103, 'cra_level'] = 'middle'\n",
    "data_df.loc[data_df['income_group_code'] == 104, 'cra_level'] = 'upper'\n",
    "data_df.loc[data_df['income_group_code'] == 105, 'cra_level'] = 'unknown'\n",
    "\n",
    "print('calculating working loans...')\n",
    "data_df = data_df.assign(avgSmallLoan = data_df['amtLoans1'] / data_df['nLoans1'])\n",
    "\n",
    "data_df = data_df.assign(nWorkingLoans = 0)\n",
    "data_df.loc[data_df['avgSmallLoan'] < 10000, \n",
    "                           'nWorkingLoans'] = data_df['nLoans'][data_df['avgSmallLoan'] < 10000] - data_df['nLoans1'][data_df['avgSmallLoan'] < 10000]\n",
    "data_df.loc[data_df['avgSmallLoan'] >= 10000, \n",
    "                           'nWorkingLoans'] = data_df['nLoans'][data_df['avgSmallLoan'] >= 10000]\n",
    "\n",
    "data_df = data_df.assign(amtWorkingLoans = 0)\n",
    "data_df.loc[data_df['avgSmallLoan'] < 10000, \n",
    "                           'amtWorkingLoans'] = data_df['amtLoans'][data_df['avgSmallLoan'] < 10000] - data_df['amtLoans1'][data_df['avgSmallLoan'] < 10000]\n",
    "data_df.loc[data_df['avgSmallLoan'] >= 10000, \n",
    "                           'amtWorkingLoans'] = data_df['amtLoans'][data_df['avgSmallLoan'] >= 10000]\n",
    "\n",
    "data_df = data_df.sort_index()\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Kept {0:,.0f} tract-years in {1:,.2f} seconds!'.format(len(data_df), e-s))\n",
    "print(data_df.groupby('income_group').size())\n",
    "print(data_df.groupby('cra_level').size())\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#data_df[data_df['cra_level'] == 'unknown']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to jobs data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get raw jobs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# s = time.time()\n",
    "\n",
    "# jobs_df = pandas.DataFrame()\n",
    "\n",
    "# state_abbreviations_df = pandas.read_csv(extrasdir+'statecodes.csv', low_memory=False)\n",
    "\n",
    "# state_abbrev_list = state_abbreviations_df['STUSAB'][state_abbreviations_df['STATE'] < 60].apply(lambda x: x.lower()).tolist()\n",
    "\n",
    "# sum_columns = []\n",
    "# sum_columns += ['C000', 'CA01', 'CA02', 'CA03', 'CE01', 'CE02', 'CE03']\n",
    "# sum_columns += ['CNS01', 'CNS02', 'CNS03', 'CNS04', 'CNS05', 'CNS06', 'CNS07', 'CNS08']\n",
    "# sum_columns += ['CNS09', 'CNS10', 'CNS11', 'CNS12', 'CNS13', 'CNS14', 'CNS15', 'CNS16']\n",
    "# sum_columns += ['CNS17', 'CNS18', 'CNS19', 'CNS20', 'CR01', 'CR02', 'CR03', 'CR04']\n",
    "# sum_columns += ['CR05', 'CR07', 'CT01', 'CT02', 'CD01', 'CD02', 'CD03', 'CD04', 'CS01']\n",
    "# sum_columns += ['CS02', 'CFA01', 'CFA02', 'CFA03', 'CFA04', 'CFA05', 'CFS01', 'CFS02']\n",
    "# sum_columns += ['CFS03', 'CFS04', 'CFS05']\n",
    "\n",
    "\n",
    "# for i in range(0, len(state_abbrev_list)):\n",
    "#     state_number = state_abbreviations_df[state_abbreviations_df['STUSAB'].apply(lambda x: x.lower()) == state_abbrev_list[i]]['STATE'].values[0]\n",
    "# #    print(i,\":\",state_number)\n",
    "#     if (debug >= 1):\n",
    "#         print('reading state jobs data for {0:} ({1:,.0f} tracts in 2018)...'.format(state_abbrev_list[i].upper(), len(all_tracts_gdf[all_tracts_gdf['STATEFP'] == state_number])))\n",
    "#     print('Found {0:,.0f} tract-years with loans in {1:}...'.format(len(data_df[data_df['state'] == state_number]), state_abbrev_list[i]))\n",
    "#     if (state_abbrev_list[i] in ['ca', 'fl', 'il', 'ny', 'tx', 'va']):\n",
    "#         if (debug >= 2):\n",
    "#             print('reading state jobs data for {0:} file 1...'.format(state_abbrev_list[i].upper()))\n",
    "#         state_raw_jobs_df = pandas.read_csv(jobs_dir+'jobs_data_{0:}_1.csv'.format(state_abbrev_list[i]), low_memory=False)\n",
    "#         if (debug >= 2):\n",
    "#             print('reading state jobs data for {0:} file 2...'.format(state_abbrev_list[i].upper()))\n",
    "#         state_raw_jobs_df = pandas.concat((state_raw_jobs_df,pandas.read_csv(jobs_dir+'jobs_data_{0:}_2.csv'.format(state_abbrev_list[i]), low_memory=False)), axis=0, sort=False)\n",
    "#     else:\n",
    "#         state_raw_jobs_df = pandas.read_csv(jobs_dir+'jobs_data_{0:}.csv'.format(state_abbrev_list[i]), low_memory=False)\n",
    "# #    print(state_raw_jobs_df)\n",
    "#     print('\\tread {0:,.0f} block-group-years...'.format(len(state_raw_jobs_df)))\n",
    "#     state_raw_jobs_df = state_raw_jobs_df[state_raw_jobs_df['year'] >= 2010]\n",
    "#     print('\\tkept jobs since 2010 (n = {0:,.0f}), adding geoids...'.format(len(state_raw_jobs_df)))\n",
    "#     if (i <= 6):\n",
    "#         state_raw_jobs_df = state_raw_jobs_df.assign(tract_geoid = state_raw_jobs_df['w_geocode'].apply(lambda x: '14000US0'+str(x)[0:10]))\n",
    "#         #print(state_raw_jobs_df['tract_geoid'])\n",
    "#     else:\n",
    "# #        pass\n",
    "# #        print(str(state_raw_jobs_df['w_geocode'].apply(lambda x: str(x)[0:11])))\n",
    "#         state_raw_jobs_df = state_raw_jobs_df.assign(tract_geoid = state_raw_jobs_df['w_geocode'].apply(lambda x: '14000US'+str(x)[0:11]))\n",
    "# #    print(state_raw_jobs_df[['w_geocode', 'tract_geoid']].sample(10))\n",
    "#     print('\\tkeeping only center city tracts...')\n",
    "# #     state_number = state_abbreviations_df[state_abbreviations_df['STUSAB'].apply(lambda x: x.lower()) == state_abbrev_list[i]]['STATE'].values[0]\n",
    "#     #print(data_df[data_df['state'] == state_number].index.unique(level='GEOID').tolist())\n",
    "#     state_raw_jobs_df = state_raw_jobs_df[state_raw_jobs_df['tract_geoid'].isin(data_df[data_df['state'] == state_number].index.unique(level='GEOID').tolist())]\n",
    "    \n",
    "#     print('\\tkept {0:,.0f} block-group-years; grouping and summing...'.format(len(state_raw_jobs_df)))\n",
    "#     state_jobs_df = state_raw_jobs_df.groupby(['tract_geoid', 'year'])[sum_columns].sum()\n",
    "#     q = time.time()\n",
    "#     print('Found jobs for {0:,.0f} tract-years in {1:} ({2:,.0f} seconds elapsed)'.format(len(state_jobs_df), state_abbrev_list[i].upper(), q-s))\n",
    "#     jobs_df = pandas.concat((jobs_df, state_jobs_df), axis=0, sort=False)\n",
    "#     print('\\n')\n",
    "# print('Writing out...')\n",
    "# jobs_df.to_csv(thisdir+'jobs.csv')\n",
    "\n",
    "# e = time.time()\n",
    "# g = g + (e-s)\n",
    "# print('Wrote out {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(jobs_df), e-s))\n",
    "# #print('ok')\n",
    "# #jobs_df\n",
    "# #state_raw_jobs_df[['w_geocode', 'tract_geoid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy 2017 jobs data to 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "jobs_df = pandas.read_csv(thisdir+'jobs.csv', low_memory=False)\n",
    "jobs_df.index.name = 'rownumber'\n",
    "\n",
    "#jobs_df.loc[:, 'tract_geoid'] = jobs_df['tract_geoid'].apply(lambda x: '14000US{0:11d}'.format(x))\n",
    "jobs_df = jobs_df.set_index(['tract_geoid', 'year'], drop=True)\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Read {0:,.0f} rows in {1:,.0f} seconds!'.format(len(jobs_df), e-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "if (len(jobs_df.reset_index()[jobs_df.reset_index()['year'] == 2017]) == 0):\n",
    "    jobs_2017_df = pandas.DataFrame(jobs_df.reset_index()[jobs_df.reset_index()['year'] == 2016])#.set_index(['tract_geoid', 'year'])\n",
    "    jobs_2017_df.loc[:, 'year'] = 2017\n",
    "    jobs_2017_df = jobs_2017_df.set_index(['tract_geoid', 'year'])\n",
    "    jobs_df = pandas.concat((jobs_df, jobs_2017_df), axis=0, sort=False).sort_index()\n",
    "\n",
    "jobs_2018_df = pandas.DataFrame(jobs_df.reset_index()[jobs_df.reset_index()['year'] == 2017])#.set_index(['tract_geoid', 'year'])\n",
    "\n",
    "jobs_2018_df.loc[:, 'year'] = 2018\n",
    "\n",
    "jobs_2018_df = jobs_2018_df.set_index(['tract_geoid', 'year'])\n",
    "\n",
    "jobs_df = pandas.concat((jobs_df, jobs_2018_df), axis=0, sort=False).sort_index()#.set_index(['tract_geoid', 'year'])\n",
    "\n",
    "jobs_df = jobs_df.reset_index().rename(columns={'tract_geoid': 'GEOID', 'year': 'activity_year'})\n",
    "\n",
    "#jobs_df.loc[:, 'GEOID'] = jobs_df['GEOID'].apply(lambda x: '{0:11d}'.format(x))\n",
    "jobs_df = jobs_df.set_index(['GEOID', 'activity_year'])\n",
    "#jobs_df.index.name = data_df.index.name\n",
    "\n",
    "# print('bafcking up...')\n",
    "# jobs_df_bk = jobs_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Copied 2017 values to 2018 (now {0:,.0f} tract-years in jobs_df) in {1:,.1f} seconds!'.format(len(jobs_df), e-s))\n",
    "#jobs_df.xs(2018, level='activity_year')\n",
    "#jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the jobs columns we want to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup...')\n",
    "# jobs_df = jobs_df_bk\n",
    "\n",
    "jobs_metadata_df = pandas.read_csv(code_lookup_dir+'wac_jobs_metadata.csv', encoding='utf-8', index_col='varnum')\n",
    "jobs_metadata_df = jobs_metadata_df.set_index('variable')\n",
    "\n",
    "jobs_columns = ['C000', 'CA01', 'CA02', 'CA03', 'CE01', 'CE02', 'CE03', 'CNS01']\n",
    "jobs_columns += ['CNS02', 'CNS03', 'CNS04', 'CNS05', 'CNS06', 'CNS07', 'CNS08']\n",
    "jobs_columns += ['CNS09', 'CNS10', 'CNS11', 'CNS12', 'CNS13', 'CNS14', 'CNS15']\n",
    "jobs_columns += ['CNS16', 'CNS17', 'CNS18', 'CNS19', 'CNS20', 'CR01', 'CR02']\n",
    "jobs_columns += ['CR03', 'CR04', 'CR05', 'CR07', 'CT01', 'CT02', 'CD01', 'CD02']\n",
    "jobs_columns += ['CD03', 'CD04', 'CS01', 'CS02', 'CFA01', 'CFA02', 'CFA03']\n",
    "jobs_columns += ['CFA04', 'CFA05', 'CFS01', 'CFS02', 'CFS03', 'CFS04', 'CFS05']\n",
    "\n",
    "print('We select these jobs columns:')\n",
    "jobs_columns_we_want = ['C000', 'CFS01']\n",
    "for x in jobs_df[jobs_columns_we_want].columns:\n",
    "    print('variable: {0:}\\t\\tdescription:{1:}'.format(x, jobs_metadata_df['description'][jobs_metadata_df.index == x].tolist()[0]))\n",
    "jobs_df = jobs_df.assign(totaljobs = jobs_df['C000'])\n",
    "jobs_df = jobs_df.assign(sbjobs = jobs_df['CFS01'])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Done in {0:,.0f} seconds!'.format(e-s))\n",
    "#jobs_df.sample(1).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join jobs data onto loan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "print('joining {0:,.0f} jobs tract-years onto {1:,.0f} tract-years of loans data...'.format(len(jobs_df), len(data_df)))\n",
    "data_df = data_df.join(jobs_df)\n",
    "\n",
    "\n",
    "# print('backing up...')\n",
    "# data_df_bk = data_df\n",
    "# jobs_df_bk = jobs_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Joined, result is {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(data_df), e-s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loans per job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "\n",
    "print('Calulating loans per job (total and with firm size 0-19)...')\n",
    "\n",
    "data_df = data_df.assign(nLoans1_per_totaljob = data_df['nLoans1'] / data_df['C000'])\n",
    "data_df = data_df.assign(amtLoans1_per_totaljob = data_df['amtLoans1'] / data_df['C000'])\n",
    "data_df = data_df.assign(nLoans100k_per_totaljob = data_df['nLoans100k'] / data_df['C000'])\n",
    "data_df = data_df.assign(amtLoans100k_per_totaljob = data_df['amtLoans100k'] / data_df['C000'])\n",
    "data_df = data_df.assign(nLoans250k_per_totaljob = data_df['nLoans250k'] / data_df['C000'])\n",
    "data_df = data_df.assign(amtLoans250k_per_totaljob = data_df['amtLoans250k'] / data_df['C000'])\n",
    "data_df = data_df.assign(nLoansToSmallest_per_totaljob = data_df['nLoansToSmallest'] / data_df['C000'])\n",
    "data_df = data_df.assign(amtLoansToSmallest_per_totaljob = data_df['amtLoansToSmallest'] / data_df['C000'])\n",
    "data_df = data_df.assign(nLoans_per_totaljob = data_df['nLoans'] / data_df['C000'])\n",
    "data_df = data_df.assign(amtLoans_per_totaljob = data_df['amtLoans'] / data_df['C000'])\n",
    "data_df = data_df.assign(nWorkingLoans_per_totaljob = data_df['nWorkingLoans'] / data_df['C000'])\n",
    "data_df = data_df.assign(amtWorkingLoans_per_totaljob = data_df['amtWorkingLoans'] / data_df['C000'])\n",
    "\n",
    "data_df = data_df.assign(nLoans1_per_sbjob = data_df['nLoans1'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(amtLoans1_per_sbjob = data_df['amtLoans1'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(nLoans100k_per_sbjob = data_df['nLoans100k'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(amtLoans100k_per_sbjob = data_df['amtLoans100k'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(nLoans250k_per_sbjob = data_df['nLoans250k'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(amtLoans250k_per_sbjob = data_df['amtLoans250k'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(nLoansToSmallest_per_sbjob = data_df['nLoansToSmallest'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(amtLoansToSmallest_per_sbjob = data_df['amtLoansToSmallest'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(nLoans_per_sbjob = data_df['nLoans'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(amtLoans_per_sbjob = data_df['amtLoans'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(nWorkingLoans_per_sbjob = data_df['nWorkingLoans'] / data_df['CFS01'])\n",
    "data_df = data_df.assign(amtWorkingLoans_per_sbjob = data_df['amtWorkingLoans'] / data_df['CFS01'])\n",
    "\n",
    "print('recoding infinite values to NaN...')\n",
    "\n",
    "per_job_columns = ['nLoans1_per_totaljob', 'amtLoans1_per_totaljob', 'nLoans100k_per_totaljob']\n",
    "per_job_columns += ['amtLoans100k_per_totaljob', 'nLoans250k_per_totaljob', 'amtLoans250k_per_totaljob']\n",
    "per_job_columns += ['nLoansToSmallest_per_totaljob', 'amtLoansToSmallest_per_totaljob']\n",
    "per_job_columns += ['nLoans_per_totaljob', 'amtLoans_per_totaljob', 'nWorkingLoans_per_totaljob']\n",
    "per_job_columns += ['amtWorkingLoans_per_totaljob', 'nLoans1_per_sbjob', 'amtLoans1_per_sbjob']\n",
    "per_job_columns += ['nLoans100k_per_sbjob', 'amtLoans100k_per_sbjob', 'nLoans250k_per_sbjob']\n",
    "per_job_columns += ['amtLoans250k_per_sbjob', 'nLoansToSmallest_per_sbjob', 'amtLoansToSmallest_per_sbjob']\n",
    "per_job_columns += ['nLoans_per_sbjob', 'amtLoans_per_sbjob', 'nWorkingLoans_per_sbjob']\n",
    "per_job_columns += ['amtWorkingLoans_per_sbjob']\n",
    "\n",
    "\n",
    "for x in per_job_columns:\n",
    "    data_df.loc[data_df[x] == np.inf, x] = np.nan\n",
    "\n",
    "print('backing up...')\n",
    "data_df_bk = data_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('calculated loans per job for {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(data_df), e-s))\n",
    "\n",
    "data_df.sample(2).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "\n",
    "# loan_sum_columns = [x for x in data_df.columns.tolist() if ('loans' in x.lower()) and ('per' not in x.lower())]\n",
    "# jobs_sum_columns = [x for x in data_df.columns.tolist() if ('jobs' in x.lower()) or (x[0] == 'C')]\n",
    "# info_columns = ['loan_type','action_taken_type','state','county','msa','census_tract','split_county_indicator','population_classification','income_group_code','income_group','cra_level']\n",
    "\n",
    "# averaged_columns = [x for x in data_df.columns.tolist() if ((x not in loan_sum_columns) and (x not in jobs_sum_columns) and (x not in info_columns))]\n",
    "\n",
    "# #[x for x in data_df.columns.tolist() if ((x not in loan_sum_columns) and (x not in jobs_sum_columns) and (x not in info_columns) and (x not in averaged_columns))]\n",
    "# #jobs_2018_columns = [x+'_2018' for x in jobs_sum_columns]\n",
    "\n",
    "# tract_data_df = pandas.DataFrame(data=None, columns=[info_columns+loan_sum_columns+jobs_sum_columns+averaged_columns], index=data_df.index.unique(level='GEOID'))\n",
    "# # for thiscol in averaged_columns:\n",
    "# #     print(thiscol)\n",
    "# #tract_data_df.loc[:, info_columns] = \n",
    "# for i in range(0, len(jobs_sum_columns)):\n",
    "#     print('{0:}'.format(jobs_sum_columns[i]))\n",
    "#     print(data_df.xs(2018, level='activity_year')[jobs_sum_columns[i]])\n",
    "#     print('\\n')\n",
    "# #    tract_data_df.loc[:, jobs_sum_columns[i]] = data_df.xs(2018, level='activity_year')[jobs_sum_columns[i]]#.values#[0]\n",
    "\n",
    "# tract_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "\n",
    "#years = np.arange(2018, 2009, -1)\n",
    "\n",
    "\n",
    "acs5_estimates_df = pandas.DataFrame()\n",
    "acs5_margins_of_error_df = pandas.DataFrame()\n",
    "\n",
    "print('Getting ACS 5-year census data...')\n",
    "\n",
    "for thisyear in years:\n",
    "    print('\\t{0:.0f} estimates...'.format(thisyear))\n",
    "    acs5_estimates_this_year_df = pandas.read_csv(acs5_basedir+'{0:.0f}/estimates/estimates_acs{0:.0f}_tract_for_cra_analysis_mac.csv'.format(thisyear), low_memory=False, encoding='utf-8', index_col='GEOID')\n",
    "    acs5_estimates_this_year_df = acs5_estimates_this_year_df.drop([x for x in acs5_estimates_this_year_df.columns.tolist() if 'unnamed' in x.lower()], axis=1)\n",
    "    acs5_estimates_this_year_df = acs5_estimates_this_year_df.assign(year = thisyear)\n",
    "    acs5_estimates_df = pandas.concat((acs5_estimates_df, acs5_estimates_this_year_df), axis=0, sort=False)\n",
    "        \n",
    "    print('\\t{0:.0f} margins of error...'.format(thisyear))\n",
    "    acs5_margins_of_error_this_year_df = pandas.read_csv(acs5_basedir+'{0:.0f}/margins_of_error/margins_of_error_acs{0:.0f}_tract_for_cra_analysis_mac.csv'.format(thisyear), low_memory=False, encoding='utf-8', index_col='GEOID')\n",
    "    if (thisyear <= 2014):\n",
    "        acs5_margins_of_error_this_year_df = acs5_margins_of_error_this_year_df.drop([x for x in acs5_margins_of_error_this_year_df.columns.tolist() if 'unnamed' in x.lower()], axis=1)\n",
    "    acs5_margins_of_error_this_year_df = acs5_margins_of_error_this_year_df.assign(year = thisyear)\n",
    "    acs5_margins_of_error_df = pandas.concat((acs5_margins_of_error_df, acs5_margins_of_error_this_year_df), axis=0, sort=False)\n",
    "\n",
    "    \n",
    "    \n",
    "print('discarding block groups, keeping census tracts...')\n",
    "acs5_estimates_df = acs5_estimates_df.reset_index()\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df.reset_index()\n",
    "\n",
    "acs5_estimates_df = acs5_estimates_df[acs5_estimates_df['GEOID'].apply(lambda x: x[0:3] == '140')]\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df[acs5_margins_of_error_df['GEOID'].apply(lambda x: x[0:3] == '140')]\n",
    "\n",
    "acs5_estimates_df = acs5_estimates_df.sort_index()\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df.sort_index()\n",
    "\n",
    "print('renaming error columns...')\n",
    "acs5_margins_of_error_df.columns = acs5_margins_of_error_df.columns.tolist()[:7] + [x+'_err' for x in acs5_margins_of_error_df.columns.tolist()[7:-6]] + acs5_margins_of_error_df.columns.tolist()[-6:]\n",
    "\n",
    "print('setting indices...')\n",
    "acs5_estimates_df = acs5_estimates_df.rename(columns={'year': 'activity_year'})\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df.rename(columns={'year': 'activity_year'})\n",
    "\n",
    "acs5_estimates_df = acs5_estimates_df.set_index(['GEOID', 'activity_year'])\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df.set_index(['GEOID', 'activity_year'])\n",
    "\n",
    "print('backing up...')\n",
    "acs5_estimates_df_bk = acs5_estimates_df\n",
    "acs5_margins_of_error_df_bk = acs5_margins_of_error_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Read {0:,.0f} rows of estimates and {1:,.0f} of margins of error in {2:,.0f} minutes {3:.0f} seconds!'.format(len(acs5_estimates_this_year_df), len(acs5_margins_of_error_this_year_df), np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "print('\\n')\n",
    "\n",
    "#acs5_margins_of_error_df\n",
    "#acs5_margins_of_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "\n",
    "# loan_count_columns = [x for x in data_df.columns.tolist() if 'loans' in x.lower() and 'per' not in x.lower()]\n",
    "# [x for x in data_df.columns.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "print('getting from backup...')\n",
    "data_df = data_df_bk\n",
    "acs5_estimates_df = acs5_estimates_df_bk\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df_bk\n",
    "\n",
    "data_df = data_df.join(acs5_estimates_df, how='left') # 14000US01001020100\n",
    "\n",
    "#data_df = data_df.drop(['State','Logical Record Number','Geography Name','STATE','Name'], axis=1)\n",
    "data_df = data_df.drop(['State','Logical Record Number','Geography Name','STATE'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_df = data_df.join(acs5_margins_of_error_df[acs5_margins_of_error_df.columns[6:-2]], how='left')\n",
    "\n",
    "\n",
    "print('backing up...')\n",
    "data_df_bk = data_df\n",
    "print('ok')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Joined census data for {0:,.0f} tract-years'.format(len(data_df)))\n",
    "#data_df[['FILEID','FILETYPE','STUSAB','CHARITER','SEQUENCE','LOGRECNO','State','Logical Record Number','Geography Name','STATE','Name','GEOGRAPHY NAME']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate composite demographic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = pandas.read_csv(acs5_basedir+'2018/variables/variables_acs_5yr_all.csv', encoding='utf-8', low_memory=False)\n",
    "\n",
    "\n",
    "# # 'B15003_001',\n",
    "# #  'B15003_002',\n",
    "# #  'B15003_003',\n",
    "# #  'B15003_004',\n",
    "# #  'B15003_005',\n",
    "# #  'B15003_006',\n",
    "# #  'B15003_007',\n",
    "# #  'B15003_008',\n",
    "# #  'B15003_009',\n",
    "# #  'B15003_010',\n",
    "# #  'B15003_011',\n",
    "# #  'B15003_012',\n",
    "# #  'B15003_013',\n",
    "# #  'B15003_014',\n",
    "# #  'B15003_015',\n",
    "# #  'B15003_016',\n",
    "# #  'B15003_017',\n",
    "# #  'B15003_018',\n",
    "# #  'B15003_019',\n",
    "# #  'B15003_020',\n",
    "# #  'B15003_021',\n",
    "# #  'B15003_022',\n",
    "# #  'B15003_023',\n",
    "# #  'B15003_024',\n",
    "# #  'B15003_025',\n",
    "# z[['variable','description']][z['variable'].apply(lambda x: 'B15003' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "print('getting from backup...')\n",
    "data_df = data_df_bk\n",
    "\n",
    "print('\\ncalculating and renaming estimates columns for IVs...')\n",
    "\n",
    "# if (debug >= 1):\n",
    "#     print('...high school graduates or higher 25 years and older...')\n",
    "# h = data_df['B15003_002'] + data_df['B15003_003'] + data_df['B15003_004'] \n",
    "# h += data_df['B15003_005'] + data_df['B15003_006'] + data_df['B15003_007']\n",
    "# h += data_df['B15003_008'] + data_df['B15003_009']\n",
    "# h += data_df['B15003_010'] + data_df['B15003_011'] + data_df['B15003_012'] \n",
    "# h += data_df['B15003_013'] + data_df['B15003_014'] + data_df['B15003_015'] \n",
    "# h += data_df['B15003_016'] + data_df['B15003_017'] + data_df['B15003_018'] \n",
    "# data_df = data_df.assign(hs_grad_25plus = pandas.to_numeric(h, errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('population 25 and older with bachelors degree or higher (B15003_022 t.e.m. B15003_025)...')\n",
    "    h = data_df['B15003_022'] + data_df['B15003_023'] + data_df['B15003_024']  + data_df['B15003_025']\n",
    "data_df = data_df.assign(educated = pandas.to_numeric(h, errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...population 25 years and older...')\n",
    "data_df = data_df.rename(columns={'B15003_001': 'pop_25plus'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...householder sex & race, unempoyment, poverty, home value, home age, travel time...')\n",
    "data_df = data_df.rename(columns = {  \n",
    "    'B11001_006': 'female_householders',\n",
    "    'B11001A_001': 'white_householders',\n",
    "    'B11001B_001': 'black_householders',\n",
    "    'B23025_005': 'unemployed_16plus',\n",
    "    'B17001_002': 'poverty_past_12_months',\n",
    "    'B25077_001': 'median_home_value',\n",
    "    'B25035_001': 'median_year_built',\n",
    "    'B25035_001': 'median_year_built',\n",
    "    'B08013_001': 'travel_time_to_work'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...race, owner-occupied units, mfi, vacants...')\n",
    "data_df = data_df.rename(columns = {\n",
    "    'B02001_002': 'pop_white',\n",
    "    'B02001_003': 'pop_black',\n",
    "    'B25003_002': 'owner_occ_housing_units',\n",
    "    'B19113_001': 'mfi',\n",
    "    'B25002_001': 'total_housing_units',\n",
    "    'B25002_002': 'occupied_housing_units',\n",
    "    'B25002_003': 'vacant_housing_units'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('....comparison variables: total population, total households, poverty status...')\n",
    "data_df = data_df.rename(columns = {\n",
    "    'B01001_001': 'pop_total',\n",
    "    'B11001_001': 'total_households',\n",
    "    'B23025_002': 'labor_force_16plus',\n",
    "    'B17001_001': 'poverty_status_known'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('MFI & median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "    print('median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "data_df.loc[data_df['mfi'] == '.', 'mfi'] = pandas.to_numeric(data_df['mfi'][data_df['mfi'] == '.'], errors='coerce')\n",
    "data_df.loc[data_df['median_home_value'] == '.', 'median_home_value'] = pandas.to_numeric(data_df['median_home_value'][data_df['median_home_value'] == '.'], errors='coerce')\n",
    "\n",
    "\n",
    "#data_df.columns[-80:].tolist()\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "# print('backing up...')\n",
    "# data_df_bk = data_df\n",
    "\n",
    "print('Done in {0:,.0f} seconds!'.format(e-s))\n",
    "data_df[['educated', 'mfi', 'median_home_value']]\n",
    "#data_df.sample(1).T\n",
    "#data_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create error calculating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Guide on how to calculate errors in percentages:\n",
    "# https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch08.pdf\n",
    "    \n",
    "## Aggregating Data Across Population Subgroups: add error for each group in quadrature, divide by 1.645 for serr\n",
    "s = time.time()\n",
    "\n",
    "def find_serr_educated(row):\n",
    "\n",
    "    return pandas.to_numeric(np.sqrt(row['B15003_022_err']**2 + row['B15003_023_err']**2 + row['B15003_024_err']**2 + row['B15003_025_err']**2\n",
    "                                ) / 1.645, errors='coerce')\n",
    "\n",
    "# def find_serr_householders(row):\n",
    "#     return pandas.to_numeric(np.sqrt(row['B11001_002_err']**2 + row['B11001_007_err']**2 \n",
    "#                                 ) / 1.645, errors='coerce')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Defined standard-error-calculating functions!')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "print('\\ncalculating and renaming margins of error columns for IVs...')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...margins for race, owner-occupied units, mfi...')\n",
    "data_df = data_df.rename(columns = {\n",
    "    'B02001_002_err': 'pop_white_err',\n",
    "    'B02001_003_err': 'pop_black_err',\n",
    "    'B25003_002_err': 'owner_occ_housing_units_err',\n",
    "    'B19113_001_err': 'mfi_err',\n",
    "    'B08013_001_err': 'travel_time_to_work_err',\n",
    "    'B15003_001_err': 'pop_25plus_err'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...standard errors for hs graduates 25 and older (using custom serr-finding function...')\n",
    "data_df = data_df.assign(educated_serr = pandas.to_numeric(data_df.apply(lambda row: find_serr_educated(row), axis=1), errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...margins of error for householder sex & race, unempoyment, poverty, home value, home age...')\n",
    "data_df = data_df.rename(columns = {     \n",
    "    'B11001_001_err': 'total_households_err',\n",
    "    'B11001_006_err': 'female_householders_err',\n",
    "    'B11001A_001_err': 'black_householders_err',\n",
    "    'B11001B_001_err': 'white_householders_err',\n",
    "    'B23025_005_err': 'unemployed_16plus_err',\n",
    "    'B17001_002_err': 'poverty_past_12_months_err',\n",
    "    'B25077_001_err': 'median_home_value_err',\n",
    "    'B25035_001_err': 'median_year_built_err',\n",
    "    'B25003_002_err': 'owner_occ_housing_units_err',\n",
    "    'B19113_001_err': 'mfi_err',\n",
    "    'B25002_001_err': 'total_housing_units_err',\n",
    "    'B25002_002_err': 'occupied_housing_units_err',\n",
    "    'B25002_003_err': 'vacant_housing_units_err'\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('MFI & median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "data_df.loc[data_df['mfi_err'] == '.', 'mfi_err'] = pandas.to_numeric(data_df['mfi_err'][data_df['mfi_err'] == '.'], errors='coerce')\n",
    "data_df.loc[data_df['median_home_value_err'] == '.', 'median_home_value_err'] = pandas.to_numeric(data_df['median_home_value_err'][data_df['median_home_value_err'] == '.'], errors='coerce')\n",
    "\n",
    "\n",
    "print('\\ncalculating and renaming margins of error for comparison variables...')\n",
    "if (debug >= 1):\n",
    "    print('...race, owner-occupied units, mfi...')\n",
    "data_df = data_df.rename(columns = {\n",
    "    'B01001_001_err': 'pop_total_err',\n",
    "    'B17001_001_err': 'poverty_status_known_err'\n",
    "})\n",
    "\n",
    "# if (debug >= 1):\n",
    "#     print('...total householders...')\n",
    "# data_df = data_df.assign(total_householders_serr = pandas.to_numeric(data_df.apply(lambda row: find_serr_householders(row), axis=1), errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...labor force, poverty status known...')\n",
    "data_df = data_df.rename(columns = {\n",
    "    'B23025_002_err': 'labor_force_16plus_err',\n",
    "    'B17001_001_err': 'poverty_status_known_err'\n",
    "})\n",
    "\n",
    "\n",
    "print('dropping columns we do not care about...')\n",
    "\n",
    "columns_do_not_care = []\n",
    "columns_do_not_care += ['B11001_002', 'B11001_007']\n",
    "columns_do_not_care += ['B11001_002_err', 'B11001_007_err']\n",
    "columns_do_not_care += ['B01001_048_err','B01001_049_err']#,'STATE']\n",
    "\n",
    "data_df = data_df.drop(columns_do_not_care, axis=1)\n",
    "\n",
    "\n",
    "# print('Calculated errors for all columns!')\n",
    "# print('backing up...')\n",
    "# data_df_bk = data_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "data_df.sample(1).T\n",
    "#print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare to percentify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "vars_for_percentification = ['pop_white', 'pop_black', 'black_householders', 'white_householders']\n",
    "vars_for_percentification += ['owner_occ_housing_units', 'educated', 'female_householders']\n",
    "vars_for_percentification += ['unemployed_16plus', 'poverty_past_12_months']\n",
    "\n",
    "vars_for_percentification += ['pop_white_err', 'pop_black_err', 'black_householders_err', 'white_householders_err']\n",
    "vars_for_percentification += ['owner_occ_housing_units_err', 'educated_serr', 'female_householders_err']\n",
    "vars_for_percentification += ['unemployed_16plus_err', 'poverty_past_12_months_err']\n",
    "\n",
    "vars_for_percentification += ['pop_total', 'total_householders', 'pop_25plus', 'labor_force_16plus']\n",
    "vars_for_percentification += ['poverty_status_known', 'vacant_housing_units', 'total_housing_units']\n",
    "\n",
    "vars_for_percentification += ['pop_total_err', 'total_householders_serr', 'pop_25plus_err', 'labor_force_16plus_err']\n",
    "vars_for_percentification += ['poverty_status_known_err', 'vacant_housing_units_err', 'total_housing_units_err']\n",
    "#vars_for_percentification\n",
    "#city_tracts_years_df[vars_for_percentification].columns.tolist()\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate percentages for needed demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "\n",
    "#[x for x in vars_for_percentification if \"_err\" not in x]\n",
    "\n",
    "data_df = data_df.assign(pct_white = pandas.to_numeric((data_df['pop_white'] / data_df['pop_total']), errors='coerce'))\n",
    "data_df = data_df.assign(pct_black = pandas.to_numeric((data_df['pop_black'] / data_df['pop_total']), errors='coerce'))\n",
    "\n",
    "data_df = data_df.assign(pct_white_householders = pandas.to_numeric((data_df['white_householders'] / data_df['total_households']), errors='coerce'))\n",
    "data_df = data_df.assign(pct_black_householders = pandas.to_numeric((data_df['black_householders'] / data_df['total_households']), errors='coerce'))\n",
    "data_df = data_df.assign(pct_female_householders = pandas.to_numeric((data_df['female_householders'] / data_df['total_households']), errors='coerce'))\n",
    "\n",
    "data_df = data_df.assign(peducated = pandas.to_numeric(data_df['educated'], errors='coerce') / pandas.to_numeric(data_df['pop_25plus'], errors='coerce'))\n",
    "data_df = data_df.assign(pct_unemployed = pandas.to_numeric(data_df['unemployed_16plus'], errors='coerce') / pandas.to_numeric(data_df['labor_force_16plus'], errors='coerce'))\n",
    "data_df = data_df.assign(pct_poverty = pandas.to_numeric(data_df['poverty_past_12_months'], errors='coerce') / pandas.to_numeric(data_df['poverty_status_known'], errors='coerce'))\n",
    "data_df = data_df.assign(pct_vacant = pandas.to_numeric(data_df['vacant_housing_units'], errors='coerce') / pandas.to_numeric(data_df['total_housing_units'], errors='coerce'))\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')\n",
    "#data_df.columns.tolist()\n",
    "\n",
    "data_df.sample(1).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to calculate errors in percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guide on how to do this:\n",
    "#### https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch08.pdf\n",
    "\n",
    "# X and Y are the measured values (not the errors) - X for the subsgroup and Y for the whole sample\n",
    "# Let P = X/Y  (the proportion we calculated in the last step)\n",
    "# dX and dY are the measured errors\n",
    "# dP = (1/Y) * np.sqrt(dX**2 - (P**2 * dY**2))\n",
    "# Standard error of P is dP/1.645\n",
    "#### this calculation is done verbosely in fnid_pop_white_serr, quickly in other functions\n",
    "\n",
    "s = time.time()\n",
    "def find_errors_in_pct(X, Y, dX, dY, verboselevel = 0):\n",
    "    try:\n",
    "        P = X / Y\n",
    "        oneoverY = 1 / Y\n",
    "        dXsq = dX**2\n",
    "        dYsq = dY**2\n",
    "        Psq = P**2\n",
    "        PsqdYsq = Psq * dYsq\n",
    "        if (PsqdYsq <= dXsq):\n",
    "            underroot = dXsq - PsqdYsq\n",
    "        else:\n",
    "            underroot = dXsq + PsqdYsq\n",
    "        rooty = np.sqrt(underroot)\n",
    "        dP = oneoverY * rooty\n",
    "        SE = dP / 1.645\n",
    "        if (verboselevel >= 2):\n",
    "#            print('X = pop_white, Y = pop_total')\n",
    "            print('X = {0:.0f}, dX = {1:.0f} ({2:.1%} error)'.format(X, dX, dX/X))\n",
    "            print('Y = {0:.0f}, dY = {1:.0f} ({2:.1%} error)'.format(Y, dY, dY/Y))\n",
    "        if (verboselevel >= 3):\n",
    "            print('P = {0:.3f}'.format(P))\n",
    "            print('dXsq = {0:.0f}, dYsq = {1:.0f}, Psq = {2:.3f}'.format(dXsq, dYsq, Psq))\n",
    "            print('PsqdYsq = {0:.0f}, underroot = {1:.0f}, rooty = {2:.3f}'.format(PsqdYsq, underroot, rooty))\n",
    "            print('dP = {0:.3f}'.format(dP))\n",
    "            print('SE = {0:.3f}'.format(SE))\n",
    "        if (verboselevel >= 2):\n",
    "            print('RESULT: {0:.2%} +/- {1:.2%}'.format(P, SE)) \n",
    "            print('\\n')\n",
    "        return pandas.to_numeric(SE, errors='coerce')\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "    \n",
    "e = time.time()\n",
    "g = g + (e-s)    \n",
    "print('Defined functions to calculate standard errors in percentages!')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate errors in percntages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboselevel = 0\n",
    "s = time.time()\n",
    "\n",
    "print('Calculating errors in percentages...')\n",
    "data_df = data_df.assign(pct_white_serr = np.nan)\n",
    "data_df = data_df.assign(pct_black_serr = np.nan)\n",
    "data_df = data_df.assign(pct_white_householders_serr = np.nan)\n",
    "data_df = data_df.assign(pct_black_householders_serr = np.nan)\n",
    "data_df = data_df.assign(pct_female_householders_serr = np.nan)\n",
    "data_df = data_df.assign(peducated_serr = np.nan)\n",
    "data_df = data_df.assign(pct_unemployed_serr = np.nan)\n",
    "data_df = data_df.assign(pct_poverty_serr = np.nan)\n",
    "data_df = data_df.assign(pct_vacant_serr = np.nan)\n",
    "\n",
    "#data_df.loc[:, \n",
    "#              'poverty_status_known_last12months_total_err'] = pandas.to_numeric(data_df['poverty_status_known_last12months_total_err'], errors='coerce')\n",
    "\n",
    "\n",
    "for ix, thisrow in data_df.iterrows():\n",
    "    if (verboselevel >= 2):\n",
    "        print('Census tract {0:}...'.format(ix))\n",
    "    #print('pct_white_serr...')\n",
    "    data_df.loc[ix, 'pct_white_serr'] = find_errors_in_pct(thisrow['pop_white'], thisrow['pop_total'], thisrow['pop_white_err'], thisrow['pop_total_err'], verboselevel)\n",
    "    #print('pct_black_serr...')\n",
    "    data_df.loc[ix, 'pct_black_serr'] = find_errors_in_pct(thisrow['pop_black'], thisrow['pop_total'], thisrow['pop_black_err'], thisrow['pop_total_err'], verboselevel)\n",
    "    #print('pct_white_householders_serr...')\n",
    "    data_df.loc[ix, 'pct_white_householders_serr'] = find_errors_in_pct(thisrow['white_householders'], thisrow['total_households'], thisrow['white_householders_err'], thisrow['total_households_err'], verboselevel)\n",
    "    data_df.loc[ix, 'pct_black_householders_serr'] = find_errors_in_pct(thisrow['white_householders'], thisrow['total_households'], thisrow['white_householders_err'], thisrow['total_households_err'], verboselevel)\n",
    "    data_df.loc[ix, 'pct_female_householders_serr'] = find_errors_in_pct(thisrow['white_householders'], thisrow['total_households'], thisrow['white_householders_err'], thisrow['total_households_err'], verboselevel)\n",
    "    \n",
    "    data_df.loc[ix, 'peducated_serr'] = find_errors_in_pct(thisrow['educated'], thisrow['pop_25plus'], thisrow['educated_serr'], thisrow['pop_25plus_err'], verboselevel)\n",
    "    data_df.loc[ix, 'pct_unemployed_serr'] = find_errors_in_pct(thisrow['unemployed_16plus'], thisrow['unemployed_16plus_err'], thisrow['labor_force_16plus_err'], thisrow['labor_force_16plus_err'], verboselevel)\n",
    "    data_df.loc[ix, 'pct_poverty_serr'] = find_errors_in_pct(thisrow['poverty_past_12_months'], thisrow['poverty_status_known'], thisrow['poverty_past_12_months_err'], thisrow['poverty_status_known_err'], verboselevel)\n",
    "    data_df.loc[ix, 'pct_vacant_serr'] = find_errors_in_pct(thisrow['vacant_housing_units'], thisrow['total_housing_units'], thisrow['vacant_housing_units_err'], thisrow['total_housing_units_err'], verboselevel)\n",
    "\n",
    "if (verboselevel >= 1):\n",
    "    for ix, thisrow in data_df.iterrows():\n",
    "        print('Census tract {0:,.0f}'.format(ix))\n",
    "        print('{0:,.0f} +/- {1:,.0f} white'.format(\n",
    "            thisrow['pop_white'], thisrow['pop_white_err']\n",
    "        ))\n",
    "        print('{0:,.0f} +/- {1:,.0f} total'.format(\n",
    "            thisrow['pop_total'], thisrow['pop_total_err']\n",
    "        ))\n",
    "        print('{0:.1%} +/- {1:.1%}'.format(\n",
    "            thisrow['pct_white'], thisrow['pct_white_serr']\n",
    "        ))\n",
    "        print('\\n')\n",
    "\n",
    "print('backing up...')\n",
    "data_df_bk = data_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('OMG that took a while, processed {0:,.0f} rows in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(data_df), np.floor((e-s)/60), np.floor((e-s)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('backing up...')\n",
    "e = time.time()\n",
    "# g = g + (e-s)\n",
    "data_df_bk = data_df\n",
    "print('OMG that took a while, processed {0:,.0f} rows in {1:,.0f} hours {2:,.0f} minutes {3:,.0f} seconds!'.format(len(data_df), np.floor((e-s)/3600), np.floor((e-s)%60), np.floor((e-s)%60)%60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct for inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got inflation data from https://www.bls.gov/cpi/tables/supplemental-files/historical-cpi-u-202007.pdf\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "print('getting from backup...')\n",
    "data_df = data_df_bk\n",
    "\n",
    "# I don't know why this didn't work earlier, but what the hell...\n",
    "data_df.loc[:, 'mfi'] = pandas.to_numeric(data_df['mfi'], errors='coerce')\n",
    "data_df.loc[:, 'median_home_value'] = pandas.to_numeric(data_df['median_home_value'], errors='coerce')\n",
    "data_df.loc[:, 'mfi_err'] = pandas.to_numeric(data_df['mfi_err'], errors='coerce')\n",
    "data_df.loc[:, 'median_home_value_err'] = pandas.to_numeric(data_df['median_home_value_err'], errors='coerce')\n",
    "\n",
    "money_columns = []\n",
    "money_columns += ['amtLoans1', 'amtLoans100k', 'amtLoans250k', 'amtLoansToSmallest']\n",
    "money_columns += ['avgSmallLoan', 'amtLoans', 'amtWorkingLoans']\n",
    "money_columns += ['amtLoans1_per_totaljob', 'amtLoans100k_per_totaljob', 'amtLoans250k_per_totaljob']\n",
    "money_columns += ['amtLoansToSmallest_per_totaljob', 'amtLoans_per_totaljob', 'amtWorkingLoans_per_totaljob']\n",
    "money_columns += ['amtLoans1_per_sbjob', 'amtLoans100k_per_sbjob', 'amtLoans250k_per_sbjob']\n",
    "money_columns += ['amtLoansToSmallest_per_sbjob', 'amtLoans_per_sbjob', 'amtWorkingLoans_per_sbjob']\n",
    "money_columns += ['mfi', 'median_home_value', 'mfi_err', 'median_home_value_err']\n",
    "\n",
    "raw_inflation_df = pandas.read_csv(inflation_dir+'cpi-1913-2019.csv', encoding='utf-8', low_memory=False, index_col='Year')\n",
    "raw_inflation_df.index.name = 'year'\n",
    "\n",
    "for thisyear in years:\n",
    "    inflation_factor = 1 / (raw_inflation_df['Jan'].loc[thisyear] / raw_inflation_df['Jan'].loc[2017])\n",
    "    for thiscol in money_columns:\n",
    "        varname = thiscol + '_adj'\n",
    "        data_df[varname] = data_df[thiscol].apply(lambda x: x * inflation_factor)\n",
    "\n",
    "# print(data_df['income_group'][data_df['income_group'].isnull()].count())\n",
    "# print(data_df['cra_level'][data_df['cra_level'].isnull()].count())\n",
    "        \n",
    "# print('somehow we got all the way here and there are still some NaN values for income group and CRA level, so fill them in as unknown...')\n",
    "# data_df.loc[:, 'income_group'] = data_df['income_group'].fillna('unknown')\n",
    "# data_df.loc[:, 'cra_level'] = data_df['cra_level'].fillna('unknown')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "# print('backing up...')\n",
    "# data_df_bk = data_df\n",
    "\n",
    "\n",
    "\n",
    "print('Inflation-corrected {0:,.0f} tracts in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(data_df), np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "\n",
    "\n",
    "data_df.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('let us write out so we do not lose anything...')\n",
    "\n",
    "data_df.to_csv(cities_tracts_dir+\"data_df_backup.csv\")\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 240,604 rows in 46 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "data_df = pandas.read_csv(cities_tracts_dir+\"data_df_backup.csv\", low_memory=False, index_col=['GEOID', 'activity_year'])\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('read {0:,.0f} rows in {1:,.0f} seconds!'.format(len(data_df), e-s))\n",
    "#datab_df.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add community statistical areas (Baltimore only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2010 CSAs for 1,767 Baltimore city tracts in 2.1 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "\n",
    "data_df = data_df.assign(baltimore_csa2010 = np.nan)\n",
    "\n",
    "tract_to_csa_df = pandas.read_csv(code_lookup_dir+'census_tract_to_neighborhood.csv', index_col='NAME10')\n",
    "tract_to_csa_df = tract_to_csa_df.assign(GEOID = tract_to_csa_df['GEOID10'].apply(lambda x: '14000US'+str(x)))\n",
    "\n",
    "\n",
    "data_df.loc[(data_df['state'] == 24) & (data_df['county'] == 510), 'baltimore_csa2010'] = data_df[(data_df['state'] == 24) & (data_df['county'] == 510)].reset_index().merge(tract_to_csa_df.reset_index(), how='left', on='GEOID').set_index(['GEOID', 'activity_year'])['CSA2010']\n",
    "\n",
    "# print('backing up...')\n",
    "# data_df_bk = data_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "#print('Added 2010 CSAs for {0:,.0f} Baltimore city tracts in {1:,.0f} hours {2:,.0f} minutes {3:,.1f} seconds!'.format(len(data_df['baltimore_csa2010'].dropna()), np.floor((e-s)/3600), np.floor(((e-s)/3600)/60), (e-s)%60))\n",
    "print('Added 2010 CSAs for {0:,.0f} Baltimore city tracts in {1:,.1f} seconds!'.format(len(data_df['baltimore_csa2010'].dropna()), (e-s)%60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get summary data for each census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing tract data...\n",
      "getting info columns...\n",
      "summing loan columns...\n",
      "getting jobs data from 2018...\n",
      "getting census data from 2018...\n",
      "Averaging columns...\n",
      "selecting column order...\n",
      "Got data for 28,299 tracts in 0 hours 0 minutes 58.0 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_df = data_df_bk\n",
    "\n",
    "print('initializing tract data...')\n",
    "info_columns = ['loan_type','action_taken_type','state','county','msa','census_tract','split_county_indicator','population_classification','income_group_code','income_group','cra_level','baltimore_csa2010','FILEID', 'LOGRECNO', 'FILETYPE','STUSAB','CHARITER','SEQUENCE']\n",
    "\n",
    "#adjusted_columns = [x for x in data_df.columns.tolist() if '_adj' in x]\n",
    "\n",
    "loan_sum_columns = [x for x in data_df.columns.tolist() if ('loans' in x.lower()) and ('per' not in x.lower())]\n",
    "loan_sum_columns = [x for x in loan_sum_columns if not(('amt' in x) and ('_adj' not in x))]\n",
    "\n",
    "jobs_2018_columns = [x for x in data_df.columns.tolist() if ('jobs' in x.lower()) or ((x[0] == 'C') and (x != 'CHARITER'))]\n",
    "\n",
    "other_columns = [x for x in  data_df.columns.tolist() if (x not in loan_sum_columns) and (x not in jobs_2018_columns) and (x not in info_columns)]\n",
    "\n",
    "averaged_columns = [x for x in other_columns if (('per' in x))]\n",
    "averaged_columns = [x for x in averaged_columns if not(('amt' in x) and ('_adj' not in x))]\n",
    "#averaged_columns.append('avgSmallLoan')\n",
    "averaged_columns.append('avgSmallLoan_adj')\n",
    "\n",
    "census_2018_columns = [x for x in other_columns if (x not in averaged_columns) and ('amt' not in x)]\n",
    "census_2018_columns = [x for x in census_2018_columns if x not in ['GEOGRAPHY NAME', 'State_err', 'Logical Record Number']]\n",
    "\n",
    "\n",
    "tract_data_df = pandas.DataFrame(data=None, columns=info_columns+loan_sum_columns+jobs_2018_columns+census_2018_columns+averaged_columns, index=data_df.xs(2018, level='activity_year').index)\n",
    "\n",
    "print('getting info columns...')\n",
    "for x in info_columns:\n",
    "    tract_data_df.loc[:, x] = data_df.xs(2018, level='activity_year')[x]\n",
    "\n",
    "print('summing loan columns...')\n",
    "for x in loan_sum_columns:\n",
    "#    print('\\tsumming {0:}...'.format(x))\n",
    "    tract_data_df.loc[:, x] = data_df.reset_index()[data_df.reset_index()['GEOID'].isin(tract_data_df.index)].groupby('GEOID')[x].sum()\n",
    "\n",
    "print('getting jobs data from 2018...')\n",
    "for x in jobs_2018_columns:\n",
    "    #print(x)\n",
    "    tract_data_df.loc[:, x] = data_df.xs(2018, level='activity_year')[x]\n",
    "#    print('\\n')\n",
    "    #tract_data_df.loc[:, x] = data_df.reset_index()[data_df.reset_index()['GEOID'].isin(tract_data_df.index)]\n",
    "# # #tract_data_df\n",
    "\n",
    "print('getting census data from 2018...')\n",
    "for x in census_2018_columns:\n",
    "    tract_data_df.loc[:, x] = data_df.xs(2018, level='activity_year')[x]\n",
    "\n",
    "print('Averaging columns...')\n",
    "\n",
    "for i in range(0, len(averaged_columns[:-1])):\n",
    "    if (debug >= 3):\n",
    "        print('\\tcalculating {0:}...'.format(averaged_columns[i]))\n",
    "    part1 = averaged_columns[i][0:averaged_columns[i].find(\"_\")]\n",
    "    part2 = averaged_columns[i][averaged_columns[i].find(\"per_\")+4:]+'s'\n",
    "    if ('_adj' in part2):\n",
    "        part1 = part1 + '_adj'\n",
    "        part2 = part2[0:part2.find(\"_adj\")]+'s'\n",
    "    tract_data_df.loc[:, averaged_columns[i]] = data_df.reset_index().groupby('GEOID')[part1].sum()/data_df.reset_index().groupby('GEOID')[part2].sum()\n",
    "\n",
    "tract_data_df.loc[:, 'avgSmallLoan_adj'] = data_df.reset_index().groupby('GEOID')['amtLoans1_adj'].sum() / data_df.reset_index().groupby('GEOID')['nLoans1'].sum()\n",
    "\n",
    "print('selecting column order...')\n",
    "new_columns = []\n",
    "new_columns += ['state','county','census_tract','Geography Name']\n",
    "new_columns += ['msa'] # metropolitan statistical area (MSA) number\n",
    "new_columns += ['loan_type', 'action_taken_type'] # loan type (always 4 for business) and action taken (always 1 for origination)\n",
    "new_columns += ['income_group_code'] # income group code (1 to 14 or 101 to 105)\n",
    "new_columns += ['income_group', 'cra_level'] # income group and CRA level (human-readable)\n",
    "new_columns += ['nLoans1','amtLoans1_adj','nLoans100k','amtLoans100k_adj','nLoans250k','amtLoans250k_adj','nLoansToSmallest','amtLoansToSmallest_adj'] # number and amount of loans (directly from CRA data)\n",
    "new_columns += ['nLoans', 'amtLoans_adj', 'avgSmallLoan_adj', 'nWorkingLoans', 'amtWorkingLoans_adj'] # calculated total and working loans\n",
    "new_columns += ['totaljobs', 'sbjobs'] # total and small business jobs\n",
    "\n",
    "# all jobs columns under original names\n",
    "new_columns += ['C000','CA01','CA02','CA03','CE01','CE02','CE03']\n",
    "new_columns += ['CNS01','CNS02','CNS03','CNS04','CNS05','CNS06','CNS07','CNS08','CNS09','CNS10','CNS11','CNS12','CNS13','CNS14','CNS15','CNS16','CNS17','CNS18','CNS19','CNS20']\n",
    "new_columns += ['CR01','CR02','CR03','CR04','CR05','CR07','CT01','CT02','CD01','CD02','CD03','CD04','CS01','CS02']\n",
    "new_columns += ['CFA01','CFA02','CFA03','CFA04','CFA05','CFS01','CFS02','CFS03','CFS04','CFS05']\n",
    "\n",
    "# loans per job\n",
    "new_columns += ['nLoans1_per_totaljob','amtLoans1_per_totaljob_adj','nLoans100k_per_totaljob','amtLoans100k_per_totaljob_adj']\n",
    "new_columns += ['nLoans250k_per_totaljob','amtLoans250k_per_totaljob_adj','nLoansToSmallest_per_totaljob','amtLoansToSmallest_per_totaljob_adj']\n",
    "new_columns += ['nLoans_per_totaljob','amtLoans_per_totaljob_adj','nWorkingLoans_per_totaljob','amtWorkingLoans_per_totaljob_adj']\n",
    "new_columns += ['nLoans1_per_sbjob','amtLoans1_per_sbjob_adj','nLoans100k_per_sbjob','amtLoans100k_per_sbjob_adj','nLoans250k_per_sbjob','amtLoans250k_per_sbjob_adj']\n",
    "new_columns += ['nLoansToSmallest_per_sbjob','amtLoansToSmallest_per_sbjob_adj','nLoans_per_sbjob','amtLoans_per_sbjob_adj','nWorkingLoans_per_sbjob','amtWorkingLoans_per_sbjob_adj']\n",
    "\n",
    "# census count estimates\n",
    "new_columns += ['pop_total','pop_white','pop_black']\n",
    "new_columns += ['total_households','white_householders','black_householders','female_householders']\n",
    "new_columns += ['total_housing_units','occupied_housing_units','vacant_housing_units']\n",
    "new_columns += ['educated','unemployed_16plus','poverty_past_12_months','mfi', 'travel_time_to_work'] # high school graduates (ages 25+), unemployed, in poverty, MFI, travel time to work\n",
    "new_columns +=['owner_occ_housing_units','median_home_value','median_year_built'] # home\n",
    "new_columns += ['pop_25plus','labor_force_16plus','poverty_status_known']\n",
    "\n",
    "# census count errors\n",
    "new_columns += ['pop_total_err','pop_white_err','pop_black_err']\n",
    "new_columns += ['total_households_err','white_householders_err','black_householders_err','female_householders_err']\n",
    "new_columns += ['total_housing_units_err','occupied_housing_units_err','vacant_housing_units_err']\n",
    "new_columns += ['educated_serr','unemployed_16plus_err','poverty_past_12_months_err','mfi_err', 'travel_time_to_work_err'] # high school graduates (ages 25+), unemployed, in poverty, MFI, travel time to work\n",
    "new_columns += ['owner_occ_housing_units_err','median_home_value_err'] # home\n",
    "new_columns += ['pop_25plus_err','labor_force_16plus_err','poverty_status_known_err']\n",
    "\n",
    "# census percentages and their errors\n",
    "new_columns += ['pct_white','pct_black','pct_white_householders','pct_black_householders','pct_female_householders']\n",
    "new_columns += ['peducated','pct_unemployed','pct_poverty','pct_vacant']\n",
    "new_columns += ['pct_white_serr','pct_black_serr','pct_white_householders_serr','pct_black_householders_serr','pct_female_householders_serr']\n",
    "new_columns += ['peducated_serr','pct_unemployed_serr','pct_poverty_serr','pct_vacant_serr']\n",
    "\n",
    "# inflation-adjusted money values\n",
    "new_columns += ['amtLoans1_adj','amtLoans100k_adj','amtLoans250k_adj','amtLoansToSmallest_adj']\n",
    "new_columns += ['avgSmallLoan_adj','amtLoans_adj','amtWorkingLoans_adj']\n",
    "new_columns += ['amtLoans1_per_totaljob_adj','amtLoans100k_per_totaljob_adj','amtLoans250k_per_totaljob_adj','amtLoansToSmallest_per_totaljob_adj']\n",
    "new_columns += ['amtLoans_per_totaljob_adj','amtWorkingLoans_per_totaljob_adj']\n",
    "new_columns += ['amtLoans1_per_sbjob_adj','amtLoans100k_per_sbjob_adj','amtLoans250k_per_sbjob_adj','amtLoansToSmallest_per_sbjob_adj','amtLoans_per_sbjob_adj','amtWorkingLoans_per_sbjob_adj']\n",
    "new_columns += ['mfi_adj','median_home_value_adj','mfi_err_adj','median_home_value_err_adj']\n",
    "\n",
    "# other info columns\n",
    "#new_columns += ['STATEFP','TRACTCE','NAMELSAD','MTFCC','FUNCSTAT','ALAND','AWATER','INTPTLAT','INTPTLON']\n",
    "\n",
    "new_columns += ['split_county_indicator','population_classification', 'baltimore_csa2010']\n",
    "#new_columns += ['FILEID','LOGRECNO','FILETYPE','STUSAB','CHARITER','SEQUENCE']\n",
    "\n",
    "#new_columns += ['geometry']\n",
    "\n",
    "tract_data_df = tract_data_df[new_columns]\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Got data for {0:,.0f} tracts in {1:,.0f} hours {2:,.0f} minutes {3:,.1f} seconds!'.format(len(tract_data_df), np.floor((e-s)/3600), np.floor(((e-s)/3600)/60), (e-s)%60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading shapefiles...\n",
      "Got 73,056 tract shapefiles in 43 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('reading shapefiles...')\n",
    "tract_shapefiles_gdf = geopandas.GeoDataFrame()\n",
    "for thisfile in [shapefile_dir+x for x in os.listdir(shapefile_dir) if x[-4:] == '.shp']:\n",
    "    this_state_gdf = geopandas.read_file(thisfile)\n",
    "    tract_shapefiles_gdf = pandas.concat((tract_shapefiles_gdf, this_state_gdf), axis=0, sort=False)\n",
    "\n",
    "tract_shapefiles_gdf.loc[:, 'GEOID'] = tract_shapefiles_gdf['GEOID'].apply(lambda x: '14000US'+x)\n",
    "tract_shapefiles_gdf = tract_shapefiles_gdf.set_index('GEOID')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Got {0:,.0f} tract shapefiles in {1:,.0f} seconds!'.format(len(tract_shapefiles_gdf),e-s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing data geodataframe...\n",
      "writing out...\n",
      "Wrote data for 28,299 tracts in 618 seconds!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('constructing data geodataframe...')\n",
    "msa_central_city_tracts_gdf = geopandas.GeoDataFrame(tract_data_df.join(tract_shapefiles_gdf, how='left'), crs=tract_shapefiles_gdf.crs, geometry='geometry')\n",
    "msa_central_city_tracts_gdf.plot()\n",
    "\n",
    "print('writing out...')\n",
    "msa_central_city_tracts_gdf.to_file(cities_tracts_dir+'all_cities.shp')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Wrote data for {0:,.0f} tracts in {1:,.0f} seconds!'.format(len(msa_central_city_tracts_gdf), e-s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get only the columns we need, in the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# new_columns += data_gdf.columns.tolist()[0:9]   # geographic information about the census tract\n",
    "# new_columns += data_gdf.columns.tolist()[18:20]  # split county indicator and population classification from CRA data, probably not useful\n",
    "# new_columns += data_gdf.columns.tolist()[115:118] # geodata from ACS5\n",
    "# new_columns += data_gdf.columns.tolist()[124:125]  # owner-occupied housing units (we don't use this yet)\n",
    "# new_columns += data_gdf.columns.tolist()[194:195]  # community statistical area (useful only for Baltimore)\n",
    "# #\n",
    "# new_columns += data_gdf.columns.tolist()[10:11]   # as is traditional, geometry goes last\n",
    "\n",
    "\n",
    "data_gdf = geopandas.GeoDataFrame(data_gdf, crs=state_tracts_gdf.crs)\n",
    "\n",
    "output_gdf = data_gdf[new_columns]\n",
    "#data_gdf[new_columns].crs\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "#output_gdf[output_gdf.columns.tolist()[:-1]].columns.tolist()\n",
    "print('Done in {0:,.1f} seconds!'.format(e-s))\n",
    "## #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "print('writing output data...')\n",
    "# output_gdf.reset_index().shape  # (398,197)\n",
    "# output_gdf[output_gdf.columns.tolist()[:-1]].reset_index().shape  # (398, 196)\n",
    "\n",
    "output_gdf.reset_index().to_file(output_data_dir+'{0}.shp'.format(city.replace(\".\",\"\").replace(\" \",\"_\").lower()))\n",
    "output_gdf[output_gdf.columns.tolist()[:-1]].reset_index().to_csv(output_data_dir+'{0}_no_geo.csv'.format(city.replace(\".\",\"\").replace(\" \",\"_\").lower()))\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print('Output file written in {0:,.0f} minutes {1:,.0f} seconds!'.format(np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "\n",
    "print('Grand total time: {0:,.0f} minutes {1:,.0f} seconds!'.format(np.floor(g/60), np.floor(g%60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ggdf = geopandas.read_file(output_data_dir+'{0:}.shp'.format(city.replace(\" \",\"_\").lower()))\n",
    "# ggdf.plot(column='pct_black')#edgecolor='white')\n",
    "# plt.show()\n",
    "# #ggdf.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output_gdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
