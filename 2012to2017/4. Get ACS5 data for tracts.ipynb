{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install xlrd\n",
    "debug = 1\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import geopandas\n",
    "import os\n",
    "from pprint import pprint\n",
    "pandas.set_option('display.max_colwidth', None)\n",
    "g = 0\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data for one year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read variable names and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "thisyear = 2010\n",
    "# 2019: Wrote out 73,057 tracts and 217,740 block groups in 30 minutes 30 seconds!\n",
    "# 2018: Wrote out 73,056 tracts and 217,739 block groups in 38 minutes 24 seconds!\n",
    "# 2017: Wrote out 73,056 tracts and 217,739 block groups in 36 minutes 38 seconds!\n",
    "# 2016: Wrote out 73,056 tracts and 217,739 block groups in 38 minutes 14 seconds!\n",
    "# 2015: Wrote out 73,056 tracts and 217,739 block groups in 36 minutes 11 seconds!\n",
    "# 2014: Wrote out 73,056 tracts and 217,739 block groups in 35 minutes 2 seconds!\n",
    "# 2013: Wrote out 73,056 tracts and 217,739 block groups in 36 minutes 48 seconds!\n",
    "# 2012: Wrote out 73,056 tracts and 217,739 block groups in 34 minutes 32 seconds!\n",
    "# 2011: Wrote out 73,056 tracts and 217,739 block groups in 29 minutes 57 seconds!\n",
    "# 2010: Wrote out 73,057 tracts and 217,740 block groups in 36 minutes 33 seconds!\n",
    "\n",
    "category = 'for_cra_analysis_mac'\n",
    "\n",
    "basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/acs5/'\n",
    "yeardir = basedir + str(thisyear) + '/'\n",
    "\n",
    "if (thisyear == 2016):\n",
    "    rawdatadir = yeardir + 'rawdata/data/tab4/sumfile/prod/2012thru2016/group2/'\n",
    "elif (thisyear in [2015, 2013, 2011]):\n",
    "    rawdatadir = yeardir + 'rawdata/group2/'\n",
    "elif (thisyear == 2014):\n",
    "    rawdatadir = yeardir + 'rawdata/tab4/sumfile/prod/2010thru2014/group2/'\n",
    "elif (thisyear == 2012):\n",
    "    rawdatadir = yeardir + 'rawdata/tab4/sumfile/prod/2008thru2012/group2/'\n",
    "else:\n",
    "    rawdatadir = yeardir + 'rawdata/'\n",
    "metadir = yeardir + 'metadata/'\n",
    "vardir = yeardir + 'variables/'\n",
    "geodir = yeardir + 'geography/'\n",
    "estimates_dir = yeardir + 'estimates/'\n",
    "margins_of_error_dir = yeardir + 'margins_of_error/'\n",
    "\n",
    "for x in [estimates_dir, margins_of_error_dir]:\n",
    "    if not(os.path.exists(x)):\n",
    "        os.makedirs(x)\n",
    "    if not(os.path.exists(x+'bg/')):\n",
    "        os.makedirs(x+'bg/')\n",
    "\n",
    "this_dir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/'\n",
    "extras_dir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "\n",
    "all_variables_df = pandas.read_csv(vardir+'variables_acs_5yr_all.csv', low_memory=False, index_col='rownumber')\n",
    "\n",
    "# pre-2017, description is blank for FILEID, FILETYPE, STUSAB, CHARITER, SEQUENCE, LOGRECNO - so fill in with blank\n",
    "all_variables_df = all_variables_df.fillna('')\n",
    "\n",
    "print('Done')\n",
    "\n",
    "#all_variables_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out which variables and sequences we need for our work\n",
    "\n",
    "See the code at the bottom of this file for how I got all the variables we are getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_variables_df[all_variables_df['description'].apply(lambda x: 'vacant' in x.lower())][0:10]\n",
    "\n",
    "# # for i in range(11,50):\n",
    "# #     print('\\'B01001_{0:03d}\\','.format(i))\n",
    "\n",
    "# all_variables_df[all_variables_df['variable'].isin(['B25002_001','B25002_002','B25002_003'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 12, 25, 33, 44, 53, 58, 95, 97, 99]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which variables are we getting?\n",
    "for_cra_analysis_mac_varlist = ['FILEID',\n",
    " 'FILETYPE',\n",
    " 'STUSAB',\n",
    " 'CHARITER',\n",
    " 'SEQUENCE',\n",
    " 'LOGRECNO',\n",
    "# 'B00001_001',\n",
    "# 'B00002_001',\n",
    " 'B01001_001',\n",
    " 'B02001_001',\n",
    " 'B02001_002',\n",
    " 'B02001_003',\n",
    " 'B08013_001',\n",
    " 'B11001_001',\n",
    " 'B11001_002',\n",
    " 'B11001_006',\n",
    " 'B11001_007',\n",
    " 'B11001A_001', \n",
    " 'B11001B_001',\n",
    "'B01001_011',\n",
    "'B01001_012',\n",
    "'B01001_013',\n",
    "'B01001_014',\n",
    "'B01001_015',\n",
    "'B01001_016',\n",
    "'B01001_017',\n",
    "'B01001_018',\n",
    "'B01001_019',\n",
    "'B01001_020',\n",
    "'B01001_021',\n",
    "'B01001_022',\n",
    "'B01001_023',\n",
    "'B01001_024',\n",
    "'B01001_025',\n",
    "'B01001_026',\n",
    "'B01001_027',\n",
    "'B01001_028',\n",
    "'B01001_029',\n",
    "'B01001_030',\n",
    "'B01001_031',\n",
    "'B01001_032',\n",
    "'B01001_033',\n",
    "'B01001_034',\n",
    "'B01001_035',\n",
    "'B01001_036',\n",
    "'B01001_037',\n",
    "'B01001_038',\n",
    "'B01001_039',\n",
    "'B01001_040',\n",
    "'B01001_041',\n",
    "'B01001_042',\n",
    "'B01001_043',\n",
    "'B01001_044',\n",
    "'B01001_045',\n",
    "'B01001_046',\n",
    "'B01001_047',\n",
    "'B01001_048',\n",
    "'B01001_049',                                                                \n",
    " 'B15003_001',\n",
    " 'B15003_002',\n",
    " 'B15003_003',\n",
    " 'B15003_004',\n",
    " 'B15003_005',\n",
    " 'B15003_006',\n",
    " 'B15003_007',\n",
    " 'B15003_008',\n",
    " 'B15003_009',\n",
    " 'B15003_010',\n",
    " 'B15003_011',\n",
    " 'B15003_012',\n",
    " 'B15003_013',\n",
    " 'B15003_014',\n",
    " 'B15003_015',\n",
    " 'B15003_016',\n",
    " 'B15003_017',\n",
    " 'B15003_018',\n",
    " 'B15003_019',\n",
    " 'B15003_020',\n",
    " 'B15003_021',\n",
    " 'B15003_022',\n",
    " 'B15003_023',\n",
    " 'B15003_024',\n",
    " 'B15003_025',\n",
    " 'B17001_001',\n",
    " 'B17001_002',\n",
    " 'B17001A_001',\n",
    " 'B17001A_002',\n",
    " 'B17001B_001',\n",
    " 'B17001B_002',\n",
    " 'B19013_001',\n",
    " 'B19013A_001',\n",
    " 'B19013B_001',\n",
    " 'B19113_001',\n",
    " 'B23025_002',\n",
    " 'B23025_005',\n",
    " 'B25002_001',\n",
    " 'B25002_002',\n",
    " 'B25002_003',\n",
    " 'B25003_001',\n",
    " 'B25003_002',\n",
    " 'B25003_003',\n",
    " 'B25003A_001',\n",
    " 'B25003A_002',\n",
    " 'B25003A_003',\n",
    " 'B25003B_001',\n",
    " 'B25003B_002',\n",
    " 'B25003B_003',\n",
    " 'B25077_001',\n",
    " 'B25034_001',\n",
    " 'B25035_001']\n",
    "\n",
    "# all_variables_df[all_variables_df['variable'].isin(for_cra_analysis_mac_varlist)]['description'][5:50]\n",
    "\n",
    "# Which sequences will we need?\n",
    "sequence_list = all_variables_df[all_variables_df['variable'].isin(for_cra_analysis_mac_varlist)]['sequence_number'].drop_duplicates().tolist()\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "sequence_list\n",
    "\n",
    "\n",
    "#for_cra_analysis_mac_varlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing estimates and margins of error for sequence 1...\n",
      "Importing estimates and margins of error for sequence 10...\n",
      "Importing estimates and margins of error for sequence 12...\n",
      "Importing estimates and margins of error for sequence 25...\n",
      "Importing estimates and margins of error for sequence 33...\n",
      "Importing estimates and margins of error for sequence 44...\n",
      "Importing estimates and margins of error for sequence 53...\n",
      "Importing estimates and margins of error for sequence 58...\n",
      "Importing estimates and margins of error for sequence 95...\n",
      "Importing estimates and margins of error for sequence 97...\n",
      "Importing estimates and margins of error for sequence 99...\n",
      "checking whether the dataset has the variables we need...\n",
      "keeping only the variables we need...\n",
      "backing up...\n",
      "\n",
      "Found estimates for 290,797 tracts and margins of error for 290,797 tracts in 28 minutes 57 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "state_codes_df = pandas.read_csv(extras_dir+'statecodes.csv')\n",
    "state_codes_df = state_codes_df.set_index('STUSAB')\n",
    "\n",
    "estimates_df = pandas.DataFrame()\n",
    "margins_of_error_df = pandas.DataFrame()\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = [x.lower() for x in states if x not in ('AS', 'GU', 'MP', 'PR', 'UM', 'VI')]\n",
    "\n",
    "for i in sequence_list:\n",
    "    if (debug >= 1):\n",
    "        print('Importing estimates and margins of error for sequence {0:,.0f}...'.format(i))\n",
    "    #print(all_variables_df[all_variables_df['sequence_number'] == i]['variable'].tolist()[0:10])\n",
    "    \n",
    "    this_seq_estimates_df = pandas.DataFrame()\n",
    "    this_seq_margins_of_error_df = pandas.DataFrame()\n",
    "    for onestate in states:\n",
    "#        if (debug >= 2):\n",
    "#            print('\\tImporting estimates and margins of error for {0:}...'.format(onestate))\n",
    "        state_estimates_filename = rawdatadir + 'e{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i)\n",
    "        state_margins_of_error_filename = rawdatadir + 'm{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i)\n",
    "#         if ('e{0:.0f}5{1:}{2:04d}000.txt'.format(thisyear, onestate, i) not in os.listdir(rawdatadir)):\n",
    "#             print('{0:} not found!'.format(state_estimates_filename))\n",
    "\n",
    "        onestate_estiamtes_df = pandas.read_csv(state_estimates_filename, header=None, sep=',', encoding='utf-8', low_memory=False)\n",
    "        this_seq_estimates_df = this_seq_estimates_df.append(onestate_estiamtes_df)\n",
    "        onestate_margins_of_error_df = pandas.read_csv(state_margins_of_error_filename, header=None, sep=',', encoding='utf-8', low_memory=False)\n",
    "        this_seq_margins_of_error_df = this_seq_margins_of_error_df.append(onestate_margins_of_error_df)\n",
    "    if (i >= 2):\n",
    "        #print(this_seq_estimates_df.columns.tolist())\n",
    "        this_seq_estimates_df = this_seq_estimates_df.drop([0,1,2,3,4,5], axis=1)  # ['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO']\n",
    "        this_seq_margins_of_error_df = this_seq_margins_of_error_df.drop([0,1,2,3,4,5], axis=1)  # ['FILEID','FILETYPE','STUSAB', 'SEQUENCE', 'CHARITER','LOGRECNO']\n",
    "\n",
    "    this_seq_estimates_df.columns = all_variables_df[all_variables_df['sequence_number'] == i]['variable'].tolist()\n",
    "    this_seq_margins_of_error_df.columns = all_variables_df[all_variables_df['sequence_number'] == i]['variable'].tolist()\n",
    "    \n",
    "    estimates_df = pandas.concat((estimates_df,this_seq_estimates_df), axis=1, sort=False)\n",
    "    margins_of_error_df = pandas.concat((margins_of_error_df,this_seq_margins_of_error_df), axis=1, sort=False)\n",
    "    \n",
    "print('checking whether the dataset has the variables we need...')\n",
    "\n",
    "#requested_variables = all_variables_df[all_variables_df['sequence_number'] == 1]['variable'].tolist()\n",
    "requested_variables = for_cra_analysis_mac_varlist\n",
    "\n",
    "variables_to_get = []\n",
    "variables_not_found = []\n",
    "for x in requested_variables:\n",
    "    if x in estimates_df.columns:\n",
    "        variables_to_get.append(x)\n",
    "    else:\n",
    "        variables_not_found.append(x)\n",
    "\n",
    "        \n",
    "# if (len(variables_not_found) > 0):\n",
    "#     print('Variables not found:')\n",
    "#     for y in variables_not_found:\n",
    "#         print('{0:}: {1:}\\n'.format(y, all_variables_df[all_variables_df['variable'] == y]['description'].values[0]))\n",
    "\n",
    "print('keeping only the variables we need...')\n",
    "estimates_df = estimates_df[variables_to_get]\n",
    "margins_of_error_df = margins_of_error_df[variables_to_get]\n",
    "\n",
    "#all_variables_df[all_variables_df['variable'].isin(variables_not_found)]['description']\n",
    "\n",
    "print('backing up...')\n",
    "estimates_df_bk = estimates_df\n",
    "margins_of_error_df_bk = margins_of_error_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('\\nFound estimates for {0:,.0f} tracts and margins of error for {1:,.0f} tracts in {2:,.0f} minutes {3:,.0f} seconds!'.format(len(estimates_df), len(margins_of_error_df), np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "\n",
    "\n",
    "#estimates_df['STUSAB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading geography files...\n",
      "Retaining only low-level geographies...\n",
      "Retained 73,057 tract level geographies...\n",
      "Retained 217,740 tract level geographies...\n",
      "backing up...\n",
      "Done in 1 minutes 1 seconds!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4627</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STATE</th>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOGRECNO</th>\n",
       "      <td>4628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOID</th>\n",
       "      <td>14000US01001020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOGRAPHY NAME</th>\n",
       "      <td>Census Tract 201, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUSAB</th>\n",
       "      <td>al</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     4627\n",
       "STATE                                                  AL\n",
       "LOGRECNO                                             4628\n",
       "GEOID                                  14000US01001020100\n",
       "GEOGRAPHY NAME  Census Tract 201, Autauga County, Alabama\n",
       "STUSAB                                                 al"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = time.time()\n",
    "debug = 1\n",
    "\n",
    "geo_df = pandas.DataFrame()\n",
    "\n",
    "states = state_codes_df.index.values.tolist()\n",
    "states = [x.lower() for x in states if x not in ('AS', 'GU', 'MP', 'PR', 'UM', 'VI')]\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('Reading geography files...')\n",
    "for onestate in states:\n",
    "    if (thisyear >= 2016):\n",
    "        filename = geodir+'{0:}.xlsx'.format(onestate)\n",
    "    else:\n",
    "        filename = geodir+'{0:}.xls'.format(onestate)\n",
    "    if (debug == 2):\n",
    "        print('Reading geography for {0:}...'.format(onestate))\n",
    "    this_geo_df = pandas.read_excel(filename)\n",
    "    geo_df = pandas.concat((geo_df, this_geo_df), sort=False)\n",
    "    if (thisyear in [2017, 2018]):\n",
    "        geo_df = geo_df.assign(STUSAB = geo_df['State'].apply(lambda x: x.lower()))\n",
    "    else:\n",
    "        geo_df = geo_df.assign(STUSAB = geo_df['STATE'].apply(lambda x: x.lower()))\n",
    "\n",
    "geo_df = geo_df.rename(columns={'Geography ID': 'GEOID'})\n",
    "\n",
    "print('Retaining only low-level geographies...')\n",
    "geo_tracts_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '140'))]\n",
    "if (debug >= 1):\n",
    "    print('Retained {0:,.0f} tract level geographies...'.format(len(geo_tracts_df)))\n",
    "\n",
    "geo_bg_df = geo_df[(geo_df['GEOID'].apply(lambda x: x[0:3] == '150'))]\n",
    "if (debug >= 1):\n",
    "    print('Retained {0:,.0f} tract level geographies...'.format(len(geo_bg_df)))\n",
    "\n",
    "print('backing up...')\n",
    "geo_tracts_df_bk = geo_tracts_df\n",
    "geo_bg_df_bk = geo_bg_df\n",
    "\n",
    "##geo_df = geo_df.set_index('GEOID')  #We'll set GEOID as index colum AFTER the merge\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done in {0:.0f} minutes {1:,.0f} seconds!'.format( np.floor((e-s)/60), (e-s) % 60))\n",
    "\n",
    "#geo_df.sample(1)\n",
    "#print('skipping...')\n",
    "geo_tracts_df.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join geography to estimates and margins of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting from backup...\n",
      "Total estimate by tract of B01001_001: 303,965,272\n",
      "Margin of error by tract in B01001_001: 28,178,850\n",
      "Total estimate by block group of B01001_001: 303,965,272\n",
      "Margin of error by tract in B01001_001: 69,710,489\n",
      "\n",
      "\n",
      "Writing data...\n",
      "Estimates by tract written to estimates_acs2010_tract_for_cra_analysis_mac.csv \n",
      "Margins of error by tract written to margin_of_error_acs2010_tract_for_cra_analysis_mac.csv\n",
      "Estimates by block group written to bg/estimates_acs2010_bg_for_cra_analysis_mac.csv \n",
      "Margins of error by block group written to bg/margin_of_error_acs2010_bg_for_cra_analysis_mac.csv\n",
      "\n",
      "Wrote out 73,057 tracts and 217,740 block groups in 30 minutes 30 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('getting from backup...')\n",
    "estimates_df = estimates_df_bk\n",
    "margins_of_error_df = margins_of_error_df_bk\n",
    "geo_tracts_df = geo_tracts_df_bk\n",
    "geo_bg_df = geo_bg_df_bk\n",
    "\n",
    "#.sort_values('Logical Record Number')\n",
    "#estimates_df[['STUSAB','LOGRECNO']].dtypes #object, int64\n",
    "#geo_df[['STUSAB', 'Logical Record Number']].dtypes\n",
    "\n",
    "if (thisyear in [2017, 2018]):\n",
    "    estimates_tracts_df = estimates_df.merge(geo_tracts_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "    margins_of_error_tracts_df = margins_of_error_df.merge(geo_tracts_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "\n",
    "    estimates_bg_df = estimates_df.merge(geo_bg_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "    margins_of_error_bg_df = margins_of_error_df.merge(geo_bg_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "    \n",
    "# elif (thisyear == 2016):\n",
    "#     estimates_df = estimates_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'Logical Record Number'])\n",
    "#     margins_of_error_df = margins_of_error_df.merge(geo_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "else:\n",
    "    estimates_tracts_df = estimates_df.merge(geo_tracts_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "    margins_of_error_tracts_df = margins_of_error_df.merge(geo_tracts_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "\n",
    "    estimates_bg_df = estimates_df.merge(geo_bg_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "    margins_of_error_bg_df = margins_of_error_df.merge(geo_bg_df, left_on=['STUSAB', 'LOGRECNO'], right_on=['STUSAB', 'LOGRECNO'])\n",
    "\n",
    "estimates_tracts_df = estimates_tracts_df.set_index('GEOID')  # set the GEOID after we add the shapefiles\n",
    "margins_of_error_tracts_df = margins_of_error_tracts_df.set_index('GEOID')  # set the GEOID after we add the shapefiles\n",
    "estimates_bg_df = estimates_bg_df.set_index('GEOID')\n",
    "margins_of_error_bg_df = margins_of_error_bg_df.set_index('GEOID')\n",
    "\n",
    "print('Total estimate by tract of B01001_001: {0:,.0f}'.format(estimates_tracts_df['B01001_001'].sum()))\n",
    "print('Margin of error by tract in B01001_001: {0:,.0f}'.format(margins_of_error_tracts_df['B01001_001'].sum()))\n",
    "\n",
    "print('Total estimate by block group of B01001_001: {0:,.0f}'.format(estimates_bg_df['B01001_001'].sum()))\n",
    "print('Margin of error by tract in B01001_001: {0:,.0f}'.format(margins_of_error_bg_df['B01001_001'].sum()))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Writing data...')\n",
    "\n",
    "estimates_tracts_df.to_csv(estimates_dir + 'estimates_acs{0:.0f}_tract_{1:}.csv'.format(thisyear, category), encoding='utf-8')\n",
    "print('Estimates by tract written to estimates_acs{0:.0f}_tract_{1:}.csv '.format(thisyear, category))\n",
    "      \n",
    "margins_of_error_tracts_df.to_csv(margins_of_error_dir + 'margins_of_error_acs{0:.0f}_tract_{1:}.csv'.format(thisyear, category), encoding='utf-8')\n",
    "print('Margins of error by tract written to margin_of_error_acs{0:.0f}_tract_{1:}.csv'.format(thisyear, category))\n",
    "\n",
    "estimates_bg_df.to_csv(estimates_dir + 'bg/estimates_acs{0:.0f}_bg_{1:}.csv'.format(thisyear, category), encoding='utf-8')\n",
    "print('Estimates by block group written to bg/estimates_acs{0:.0f}_bg_{1:}.csv '.format(thisyear, category))\n",
    "      \n",
    "margins_of_error_bg_df.to_csv(margins_of_error_dir + 'bg/margins_of_error_acs{0:.0f}_bg_{1:}.csv'.format(thisyear, category), encoding='utf-8')\n",
    "print('Margins of error by block group written to bg/margin_of_error_acs{0:.0f}_bg_{1:}.csv'.format(thisyear, category))\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('\\nWrote out {0:,.0f} tracts and {1:,.0f} block groups in {2:,.0f} minutes {3:,.0f} seconds!'.format(len(estimates_tracts_df), len(estimates_bg_df), np.floor((g)/60), np.floor((g)%60)))\n",
    "\n",
    "#margins_of_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILEID</th>\n",
       "      <th>FILETYPE</th>\n",
       "      <th>STUSAB</th>\n",
       "      <th>CHARITER</th>\n",
       "      <th>SEQUENCE</th>\n",
       "      <th>LOGRECNO</th>\n",
       "      <th>B01001_001</th>\n",
       "      <th>B02001_001</th>\n",
       "      <th>B02001_002</th>\n",
       "      <th>B02001_003</th>\n",
       "      <th>...</th>\n",
       "      <th>B25003A_002</th>\n",
       "      <th>B25003A_003</th>\n",
       "      <th>B25003B_001</th>\n",
       "      <th>B25003B_002</th>\n",
       "      <th>B25003B_003</th>\n",
       "      <th>B25077_001</th>\n",
       "      <th>B25034_001</th>\n",
       "      <th>B25035_001</th>\n",
       "      <th>STATE</th>\n",
       "      <th>GEOGRAPHY NAME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14000US01001020100</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4628</td>\n",
       "      <td>1809</td>\n",
       "      <td>1809</td>\n",
       "      <td>1424</td>\n",
       "      <td>293</td>\n",
       "      <td>...</td>\n",
       "      <td>505</td>\n",
       "      <td>59</td>\n",
       "      <td>111</td>\n",
       "      <td>72</td>\n",
       "      <td>39</td>\n",
       "      <td>120700</td>\n",
       "      <td>771</td>\n",
       "      <td>1976</td>\n",
       "      <td>AL</td>\n",
       "      <td>Census Tract 201, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US01001020200</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4629</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>777</td>\n",
       "      <td>1201</td>\n",
       "      <td>...</td>\n",
       "      <td>212</td>\n",
       "      <td>90</td>\n",
       "      <td>419</td>\n",
       "      <td>218</td>\n",
       "      <td>201</td>\n",
       "      <td>138500</td>\n",
       "      <td>816</td>\n",
       "      <td>1976</td>\n",
       "      <td>AL</td>\n",
       "      <td>Census Tract 202, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US01001020300</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4630</td>\n",
       "      <td>3543</td>\n",
       "      <td>3543</td>\n",
       "      <td>2945</td>\n",
       "      <td>588</td>\n",
       "      <td>...</td>\n",
       "      <td>834</td>\n",
       "      <td>251</td>\n",
       "      <td>192</td>\n",
       "      <td>113</td>\n",
       "      <td>79</td>\n",
       "      <td>111300</td>\n",
       "      <td>1403</td>\n",
       "      <td>1976</td>\n",
       "      <td>AL</td>\n",
       "      <td>Census Tract 203, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US01001020400</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4631</td>\n",
       "      <td>4840</td>\n",
       "      <td>4840</td>\n",
       "      <td>4607</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>1519</td>\n",
       "      <td>264</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>126300</td>\n",
       "      <td>1957</td>\n",
       "      <td>1969</td>\n",
       "      <td>AL</td>\n",
       "      <td>Census Tract 204, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US01001020500</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>al</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4632</td>\n",
       "      <td>9938</td>\n",
       "      <td>9938</td>\n",
       "      <td>8332</td>\n",
       "      <td>1167</td>\n",
       "      <td>...</td>\n",
       "      <td>2206</td>\n",
       "      <td>1011</td>\n",
       "      <td>375</td>\n",
       "      <td>136</td>\n",
       "      <td>239</td>\n",
       "      <td>173000</td>\n",
       "      <td>3969</td>\n",
       "      <td>1997</td>\n",
       "      <td>AL</td>\n",
       "      <td>Census Tract 205, Autauga County, Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US56043000200</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1025</td>\n",
       "      <td>3318</td>\n",
       "      <td>3318</td>\n",
       "      <td>3067</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>836</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180000</td>\n",
       "      <td>1416</td>\n",
       "      <td>1974</td>\n",
       "      <td>WY</td>\n",
       "      <td>Census Tract 2, Washakie County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US56043000301</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1026</td>\n",
       "      <td>2900</td>\n",
       "      <td>2900</td>\n",
       "      <td>2813</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>794</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85000</td>\n",
       "      <td>1388</td>\n",
       "      <td>1954</td>\n",
       "      <td>WY</td>\n",
       "      <td>Census Tract 3.01, Washakie County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US56043000302</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1027</td>\n",
       "      <td>2053</td>\n",
       "      <td>2053</td>\n",
       "      <td>1930</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>646</td>\n",
       "      <td>278</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>162900</td>\n",
       "      <td>1003</td>\n",
       "      <td>1975</td>\n",
       "      <td>WY</td>\n",
       "      <td>Census Tract 3.02, Washakie County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US56045951100</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1028</td>\n",
       "      <td>2945</td>\n",
       "      <td>2945</td>\n",
       "      <td>2800</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>956</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>131700</td>\n",
       "      <td>1437</td>\n",
       "      <td>1976</td>\n",
       "      <td>WY</td>\n",
       "      <td>Census Tract 9511, Weston County, Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000US56045951300</th>\n",
       "      <td>ACSSF</td>\n",
       "      <td>201000000.0</td>\n",
       "      <td>wy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>4121</td>\n",
       "      <td>4121</td>\n",
       "      <td>3896</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1362</td>\n",
       "      <td>378</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105100</td>\n",
       "      <td>2059</td>\n",
       "      <td>1959</td>\n",
       "      <td>WY</td>\n",
       "      <td>Census Tract 9513, Weston County, Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73057 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FILEID     FILETYPE STUSAB  CHARITER  SEQUENCE  LOGRECNO  \\\n",
       "GEOID                                                                         \n",
       "14000US01001020100  ACSSF  201000000.0     al         0         1      4628   \n",
       "14000US01001020200  ACSSF  201000000.0     al         0         1      4629   \n",
       "14000US01001020300  ACSSF  201000000.0     al         0         1      4630   \n",
       "14000US01001020400  ACSSF  201000000.0     al         0         1      4631   \n",
       "14000US01001020500  ACSSF  201000000.0     al         0         1      4632   \n",
       "...                   ...          ...    ...       ...       ...       ...   \n",
       "14000US56043000200  ACSSF  201000000.0     wy         0         1      1025   \n",
       "14000US56043000301  ACSSF  201000000.0     wy         0         1      1026   \n",
       "14000US56043000302  ACSSF  201000000.0     wy         0         1      1027   \n",
       "14000US56045951100  ACSSF  201000000.0     wy         0         1      1028   \n",
       "14000US56045951300  ACSSF  201000000.0     wy         0         1      1029   \n",
       "\n",
       "                    B01001_001  B02001_001  B02001_002  B02001_003  ...  \\\n",
       "GEOID                                                               ...   \n",
       "14000US01001020100        1809        1809        1424         293  ...   \n",
       "14000US01001020200        2020        2020         777        1201  ...   \n",
       "14000US01001020300        3543        3543        2945         588  ...   \n",
       "14000US01001020400        4840        4840        4607         112  ...   \n",
       "14000US01001020500        9938        9938        8332        1167  ...   \n",
       "...                        ...         ...         ...         ...  ...   \n",
       "14000US56043000200        3318        3318        3067          14  ...   \n",
       "14000US56043000301        2900        2900        2813           0  ...   \n",
       "14000US56043000302        2053        2053        1930          35  ...   \n",
       "14000US56045951100        2945        2945        2800           4  ...   \n",
       "14000US56045951300        4121        4121        3896           0  ...   \n",
       "\n",
       "                   B25003A_002  B25003A_003  B25003B_001  B25003B_002  \\\n",
       "GEOID                                                                   \n",
       "14000US01001020100         505           59          111           72   \n",
       "14000US01001020200         212           90          419          218   \n",
       "14000US01001020300         834          251          192          113   \n",
       "14000US01001020400        1519          264           32           16   \n",
       "14000US01001020500        2206         1011          375          136   \n",
       "...                        ...          ...          ...          ...   \n",
       "14000US56043000200         836          291            0            0   \n",
       "14000US56043000301         794          390            0            0   \n",
       "14000US56043000302         646          278            8            8   \n",
       "14000US56045951100         956          276            0            0   \n",
       "14000US56045951300        1362          378            0            0   \n",
       "\n",
       "                    B25003B_003  B25077_001  B25034_001  B25035_001  STATE  \\\n",
       "GEOID                                                                        \n",
       "14000US01001020100           39      120700         771        1976     AL   \n",
       "14000US01001020200          201      138500         816        1976     AL   \n",
       "14000US01001020300           79      111300        1403        1976     AL   \n",
       "14000US01001020400           16      126300        1957        1969     AL   \n",
       "14000US01001020500          239      173000        3969        1997     AL   \n",
       "...                         ...         ...         ...         ...    ...   \n",
       "14000US56043000200            0      180000        1416        1974     WY   \n",
       "14000US56043000301            0       85000        1388        1954     WY   \n",
       "14000US56043000302            0      162900        1003        1975     WY   \n",
       "14000US56045951100            0      131700        1437        1976     WY   \n",
       "14000US56045951300            0      105100        2059        1959     WY   \n",
       "\n",
       "                                                 GEOGRAPHY NAME  \n",
       "GEOID                                                            \n",
       "14000US01001020100    Census Tract 201, Autauga County, Alabama  \n",
       "14000US01001020200    Census Tract 202, Autauga County, Alabama  \n",
       "14000US01001020300    Census Tract 203, Autauga County, Alabama  \n",
       "14000US01001020400    Census Tract 204, Autauga County, Alabama  \n",
       "14000US01001020500    Census Tract 205, Autauga County, Alabama  \n",
       "...                                                         ...  \n",
       "14000US56043000200     Census Tract 2, Washakie County, Wyoming  \n",
       "14000US56043000301  Census Tract 3.01, Washakie County, Wyoming  \n",
       "14000US56043000302  Census Tract 3.02, Washakie County, Wyoming  \n",
       "14000US56045951100    Census Tract 9511, Weston County, Wyoming  \n",
       "14000US56045951300    Census Tract 9513, Weston County, Wyoming  \n",
       "\n",
       "[73057 rows x 83 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = pandas.read_csv(estimates_dir+'estimates_acs{0:}_tract_for_cra_analysis_mac.csv'.format(thisyear), encoding='utf-8', low_memory=False, index_col='GEOID')\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # thevars = pandas.\n",
    "# z.columns.tolist()\n",
    "# vars_df = pandas.read_csv(vardir+'variables_acs_5yr_all.csv', index_col='rownumber')\n",
    "\n",
    "# vars_df[\n",
    "#     vars_df['variable'].isin(z.columns.tolist())][['variable', 'description']\n",
    "#                                                  ].to_csv(this_dir+'cra_study_variables.csv')\n",
    "\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
