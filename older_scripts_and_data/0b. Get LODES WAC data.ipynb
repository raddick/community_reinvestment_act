{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "data_dir = '/home/idies/workspace/Temporary/raddick/cra_scratch_final/'\n",
    "jobs_dir = data_dir + 'lodes_wac/'\n",
    "baltimore_dir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/baltimore/'\n",
    "g = 0\n",
    "print('Done!')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = time.time()\n",
    "y = os.listdir(data_dir)\n",
    "hasdir = [x for x in y if x=='lodes_wac']\n",
    "if (len(hasdir) == 0):\n",
    "    os.mkdir(jobs_dir)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done in {0:,.3f} seconds!'.format(e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download WAC data from census.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = time.time()\n",
    "theurl = 'https://lehd.ces.census.gov/data/lodes/LODES7/'\n",
    "page = request.urlopen(theurl)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "page.close()\n",
    "\n",
    "bigtable = soup.find('table')\n",
    "allstaterows = bigtable.findAll('tr')\n",
    "#for i in range(7, 61):\n",
    "for i in range(59, 61):\n",
    "#for i in range(27, 28):\n",
    "    allstatecells = allstaterows[i].findAll('td')\n",
    "    this_state_url = theurl + allstatecells[1].find('a').get('href')+'wac/'\n",
    "    print(this_state_url)\n",
    "    #print(this_state_url)\n",
    "    statepage = request.urlopen(this_state_url)\n",
    "    statesoup = BeautifulSoup(statepage, 'html.parser')\n",
    "    statepage.close()\n",
    "    statetable = statesoup.find('table')\n",
    "    filerows = statetable.findAll('tr')\n",
    "    print('Parsing state {0:,.0f} of {1:,.0f}... Downloading {2:,.0f} files...'.format(i, 60, len(filerows)-4))\n",
    "    for j in range(3, len(filerows)-1):\n",
    "        filecells = filerows[j].findAll('td')\n",
    "        this_file_url = this_state_url + filecells[1].find('a').get('href')\n",
    "        this_file_name = jobs_dir + filecells[1].text\n",
    "        with open(this_file_name, 'wb') as f:\n",
    "            r = request.urlopen(this_file_url)\n",
    "            f.write(r.read())\n",
    "            r.close()\n",
    "print('done')            \n",
    "nFiles = len(os.listdir(jobs_dir))\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done: downloaded {0:,.0f} files in {1:,.0f} minutes {2:,.0f} seconds!'.format(nFiles, np.floor((e-s)/60), (e-s)%60))\n",
    "\n",
    "#filename = metadata_dir+'2010_5yr_Summary_FileTemplates.zip'\n",
    "#print('reading contents of gzip file...')\n",
    "#with gzip.open(filename, 'rb') as f:\n",
    "#    file_content = f.read()\n",
    "#print('Contents of gzip file have been read...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "s = time.time()\n",
    "#states = ['tn', 'tx', 'ut', 'va', 'vt']\n",
    "states = ['wa', 'wi', 'wv', 'wy']\n",
    "gzipfiles = []\n",
    "for this_state in states:\n",
    "    gzipfiles += [jobs_dir+x for x in os.listdir(jobs_dir) if ('{0:}_wac'.format(this_state) in x) and (x[-3:] == '.gz')]\n",
    "print('Found {0:,.0f} gzip files...'.format(len(gzipfiles)))\n",
    "\n",
    "for i in range(0, len(gzipfiles)):\n",
    "    if ((np.mod(i, 25) == 0) or (i == len(gzipfiles) - 1)):\n",
    "        print('Un-gzipping file {0:.0f} of {1:.0f}...'.format(i, len(gzipfiles)-1))\n",
    "    csvfilename = gzipfiles[i][:-3]\n",
    "    with gzip.open(gzipfiles[i], 'rb') as f:\n",
    "        file_content = f.read()\n",
    "    with open(csvfilename, 'wb') as f:\n",
    "        f.write(file_content)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Files unzipped in {0:,.0f} seconds!'.format(e-s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete .gz files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gzfiles = [jobs_dir+x for x in os.listdir(jobs_dir) if x[-3:] == '.gz']\n",
    "for y in gzfiles:\n",
    "    os.remove(y)\n",
    "print('Deleted all .gz files!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse all CSVs into a single file per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nm...\n",
      "Importing file 0 of 679...\n",
      "Importing file 25 of 679...\n",
      "Importing file 50 of 679...\n",
      "Importing file 75 of 679...\n",
      "Importing file 100 of 679...\n",
      "Importing file 125 of 679...\n",
      "Importing file 150 of 679...\n",
      "Importing file 175 of 679...\n",
      "Importing file 200 of 679...\n",
      "Importing file 225 of 679...\n",
      "Importing file 250 of 679...\n",
      "Importing file 275 of 679...\n",
      "Importing file 300 of 679...\n",
      "Importing file 325 of 679...\n",
      "Importing file 350 of 679...\n",
      "Importing file 375 of 679...\n",
      "Importing file 400 of 679...\n",
      "Importing file 425 of 679...\n",
      "Importing file 450 of 679...\n",
      "Importing file 475 of 679...\n",
      "Importing file 500 of 679...\n",
      "Importing file 525 of 679...\n",
      "Importing file 550 of 679...\n",
      "Importing file 575 of 679...\n",
      "Importing file 600 of 679...\n",
      "Importing file 625 of 679...\n",
      "Importing file 650 of 679...\n",
      "Importing file 675 of 679...\n",
      "Importing file 679 of 679...\n",
      "writing outfile...\n",
      "Wrote 5,980,252 rows in 30 minutes 47 seconds!\n"
     ]
    }
   ],
   "source": [
    "states = ['nm']\n",
    "s = time.time()\n",
    "for this_state in states:\n",
    "    print('Processing {0:}...'.format(this_state))\n",
    "    jobs_df = pandas.DataFrame()\n",
    "    csvfiles = [jobs_dir+x for x in os.listdir(jobs_dir) if ('{0:}_wac'.format(this_state) in x) and (x[-4:] == '.csv')]\n",
    "    if (this_state == 'in'):\n",
    "        startfile = 1\n",
    "    else:\n",
    "        startfile = 0\n",
    "    for i in range(startfile, len(csvfiles)):\n",
    "        if ((np.mod(i, 25) == 0) or (i == len(csvfiles) - 1)):\n",
    "            print('Importing file {0:,.0f} of {1:,.0f}...'.format(i, len(csvfiles)-1))\n",
    "        thisyear = int(csvfiles[i][-8:-4])\n",
    "        xdf = pandas.read_csv(csvfiles[i], low_memory=False)\n",
    "        xdf = xdf.assign(year = thisyear)\n",
    "        jobs_df = jobs_df.append(xdf)\n",
    "    jobs_df = jobs_df.reset_index(drop=True)\n",
    "    jobs_df.index.name = 'rownumber'\n",
    "    print('writing outfile...')\n",
    "    jobs_df.to_csv(jobs_dir+'jobs_data_{0:}.csv'.format(this_state), encoding='utf-8')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Wrote {0:,.0f} rows in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(jobs_df), np.floor((e-s)/60), (e-s) % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Baltimore data and write it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = time.time()\n",
    "df = pandas.read_csv(jobs_dir+'jobs_data_{0:}.csv'.format(this_state), encoding='utf-8', low_memory=False, index_col='rownumber')\n",
    "#df = pandas.read_csv(jobs_dir+'jobs_data_{0:}.csv'.format(this_state), encoding='utf-8', low_memory=False, index_col='rownumber')\n",
    "#baltimore_df = df[df['w_geocode'].apply(lambda x: str(x)[0:5] == '24510')]\n",
    "#baltimore_df.to_csv(baltimore_dir+'wac_jobs_df.csv', encoding='utf-8')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Read {0:,.0f} rows in {1:,.0f} seconds!'.format(len(df), e-s))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "s = time.time()\n",
    "jobs_meta_df = pandas.read_csv(data_dir+'lodes_wac_codebook.csv', encoding='utf-8', low_memory=False, index_col='varnum')\n",
    "jobs_meta_df.to_csv(baltimore_dir+'wac_jobs_metadata.csv', encoding='utf-8')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Wrote metadata for {0:,.0f} variables in {1:,.0f} seconds!'.format(len(jobs_meta_df), e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the individual CSV files that we combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 state jobs files!\n",
      "Will delete 33,620 files...\n",
      "Deleting...\n"
     ]
    }
   ],
   "source": [
    "jobs_files_list = sorted([x[-6:-4] for x in os.listdir(jobs_dir) if 'jobs_' in x])\n",
    "print('Found {0:.0f} state jobs files!'.format(len(jobs_files_list)))\n",
    "#pprint(jobs_files_list)\n",
    "files_to_delete = sorted([jobs_dir+y for y in os.listdir(jobs_dir) if ('.csv' in y) and ('jobs_' not in y)])\n",
    "print('Will delete {0:,.0f} files...'.format(len(files_to_delete)))\n",
    "print('Deleting...')\n",
    "for z in files_to_delete:\n",
    "    os.remove(z)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
