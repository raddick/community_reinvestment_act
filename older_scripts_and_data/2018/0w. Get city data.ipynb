{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n",
      "Now in directory: /home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "print('Importing packages...')\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import time\n",
    "#import zipfile\n",
    "import geopandas\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pandas.set_option('display.max_colwidth', -1)\n",
    "debug = 1\n",
    "current_year = 2017\n",
    "# Directories to look in\n",
    "thisdir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/'\n",
    "data_dir = '/home/idies/workspace/Temporary/raddick/cra_scratch_final/'\n",
    "jobs_dir = data_dir + 'lodes_wac/'\n",
    "census_dir = data_dir + 'acs5/'\n",
    "output_data_dir = thisdir + 'final_data/'\n",
    "\n",
    "census_shapefile_tiger_basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/shapefiles/'\n",
    "#shapefile_dir = census_shapefile_tiger_basedir + '{0:.0f}/TRACT/'.format(thisyear)\n",
    "\n",
    "#baltimore_dir = thisdir + 'baltimore/'\n",
    "\n",
    "code_lookup_dir = thisdir + 'code_guide_lookups/'\n",
    "inflation_dir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/datasets/inflation/'\n",
    "extrasdir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "\n",
    "city_data_dir = thisdir + 'city_data/'\n",
    "scale = 0.25\n",
    "g = 0 # global time\n",
    "\n",
    "os.chdir(thisdir)\n",
    "print('Now in directory: {0:}'.format(os.getcwd()))\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Baltimore!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "city = 'Baltimore'\n",
    "\n",
    "if (city == 'Baltimore'):\n",
    "    thestate = 24\n",
    "    state_abbrev = 'md'\n",
    "    citycode = 4000\n",
    "    baltimore_shapefile_dir = '/home/idies/workspace/Storage/raddick/Baltimore/shapefiles/'\n",
    "    plotlimits = {'N': 39.5, 'S': 39, 'E': -76, 'W': -77}\n",
    "\n",
    "elif (city == 'Washington DC'):\n",
    "    thestate = 11   # state_codes_df[state_codes_df['STATE_NAME'] == 'Missouri']\n",
    "    state_abbrev = 'dc'\n",
    "    citycode = 50000\n",
    "    plotlimits = {'N': 39.01, 'S': 38.79, 'E': -76.9, 'W': -77.13}\n",
    "\n",
    "elif (city == 'Detroit'):\n",
    "    thestate = 26   # state_codes_df[state_codes_df['STATE_NAME'] == 'Missouri']\n",
    "    state_abbrev = 'mi'\n",
    "    citycode = 22000\n",
    "\n",
    "elif (city == 'Newark'):\n",
    "    thestate = 34\n",
    "    state_abbrev = 'nj'\n",
    "    citycode = 51000\n",
    "    \n",
    "#elif (city == 'St. Louis'):\n",
    "#    thestate = 29   # state_codes_df[state_codes_df['STATE_NAME'] == 'Missouri']\n",
    "#    state_abbrev = 'mo'\n",
    "#    thecounty = 510\n",
    "#    cityname_file = 'st_louis'\n",
    "    \n",
    "#elif (city == 'Richmond'):\n",
    "#    thestate = 51   # state_codes_df[state_codes_df['STATE_NAME'] == 'Missouri']\n",
    "#    state_abbrev = 'va'\n",
    "#    thecounty = 760\n",
    "#    cityname_file = 'richmond'\n",
    "    \n",
    "#elif (city == 'San Francisco'):\n",
    "#    thestate = 6   # state_codes_df[state_codes_df['STATE_NAME'] == 'California']\n",
    "#    state_abbrev = 'ca'\n",
    "#    thecounty = 75\n",
    "#    cityname_file = 'san_francisco'\n",
    "    \n",
    "#elif (city == 'Philadelphia'):\n",
    "#    thestate = 42\n",
    "#    state_abbrev = 'pa'\n",
    "#    thecounty = 101\n",
    "#    cityname_file = 'philadelphia'\n",
    "\n",
    "elif (city == 'Cleveland'):\n",
    "    thestate = 39\n",
    "    state_abbrev = 'oh'\n",
    "    citycode = 16000\n",
    "    plotlimits = {'N': 41.65, 'S': 41.35, 'E': -81.5, 'W': -81.9}\n",
    "\n",
    "else:\n",
    "    print('ERROR: Select city from list!')\n",
    "\n",
    "cityname_file = city.lower().replace(' ','_')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Selected {0:}!'.format(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify tracts that belong to the city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do city borders line up with census tracts (in 2017)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 2017 census tracts...\n"
     ]
    },
    {
     "ename": "DriverError",
     "evalue": "/home/idies/workspace/Temporary/raddick/census_scratch/shapefiles/2017/TRACT/tl_2017_24_tract.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: /home/idies/workspace/Temporary/raddick/census_scratch/shapefiles/2017/TRACT/tl_2017_24_tract.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-36945b593947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthisyear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Parsing {0:.0f} census tracts...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisyear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstate_tracts_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcensus_shapefile_tiger_basedir\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'{0:.0f}/TRACT/tl_{0:.0f}_{1:02d}_tract.shp'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisyear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthestate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstate_places_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcensus_shapefile_tiger_basedir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'{0:.0f}/PLACE/tl_{0:.0f}_{1:.0f}_place.shp'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisyear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthestate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'   overlaying city on state census tract geometries...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, bbox, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 253\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: /home/idies/workspace/Temporary/raddick/census_scratch/shapefiles/2017/TRACT/tl_2017_24_tract.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "#years = np.arange(current_year, 2010, -1)\n",
    "city_tracts_years_gdf = geopandas.GeoDataFrame()\n",
    "years = np.arange(2017, 2016, -1)\n",
    "for thisyear in years:\n",
    "    print('Parsing {0:.0f} census tracts...'.format(thisyear))\n",
    "    state_tracts_gdf = geopandas.read_file(census_shapefile_tiger_basedir +'{0:.0f}/TRACT/tl_{0:.0f}_{1:02d}_tract.shp'.format(thisyear, thestate))\n",
    "    state_places_gdf = geopandas.read_file(census_shapefile_tiger_basedir + '{0:.0f}/PLACE/tl_{0:.0f}_{1:.0f}_place.shp'.format(thisyear, thestate))\n",
    "    print('   overlaying city on state census tract geometries...')\n",
    "    year_city_geo_gdf = state_places_gdf[state_places_gdf['GEOID'] == '{0:.0f}{1:05d}'.format(thestate, citycode)]\n",
    "    \n",
    "    year_city_geo_gdf = year_city_geo_gdf.to_crs(state_tracts_gdf.crs)\n",
    "    year_city_geo_gdf = year_city_geo_gdf.rename(columns={'NAME': 'city_name'})\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
    "state_tracts_gdf.plot(ax=ax, color='none', edgecolor='black')\n",
    "year_city_geo_gdf.plot(ax=ax, color='pink', alpha=0.7)\n",
    "#plotlimits = {'N': 39.01, 'S': 38.79, 'E': -76.9, 'W': -77.13}\n",
    "plt.xlim([plotlimits['W'], plotlimits['E']])\n",
    "plt.ylim([plotlimits['S'], plotlimits['N']])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get city census tracts for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 2017 census tracts...\n",
      "   overlaying city on state census tract geometries...\n",
      "    removing boundary tracts that had only tiny overlaps...\n",
      "Parsing 2016 census tracts...\n",
      "   overlaying city on state census tract geometries...\n",
      "    removing boundary tracts that had only tiny overlaps...\n",
      "Parsing 2015 census tracts...\n",
      "   overlaying city on state census tract geometries...\n",
      "    removing boundary tracts that had only tiny overlaps...\n",
      "Parsing 2014 census tracts...\n",
      "   overlaying city on state census tract geometries...\n",
      "    removing boundary tracts that had only tiny overlaps...\n",
      "Parsing 2013 census tracts...\n",
      "   overlaying city on state census tract geometries...\n",
      "    removing boundary tracts that had only tiny overlaps...\n",
      "Parsing 2012 census tracts...\n",
      "   overlaying city on state census tract geometries...\n",
      "    removing boundary tracts that had only tiny overlaps...\n",
      "Parsing 2011 census tracts...\n",
      "   overlaying city on state census tract geometries...\n",
      "    removing boundary tracts that had only tiny overlaps...\n",
      "converting columns and setting index...\n",
      "Read 1,400 tract-years in 0 minutes 31 seconds!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['STATEFP',\n",
       " 'TRACTCE',\n",
       " 'NAMELSAD',\n",
       " 'MTFCC',\n",
       " 'FUNCSTAT',\n",
       " 'ALAND',\n",
       " 'AWATER',\n",
       " 'INTPTLAT',\n",
       " 'INTPTLON',\n",
       " 'city_name',\n",
       " 'geometry']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = time.time()\n",
    "years = np.arange(current_year, 2010, -1)\n",
    "city_tracts_years_gdf = geopandas.GeoDataFrame()\n",
    "for thisyear in years:\n",
    "    print('Parsing {0:.0f} census tracts...'.format(thisyear))\n",
    "    state_tracts_gdf = geopandas.read_file(census_shapefile_tiger_basedir +'{0:.0f}/TRACT/tl_{0:.0f}_{1:02d}_tract.shp'.format(thisyear, thestate))\n",
    "    state_places_gdf = geopandas.read_file(census_shapefile_tiger_basedir + '{0:.0f}/PLACE/tl_{0:.0f}_{1:.0f}_place.shp'.format(thisyear, thestate))\n",
    "    print('   overlaying city on state census tract geometries...')\n",
    "    year_city_geo_gdf = state_places_gdf[state_places_gdf['GEOID'] == '{0:.0f}{1:05d}'.format(thestate, citycode)]\n",
    "    \n",
    "    year_city_geo_gdf = year_city_geo_gdf.to_crs(state_tracts_gdf.crs)\n",
    "    year_city_geo_gdf = year_city_geo_gdf.rename(columns={'NAME': 'city_name'})\n",
    "\n",
    "    year_city_tracts_gdf = geopandas.overlay(state_tracts_gdf, year_city_geo_gdf[['city_name', 'geometry']], how='intersection')\n",
    "    #year_city_tracts_gdf = year_city_tracts_gdf.assign(census_tract = year_city_tracts_gdf['TRACTCE'].apply(lambda x: str(x)[0:4]+'.'+str(x)[4:6]))\n",
    "    year_city_tracts_gdf = year_city_tracts_gdf.set_index('GEOID')\n",
    "    year_city_tracts_gdf = year_city_tracts_gdf.assign(year = thisyear)\n",
    "\n",
    "    print('    removing boundary tracts that had only tiny overlaps...')\n",
    "    year_city_tracts_gdf = year_city_tracts_gdf[year_city_tracts_gdf.geometry.area >= 1e-10]    \n",
    "\n",
    "    city_tracts_years_gdf = city_tracts_years_gdf.append(year_city_tracts_gdf)\n",
    "\n",
    "print('converting columns and setting index...')\n",
    "city_tracts_years_gdf.loc[:, 'COUNTYFP'] = pandas.to_numeric(city_tracts_years_gdf['COUNTYFP'], errors='coerce')\n",
    "city_tracts_years_gdf.loc[:, 'NAME'] = pandas.to_numeric(city_tracts_years_gdf['NAME'], errors='coerce')\n",
    "#city_tracts_years_gdf.loc[:, 'census_tract'] = pandas.to_numeric(city_tracts_years_gdf['census_tract'], errors='coerce')\n",
    "city_tracts_years_gdf.crs = state_tracts_gdf.crs\n",
    "\n",
    "need_counties_list = city_tracts_years_gdf['COUNTYFP'].drop_duplicates().tolist()\n",
    "need_tracts_list = city_tracts_years_gdf['NAME'].drop_duplicates().tolist()\n",
    "\n",
    "city_tracts_years_gdf = city_tracts_years_gdf.set_index(['COUNTYFP', 'NAME', 'year'])\n",
    "city_tracts_years_gdf = city_tracts_years_gdf.sort_index()\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Read {0:,.0f} tract-years in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(city_tracts_years_gdf), np.floor((e-s)/60), (e-s)%60))\n",
    "\n",
    "city_tracts_years_gdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the tract boundaries remain constant across all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baltimore covers 1 county...\n",
      "County 510 contains 1,400 tract-years:\n",
      "2017: 200 tracts\n",
      "2016: 200 tracts\n",
      "2015: 200 tracts\n",
      "2014: 200 tracts\n",
      "2013: 200 tracts\n",
      "2012: 200 tracts\n",
      "2011: 200 tracts\n",
      "Checked consistency of 1,400 tract-years in 0 minutes 1 seconds!\n"
     ]
    }
   ],
   "source": [
    "#city_tracts_years_gdf.reset_index()[city_tracts_years_gdf.reset_index()['year'] == 2017].plot()\n",
    "s = time.time()\n",
    "counties_list = city_tracts_years_gdf.index.get_level_values('COUNTYFP').drop_duplicates().tolist()\n",
    "\n",
    "print('{0:} covers {1:,.0f} county...'.format(city, len(counties_list)))\n",
    "\n",
    "for x in counties_list:\n",
    "    print('County {0:,.0f} contains {1:,.0f} tract-years:'.format(x, len(city_tracts_years_gdf.reset_index()[city_tracts_years_gdf.reset_index()['COUNTYFP'] == x])))\n",
    "    for i in years:\n",
    "        print('{0:.0f}: {1:,.0f} tracts'.format(\n",
    "            i, \n",
    "            len(city_tracts_years_gdf.reset_index()[\n",
    "                (city_tracts_years_gdf.reset_index()['COUNTYFP'] == x) \n",
    "                & (city_tracts_years_gdf.reset_index()['year'] == i)])\n",
    "        ))\n",
    "        census_tracts_2017_list = city_tracts_years_gdf.xs(2017, level=-1).reset_index()[\n",
    "            city_tracts_years_gdf.xs(2017, level=-1).reset_index()['COUNTYFP'] == x].set_index('NAME').index.values.tolist()\n",
    "\n",
    "        county_x_year_i_census_tracts_list = city_tracts_years_gdf.reset_index()[\n",
    "                (city_tracts_years_gdf.reset_index()['COUNTYFP'] == x) \n",
    "                & (city_tracts_years_gdf.reset_index()['year'] == i)\n",
    "        ]['NAME'].tolist()\n",
    "        \n",
    "        new_census_tracts = []\n",
    "        missing_census_tracts = []\n",
    "        \n",
    "        for n in county_x_year_i_census_tracts_list:\n",
    "            if (n not in census_tracts_2017_list):\n",
    "                new_census_tracts.append(n)\n",
    "            for p in census_tracts_2017_list:\n",
    "                if (p not in county_x_year_i_census_tracts_list):\n",
    "                    missing_census_tracts.append(n)\n",
    "        for y in new_census_tracts:\n",
    "            print('Found census tract in {0:.0f} that is not present in 2017: {1:}'.format(i,y))\n",
    "        for z in missing_census_tracts:\n",
    "            print('Found census tract from 2017 that was not present in {0:.0f}: {1:}'.format(i,z))\n",
    "        if (len(new_census_tracts) + len(missing_census_tracts) > 0):\n",
    "            print('\\n')\n",
    "\n",
    "#fig, ax = plt.subplots(1,1, figsize=(48*scale, 48*scale))\n",
    "#city_tracts_years_gdf.xs(2012, level=-1).plot(ax=ax, color='blue', edgecolor='white')\n",
    "##city_tracts_years_gdf.loc[[2012, 35, 1711.03]].geometry.plot()#.plot(color='red')\n",
    "#for ix, thisrow in city_tracts_years_gdf.xs(2012, level=-1).iterrows():\n",
    "#    if (ix[1] in new_census_tracts):\n",
    "#        annotator = ix[1]\n",
    "#        ax.annotate(annotator, \n",
    "#                    xy=(thisrow.geometry.centroid.x, thisrow.geometry.centroid.y), \n",
    "#                    xytext=(thisrow.geometry.centroid.x,#+0.01*np.random.rand(), \n",
    "#                            thisrow.geometry.centroid.y),#+0.01*np.random.rand()), \n",
    "#                    backgroundcolor = 'white', horizontalalignment='center', verticalalignment='center',\n",
    "#                    fontsize=24*scale)\n",
    "#plt.show()\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Checked consistency of {0:,.0f} tract-years in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(city_tracts_years_gdf), np.floor((e-s)/60), (e-s)%60))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get geo-aggregated loans by census tract and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading nationwide data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idies/miniconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2,645,400 nationwide tract-years...\n",
      "getting city data...\n",
      "Found 1,381 tract-years with business loans originated in Baltimore in 0 minutes 24 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('reading nationwide data...')\n",
    "agg_loans_nationwide_df = pandas.read_csv(data_dir+'agg_loans.csv', encoding='utf-8', low_memory=False, index_col='rownumber')\n",
    "print('Read {0:,.0f} nationwide tract-years...'.format(len(agg_loans_nationwide_df), e-s))\n",
    "\n",
    "#print('selecting originated business loans since 2011 in {0:}...'.format(city))\n",
    "# Keep only business loans\n",
    "agg_loans_nationwide_df = agg_loans_nationwide_df[agg_loans_nationwide_df['loan_type'] == 4]\n",
    "# Keep only loan originations\n",
    "agg_loans_nationwide_df = agg_loans_nationwide_df[agg_loans_nationwide_df['action_taken_type'] == 1]\n",
    "# Keep only this state\n",
    "agg_loans_nationwide_df = agg_loans_nationwide_df[agg_loans_nationwide_df['state'] == thestate]\n",
    "\n",
    "print('getting city data...')\n",
    "city_loans_df = pandas.DataFrame()\n",
    "for oneyear in years:\n",
    "    #print('{0:.0f}: found {1:,.0f} originated business loans nationwide...'.format(oneyear, len(agg_loans_nationwide_df[agg_loans_nationwide_df['activity_year'] == oneyear])))\n",
    "    for onecounty in need_counties_list:\n",
    "        for onetract in need_tracts_list:\n",
    "            city_loans_df = city_loans_df.append(agg_loans_nationwide_df[(agg_loans_nationwide_df['activity_year'] == oneyear) & (agg_loans_nationwide_df['county'] == onecounty) & (agg_loans_nationwide_df['census_tract'] == onetract)])            \n",
    "\n",
    "city_loans_df = city_loans_df.set_index(['activity_year', 'county', 'census_tract'])\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Found {0:,.0f} tract-years with business loans originated in {1:} in {2:,.0f} minutes {3:,.0f} seconds!'.format(len(city_loans_df), city, np.floor((e-s)/60), (e-s)%60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge loan data into tract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added loan data for 1,400 tract-years in 0 seconds.\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "data_gdf = city_tracts_years_gdf.reset_index().merge(city_loans_df.reset_index(), how='left', \n",
    "                                       left_on=['COUNTYFP', 'NAME', 'year'], \n",
    "                                        right_on=['county', 'census_tract', 'activity_year'],\n",
    "                                        suffixes=('_tracts', '_loans')).set_index(['COUNTYFP', 'NAME', 'year'])\n",
    "\n",
    "\n",
    "\n",
    "#data_gdf.columns#[data_gdf['amtLoans1'].isnull()]\n",
    "numeric_columns = []\n",
    "numeric_columns += ['nLoans1', 'amtLoans1', 'nLoans100k', 'amtLoans100k']\n",
    "numeric_columns += ['nLoans250k', 'amtLoans250k', 'nLoansToSmallest', 'amtLoansToSmallest']\n",
    "\n",
    "for thiscol in numeric_columns:\n",
    "    data_gdf.loc[:, thiscol] = data_gdf[thiscol].fillna(0)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Added loan data for {0:,.0f} tract-years in {1:,.0f} seconds.'.format(len(data_gdf), e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add income groups, CRA levels, loan totals for each tract-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking up income group names from income_group_total...\n",
      "Adding CRA income levels (low/moderate/middle/upper/unknown)...\n",
      "Getting CRA income levels for tracts where only CRA level was reported...\n",
      "calculating total loans...\n",
      "calculating working loans...\n",
      "Kept 1,400 tract-years in Baltimore in 0.57 seconds!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>STATEFP</th>\n",
       "      <th>TRACTCE</th>\n",
       "      <th>NAMELSAD</th>\n",
       "      <th>MTFCC</th>\n",
       "      <th>FUNCSTAT</th>\n",
       "      <th>ALAND</th>\n",
       "      <th>AWATER</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>city_name</th>\n",
       "      <th>...</th>\n",
       "      <th>amtLoans250k</th>\n",
       "      <th>nLoansToSmallest</th>\n",
       "      <th>amtLoansToSmallest</th>\n",
       "      <th>income_group</th>\n",
       "      <th>cra_level</th>\n",
       "      <th>nLoans</th>\n",
       "      <th>amtLoans</th>\n",
       "      <th>avgSmallLoan</th>\n",
       "      <th>nWorkingLoans</th>\n",
       "      <th>amtWorkingLoans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTYFP</th>\n",
       "      <th>NAME</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">510</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">101.0</th>\n",
       "      <th>2011</th>\n",
       "      <td>24</td>\n",
       "      <td>010100</td>\n",
       "      <td>Census Tract 101</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>393197</td>\n",
       "      <td>148387</td>\n",
       "      <td>+39.2787586</td>\n",
       "      <td>-076.5742651</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>774000.0</td>\n",
       "      <td>90% to 100% of MFI</td>\n",
       "      <td>middle</td>\n",
       "      <td>56.0</td>\n",
       "      <td>891000.0</td>\n",
       "      <td>9833.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>360000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>24</td>\n",
       "      <td>010100</td>\n",
       "      <td>Census Tract 101</td>\n",
       "      <td>G5020</td>\n",
       "      <td>S</td>\n",
       "      <td>393197</td>\n",
       "      <td>148387</td>\n",
       "      <td>+39.2787586</td>\n",
       "      <td>-076.5742651</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>...</td>\n",
       "      <td>1468000.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>775000.0</td>\n",
       "      <td>90% to 100% of MFI</td>\n",
       "      <td>middle</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2181000.0</td>\n",
       "      <td>11918.367347</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2181000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    STATEFP TRACTCE          NAMELSAD  MTFCC FUNCSTAT   ALAND  \\\n",
       "COUNTYFP NAME  year                                                             \n",
       "510      101.0 2011  24      010100  Census Tract 101  G5020  S        393197   \n",
       "               2012  24      010100  Census Tract 101  G5020  S        393197   \n",
       "\n",
       "                     AWATER     INTPTLAT      INTPTLON  city_name  \\\n",
       "COUNTYFP NAME  year                                                 \n",
       "510      101.0 2011  148387  +39.2787586  -076.5742651  Baltimore   \n",
       "               2012  148387  +39.2787586  -076.5742651  Baltimore   \n",
       "\n",
       "                          ...       amtLoans250k  nLoansToSmallest  \\\n",
       "COUNTYFP NAME  year       ...                                        \n",
       "510      101.0 2011       ...        0.0          43.0               \n",
       "               2012       ...        1468000.0    28.0               \n",
       "\n",
       "                     amtLoansToSmallest        income_group  cra_level  \\\n",
       "COUNTYFP NAME  year                                                      \n",
       "510      101.0 2011  774000.0            90% to 100% of MFI  middle      \n",
       "               2012  775000.0            90% to 100% of MFI  middle      \n",
       "\n",
       "                     nLoans   amtLoans  avgSmallLoan nWorkingLoans  \\\n",
       "COUNTYFP NAME  year                                                  \n",
       "510      101.0 2011  56.0    891000.0   9833.333333   2.0            \n",
       "               2012  53.0    2181000.0  11918.367347  53.0           \n",
       "\n",
       "                    amtWorkingLoans  \n",
       "COUNTYFP NAME  year                  \n",
       "510      101.0 2011  360000.0        \n",
       "               2012  2181000.0       \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "print('looking up income group names from income_group_total...')\n",
    "\n",
    "data_gdf = data_gdf.rename(columns = {'income_group_total': 'income_group_code'})\n",
    "data_gdf = data_gdf.assign(income_group = np.nan)\n",
    "\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 1, 'income_group'] = '< 10% of Median Family Income (MFI)'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 2, 'income_group'] = '10% to 20% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 3, 'income_group'] = '20% to 30% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 4, 'income_group'] = '30% to 40% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 5, 'income_group'] = '40% to 50% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 6, 'income_group'] = '50% to 60% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 7, 'income_group'] = '60% to 70% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 8, 'income_group'] = '70% to 80% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 9, 'income_group'] = '80% to 90% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 10, 'income_group'] = '90% to 100% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 11, 'income_group'] = '100% to 110% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 12, 'income_group'] = '110% to 120% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 13, 'income_group'] = '> 120% of MFI'\n",
    "\n",
    "print('Adding CRA income levels (low/moderate/middle/upper/unknown)...')\n",
    "# Get levels (low, moderate, middle, upper)\n",
    "data_gdf = data_gdf.assign(cra_level = np.nan)\n",
    "data_gdf.loc[(data_gdf['income_group_code'] >= 1) & (data_gdf['income_group_code'] <= 5), 'cra_level'] = 'low'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] >= 6) & (data_gdf['income_group_code'] <= 8), 'cra_level'] = 'moderate'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] >= 9) & (data_gdf['income_group_code'] <= 12), 'cra_level'] = 'middle'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] == 13), 'cra_level'] = 'upper'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] == 14), 'cra_level'] = 'unknown'\n",
    "\n",
    "print('Getting CRA income levels for tracts where only CRA level was reported...')\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 101, 'cra_level'] = 'low'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 102, 'cra_level'] = 'moderate'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 103, 'cra_level'] = 'middle'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 104, 'cra_level'] = 'upper'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 105, 'cra_level'] = 'unknown'\n",
    "\n",
    "\n",
    "print('calculating total loans...')\n",
    "data_gdf = data_gdf.assign(nLoans = data_gdf['nLoans1'] + data_gdf['nLoans100k'] + data_gdf['nLoans250k'])\n",
    "data_gdf = data_gdf.assign(amtLoans = data_gdf['amtLoans1'] + data_gdf['amtLoans100k'] + data_gdf['amtLoans250k'])\n",
    "\n",
    "print('calculating working loans...')\n",
    "data_gdf = data_gdf.assign(avgSmallLoan = data_gdf['amtLoans1'] / data_gdf['nLoans1'])\n",
    "\n",
    "data_gdf = data_gdf.assign(nWorkingLoans = 0)\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] < 10000, \n",
    "                           'nWorkingLoans'] = data_gdf['nLoans'][data_gdf['avgSmallLoan'] < 10000] - data_gdf['nLoans1'][data_gdf['avgSmallLoan'] < 10000]\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] >= 10000, \n",
    "                           'nWorkingLoans'] = data_gdf['nLoans'][data_gdf['avgSmallLoan'] >= 10000]\n",
    "\n",
    "data_gdf = data_gdf.assign(amtWorkingLoans = 0)\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] < 10000, \n",
    "                           'amtWorkingLoans'] = data_gdf['amtLoans'][data_gdf['avgSmallLoan'] < 10000] - data_gdf['amtLoans1'][data_gdf['avgSmallLoan'] < 10000]\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] >= 10000, \n",
    "                           'amtWorkingLoans'] = data_gdf['amtLoans'][data_gdf['avgSmallLoan'] >= 10000]\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Kept {0:,.0f} tract-years in {1:} in {2:,.2f} seconds!'.format(len(data_gdf), city, e-s))\n",
    "data_gdf.groupby('cra_level').size()\n",
    "data_gdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's connect geographic aggregates to other geographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get 2011-2015 jobs data for this state (all block groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading state jobs data for MD...\n",
      "keeping only 2011-2015 jobs...\n",
      "Keeping only Baltimore counties and census tracts...\n",
      "Kept 520,776 rows in 3 minutes 7 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "writefilename = city_data_dir+'{0:}_jobs_2015.csv'.format(cityname_file)\n",
    "\n",
    "print('reading state jobs data for {0:}...'.format(state_abbrev.upper()))\n",
    "\n",
    "if (state_abbrev in ['ca', 'ny', 'fl']):\n",
    "    statejobfiles = sorted([jobs_dir+x for x in os.listdir(jobs_dir) if '{0:}'.format(state_abbrev) in x and '{0:}.'.format(state_abbrev) not in x])\n",
    "    #print(statejobfiles)\n",
    "    city_raw_jobs_df = pandas.DataFrame()\n",
    "    for thisfile in statejobfiles:\n",
    "        print('Reading {0:}...'.format(thisfile))\n",
    "        zdf = pandas.read_csv(thisfile, index_col='rownumber', low_memory=False)\n",
    "        #print(df['w_geocode'].head(1).apply(lambda x: str(x)[1:4]))\n",
    "        city_raw_jobs_df = city_raw_jobs_df.append(zdf[zdf['w_geocode'].apply(lambda x: str(x)[1:4] == '{0:03d}'.format(thecounty))])\n",
    "else:\n",
    "    state_raw_jobs_df = pandas.read_csv(jobs_dir+'jobs_data_{0:}.csv'.format(state_abbrev), index_col='rownumber')\n",
    "\n",
    "print('keeping only 2011-2015 jobs...')\n",
    "state_raw_jobs_df = state_raw_jobs_df[state_raw_jobs_df['year'] >= 2011]\n",
    "\n",
    "\n",
    "print('Keeping only {0:} counties and census tracts...'.format(city))\n",
    "if (thestate < 10):\n",
    "    state_raw_jobs_df = state_raw_jobs_df.assign(county= pandas.to_numeric(state_raw_jobs_df['w_geocode'].apply(lambda x: str(x)[1:4]), errors='coerce'))\n",
    "    state_raw_jobs_df = state_raw_jobs_df.assign(census_tract = pandas.to_numeric(state_raw_jobs_df['w_geocode'].apply(lambda x: str(x)[4:8] + '.' + str(x)[8:10]), errors='coerce'))\n",
    "else:\n",
    "    state_raw_jobs_df = state_raw_jobs_df.assign(county = pandas.to_numeric(state_raw_jobs_df['w_geocode'].apply(lambda x: str(x)[2:5]), errors='coerce'))\n",
    "    state_raw_jobs_df = state_raw_jobs_df.assign(census_tract = pandas.to_numeric(state_raw_jobs_df['w_geocode'].apply(lambda x: str(x)[5:9] + '.' + str(x)[9:11]), errors='coerce'))\n",
    "\n",
    "city_raw_jobs_df = state_raw_jobs_df[\n",
    "    (state_raw_jobs_df['county'].isin(need_counties_list))\n",
    "    & (state_raw_jobs_df['census_tract'].isin(need_tracts_list))\n",
    "]\n",
    "\n",
    "#print('Writing out 2015 jobs data...')\n",
    "#city_raw_jobs_df[city_raw_jobs_df['year'] == 2015].to_csv(writefilename)\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Kept {0:,.0f} rows in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(city_raw_jobs_df), np.floor((e-s)/60), (e-s) % 60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum jobs over census tracts for city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "sum_columns = []\n",
    "sum_columns += ['C000', 'CA01', 'CA02', 'CA03', 'CE01', 'CE02', 'CE03']\n",
    "sum_columns += ['CNS01', 'CNS02', 'CNS03', 'CNS04', 'CNS05', 'CNS06', 'CNS07', 'CNS08']\n",
    "sum_columns += ['CNS09', 'CNS10', 'CNS11', 'CNS12', 'CNS13', 'CNS14', 'CNS15', 'CNS16']\n",
    "sum_columns += ['CNS17', 'CNS18', 'CNS19', 'CNS20', 'CR01', 'CR02', 'CR03', 'CR04']\n",
    "sum_columns += ['CR05', 'CR07', 'CT01', 'CT02', 'CD01', 'CD02', 'CD03', 'CD04', 'CS01']\n",
    "sum_columns += ['CS02', 'CFA01', 'CFA02', 'CFA03', 'CFA04', 'CFA05', 'CFS01', 'CFS02']\n",
    "sum_columns += ['CFS03', 'CFS04', 'CFS05']\n",
    "\n",
    "city_jobs_df = city_raw_jobs_df.groupby(['year', 'county', 'census_tract'])[sum_columns].sum()\n",
    "#state_jobs_df\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')\n",
    "\n",
    "#city_raw_jobs_df.columns#.groupby(['county', 'census_tract']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy 2015 jobs data to 2016 and 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting 2016 and 2017 jobs to 2015 values...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('setting 2016 and 2017 jobs to 2015 values...')\n",
    "\n",
    "n16df = pandas.DataFrame(data=None, columns=sum_columns, index=city_jobs_df.xs(2015).index)\n",
    "for thiscounty in n16df.index.unique(level='county').tolist():\n",
    "    for thistract in n16df.index.unique(level='census_tract').tolist():\n",
    "        n16df.loc[(thiscounty, thistract)] = city_jobs_df.xs(2015).loc[(thiscounty, thistract)]\n",
    "n16df = n16df.assign(year = 2016)\n",
    "n16df = n16df.reset_index().set_index(['year', 'county', 'census_tract'])\n",
    "\n",
    "n17df = pandas.DataFrame(data=None, columns=sum_columns, index=city_jobs_df.xs(2015).index)\n",
    "for thiscounty in n17df.index.unique(level='county').tolist():\n",
    "    for thistract in n17df.index.unique(level='census_tract').tolist():\n",
    "        n17df.loc[(thiscounty, thistract)] = city_jobs_df.xs(2015).loc[(thiscounty, thistract)]\n",
    "n17df = n17df.assign(year = 2017)\n",
    "n17df = n17df.reset_index().set_index(['year', 'county', 'census_tract'])\n",
    "\n",
    "\n",
    "city_jobs_df = city_jobs_df.append(n16df)\n",
    "city_jobs_df = city_jobs_df.append(n17df)\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which jobs columns do we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable: C000\t\tdescription:Total number of jobs\n",
      "variable: CFS01\t\tdescription:Number of jobs for workers at firms with Firm Size: 0-19 Employees\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "jobs_metadata_df = pandas.read_csv(code_lookup_dir+'wac_jobs_metadata.csv', encoding='utf-8', index_col='varnum')\n",
    "jobs_metadata_df = jobs_metadata_df.set_index('variable')\n",
    "\n",
    "jobs_columns = ['C000', 'CA01', 'CA02', 'CA03', 'CE01', 'CE02', 'CE03', 'CNS01']\n",
    "jobs_columns += ['CNS02', 'CNS03', 'CNS04', 'CNS05', 'CNS06', 'CNS07', 'CNS08']\n",
    "jobs_columns += ['CNS09', 'CNS10', 'CNS11', 'CNS12', 'CNS13', 'CNS14', 'CNS15']\n",
    "jobs_columns += ['CNS16', 'CNS17', 'CNS18', 'CNS19', 'CNS20', 'CR01', 'CR02']\n",
    "jobs_columns += ['CR03', 'CR04', 'CR05', 'CR07', 'CT01', 'CT02', 'CD01', 'CD02']\n",
    "jobs_columns += ['CD03', 'CD04', 'CS01', 'CS02', 'CFA01', 'CFA02', 'CFA03']\n",
    "jobs_columns += ['CFA04', 'CFA05', 'CFS01', 'CFS02', 'CFS03', 'CFS04', 'CFS05']\n",
    "\n",
    "#for x in baltimore_agg_loans_df[jobs_columns].columns:\n",
    "#    print('variable: {0:}\\t\\tdescription:{1:}'.format(x, jobs_metadata_df['description'][jobs_metadata_df.index == x].tolist()[0]))\n",
    "\n",
    "jobs_columns_we_want = ['C000', 'CFS01']\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "for x in city_jobs_df[jobs_columns_we_want].columns:\n",
    "    print('variable: {0:}\\t\\tdescription:{1:}'.format(x, jobs_metadata_df['description'][jobs_metadata_df.index == x].tolist()[0]))\n",
    "#city_jobs_df.index.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge jobs data into the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added jobs data for 1,400 tract-years!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "data_gdf = data_gdf.reset_index().merge(city_jobs_df.reset_index(), how='left', \n",
    "                                               left_on=['COUNTYFP', 'NAME', 'year'], \n",
    "                                                right_on=['county', 'census_tract', 'year'],\n",
    "                                                suffixes=('_tracts', '_jobs')).set_index(['COUNTYFP', 'NAME', 'year'])\n",
    "\n",
    "for x in jobs_columns:\n",
    "    data_gdf.loc[:, x] = data_gdf[x].fillna(0)\n",
    "    \n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Added jobs data for {0:,.0f} tract-years!'.format(len(data_gdf[data_gdf['CFS01'].notnull()])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get loans per job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calulating loans per job (total and with firm size 0-19)...\n",
      "recoding infinite values to NaN...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "#sbjobs_column = jobs_varnames_df[jobs_varnames_df['description'].apply(lambda x: '0-19' in x)].index.values[0]\n",
    "#loans_columns = []\n",
    "#data_gdf[sbjobs_column]\n",
    "print('Calulating loans per job (total and with firm size 0-19)...')\n",
    "\n",
    "data_gdf = data_gdf.assign(nLoans1_per_totaljob = data_gdf['nLoans1'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(amtLoans1_per_totaljob = data_gdf['amtLoans1'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(nLoans100k_per_totaljob = data_gdf['nLoans100k'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(amtLoans100k_per_totaljob = data_gdf['amtLoans100k'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(nLoans250k_per_totaljob = data_gdf['nLoans250k'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(amtLoans250k_per_totaljob = data_gdf['amtLoans250k'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(nLoansToSmallest_per_totaljob = data_gdf['nLoansToSmallest'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(amtLoansToSmallest_per_totaljob = data_gdf['amtLoansToSmallest'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(nLoans_per_totaljob = data_gdf['nLoans'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(amtLoans_per_totaljob = data_gdf['amtLoans'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(nWorkingLoans_per_totaljob = data_gdf['nWorkingLoans'] / data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(amtWorkingLoans_per_totaljob = data_gdf['amtWorkingLoans'] / data_gdf['C000'])\n",
    "\n",
    "data_gdf = data_gdf.assign(nLoans1_per_sbjob = data_gdf['nLoans1'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(amtLoans1_per_sbjob = data_gdf['amtLoans1'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(nLoans100k_per_sbjob = data_gdf['nLoans100k'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(amtLoans100k_per_sbjob = data_gdf['amtLoans100k'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(nLoans250k_per_sbjob = data_gdf['nLoans250k'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(amtLoans250k_per_sbjob = data_gdf['amtLoans250k'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(nLoansToSmallest_per_sbjob = data_gdf['nLoansToSmallest'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(amtLoansToSmallest_per_sbjob = data_gdf['amtLoansToSmallest'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(nLoans_per_sbjob = data_gdf['nLoans'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(amtLoans_per_sbjob = data_gdf['amtLoans'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(nWorkingLoans_per_sbjob = data_gdf['nWorkingLoans'] / data_gdf['CFS01'])\n",
    "data_gdf = data_gdf.assign(amtWorkingLoans_per_sbjob = data_gdf['amtWorkingLoans'] / data_gdf['CFS01'])\n",
    "\n",
    "print('recoding infinite values to NaN...')\n",
    "\n",
    "per_job_columns = ['nLoans1_per_totaljob', 'amtLoans1_per_totaljob', 'nLoans100k_per_totaljob']\n",
    "per_job_columns += ['amtLoans100k_per_totaljob', 'nLoans250k_per_totaljob', 'amtLoans250k_per_totaljob']\n",
    "per_job_columns += ['nLoansToSmallest_per_totaljob', 'amtLoansToSmallest_per_totaljob']\n",
    "per_job_columns += ['nLoans_per_totaljob', 'amtLoans_per_totaljob', 'nWorkingLoans_per_totaljob']\n",
    "per_job_columns += ['amtWorkingLoans_per_totaljob', 'nLoans1_per_sbjob', 'amtLoans1_per_sbjob']\n",
    "per_job_columns += ['nLoans100k_per_sbjob', 'amtLoans100k_per_sbjob', 'nLoans250k_per_sbjob']\n",
    "per_job_columns += ['amtLoans250k_per_sbjob', 'nLoansToSmallest_per_sbjob', 'amtLoansToSmallest_per_sbjob']\n",
    "per_job_columns += ['nLoans_per_sbjob', 'amtLoans_per_sbjob', 'nWorkingLoans_per_sbjob']\n",
    "per_job_columns += ['amtWorkingLoans_per_sbjob']\n",
    "\n",
    "for x in data_gdf[per_job_columns]:\n",
    "    data_gdf.loc[data_gdf[x] == np.inf, x] = np.nan\n",
    "    \n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done!')\n",
    "#data_gdf.sample(2).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ACS 5-year census data...\n",
      "keeping 2011-2017...\n",
      "discarding block groups, keeping census tracts...\n",
      "finding county and census tract values...\n",
      "selecting only Baltimore counties and tracts...\n",
      "merging with loan and job data...\n",
      "Added census data for 1,400 tract-years in 3 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "print('Getting ACS 5-year census data...')\n",
    "state_acs5_df = pandas.DataFrame()\n",
    "datafiles = [census_dir+x for x in os.listdir(census_dir) if (('tracts' in x) and ('{0:}'.format(state_abbrev) in x) and ('alldata' not in x))]\n",
    "for thisfile in datafiles:\n",
    "    xdf = pandas.read_csv(thisfile, encoding='utf-8', low_memory=False)\n",
    "    xdf = xdf.assign(year = int(thisfile[-10:-6]))\n",
    "    state_acs5_df = state_acs5_df.append(xdf)\n",
    "\n",
    "column_order = state_acs5_df.columns[0:1].tolist()\n",
    "column_order += state_acs5_df.columns[-1:].tolist()\n",
    "column_order += state_acs5_df.columns[1:-1].tolist()\n",
    "\n",
    "state_acs5_df = state_acs5_df[column_order]\n",
    "\n",
    "print('keeping 2011-2017...')\n",
    "state_acs5_df = state_acs5_df[state_acs5_df['year'] >= 2011]\n",
    "\n",
    "print('discarding block groups, keeping census tracts...')\n",
    "state_acs5_df = state_acs5_df[state_acs5_df['GEOID'].apply(lambda x: x[0:3] == '140')]\n",
    "\n",
    "print('finding county and census tract values...')\n",
    "state_acs5_df = state_acs5_df.assign(county = pandas.to_numeric(state_acs5_df['GEOID'].apply(lambda x: x[9:12]), errors='coerce'))\n",
    "state_acs5_df = state_acs5_df.assign(census_tract = pandas.to_numeric(state_acs5_df['GEOID'].apply(lambda x: x[12:16] + '.' + x[16:18]), errors='coerce'))\n",
    "\n",
    "print('selecting only {0:} counties and tracts...'.format(city))\n",
    "city_acs5_df = state_acs5_df[\n",
    "    (state_acs5_df['county'].isin(need_counties_list))\n",
    "    & (state_acs5_df['census_tract'].isin(need_tracts_list))\n",
    "]\n",
    "\n",
    "city_acs5_df = city_acs5_df.set_index(['year', 'county', 'census_tract'])\n",
    "\n",
    "print('merging with loan and job data...')\n",
    "\n",
    "data_gdf = data_gdf.reset_index().merge(city_acs5_df.reset_index(), how='left', \n",
    "                                               left_on=['COUNTYFP', 'NAME', 'year'], \n",
    "                                                right_on=['county', 'census_tract', 'year'],\n",
    "                                                suffixes=('_tracts', '_acs5')).set_index(['COUNTYFP', 'NAME', 'year'])\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Added census data for {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(data_gdf), e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate composite demographic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "calculating and renaming estimates columns for IVs...\n",
      "...high school graduates or higher 25 years and older...\n",
      "...householder sex & race, unempoyment, poverty, home value, home age...\n",
      "...total householders...\n",
      "...race, owner-occupied units, mfi...\n",
      "....comparison variables: total population, total households, poverty status...\n",
      "MFI & median home value: substituting \".\" with np.nan, converting to numeric...\n",
      "...population 25plus...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('\\ncalculating and renaming estimates columns for IVs...')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...high school graduates or higher 25 years and older...')\n",
    "h = data_gdf['B15002_011'] + data_gdf['B15002_012'] + data_gdf['B15002_013'] \n",
    "h += data_gdf['B15002_014'] + data_gdf['B15002_015'] + data_gdf['B15002_016']\n",
    "h += data_gdf['B15002_017'] + data_gdf['B15002_018']\n",
    "h += data_gdf['B15002_028'] + data_gdf['B15002_029'] + data_gdf['B15002_030'] \n",
    "h += data_gdf['B15002_031'] + data_gdf['B15002_032'] + data_gdf['B15002_033'] \n",
    "h += data_gdf['B15002_034'] + data_gdf['B15002_035']\n",
    "data_gdf = data_gdf.assign(hs_grad_25plus = pandas.to_numeric(h, errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...householder sex & race, unempoyment, poverty, home value, home age...')\n",
    "data_gdf = data_gdf.rename(columns = {     \n",
    "    'B11001_006': 'female_householder',\n",
    "    'B11001A_001': 'white_householder',\n",
    "    'B11001B_001': 'black_householder',\n",
    "    'B23025_005': 'unemployed_16plus',\n",
    "    'B17001_002': 'poverty_past_12_months',\n",
    "    'B25077_001': 'median_home_value',\n",
    "    'B25035_001': 'median_year_built'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...total householders...')\n",
    "data_gdf = data_gdf.assign(total_householders = pandas.to_numeric(\n",
    "                                             (data_gdf['B11001_002'] + data_gdf['B11001_007']\n",
    "                                             ), errors='coerce'\n",
    "                                         )\n",
    "                                        )\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...race, owner-occupied units, mfi...')\n",
    "data_gdf = data_gdf.rename(columns = {\n",
    "    'B02001_002': 'pop_white',\n",
    "    'B02001_003': 'pop_black',\n",
    "    'B25003_002': 'owner_occ_housing_units',\n",
    "    'B19113_001': 'mfi'    \n",
    "})\n",
    "if (debug >= 1):\n",
    "    print('....comparison variables: total population, total households, poverty status...')\n",
    "data_gdf = data_gdf.rename(columns = {\n",
    "    'B01001_001': 'pop_total',\n",
    "    'B23025_002': 'labor_force_16plus',\n",
    "    'B17001_001': 'poverty_status_known'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('MFI & median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "data_gdf.loc[data_gdf['mfi'] == '.', 'mfi'] = pandas.to_numeric(data_gdf['mfi'][data_gdf['mfi'] == '.'], errors='coerce')\n",
    "data_gdf.loc[data_gdf['median_home_value'] == '.', 'median_home_value'] = pandas.to_numeric(data_gdf['median_home_value'][data_gdf['median_home_value'] == '.'], errors='coerce')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...population 25plus...')\n",
    "data_gdf = data_gdf.assign(pop_25plus = pandas.to_numeric(\n",
    "                                             (data_gdf['B01001_011'] + data_gdf['B01001_012'] + data_gdf['B01001_013'] \n",
    "                                              + data_gdf['B01001_014'] + data_gdf['B01001_015'] + data_gdf['B01001_016']\n",
    "                                              + data_gdf['B01001_017'] + data_gdf['B01001_018'] + data_gdf['B01001_019']\n",
    "                                              + data_gdf['B01001_020'] + data_gdf['B01001_021'] + data_gdf['B01001_022']\n",
    "                                              + data_gdf['B01001_023'] + data_gdf['B01001_024'] + data_gdf['B01001_025']\n",
    "                                              + data_gdf['B01001_035'] + data_gdf['B01001_036'] + data_gdf['B01001_037']\n",
    "                                              + data_gdf['B01001_038'] + data_gdf['B01001_039'] + data_gdf['B01001_040']\n",
    "                                              + data_gdf['B01001_041'] + data_gdf['B01001_042'] + data_gdf['B01001_043']\n",
    "                                              + data_gdf['B01001_044'] + data_gdf['B01001_045'] + data_gdf['B01001_046']\n",
    "                                              + data_gdf['B01001_047'] + data_gdf['B01001_048'] + data_gdf['B01001_049']\n",
    "                                             ), errors='coerce'\n",
    "                                         )\n",
    "                                        )\n",
    "#data_gdf.sample(1).T\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done!')\n",
    "#data_gdf[['mfi', 'median_home_value']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get errors for composite columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined standard-error-calculating functions!\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "### Guide on how to calculate errors in percentages:\n",
    "# https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch08.pdf\n",
    "    \n",
    "## Aggregating Data Across Population Subgroups: add error for each group in quadrature, divide by 1.645 for serr\n",
    "s = time.time()\n",
    "\n",
    "def find_serr_hsgrad25plus(row):\n",
    "    return pandas.to_numeric(np.sqrt(row['B15002_011_err']**2 + row['B15002_012_err']**2 + row['B15002_013_err']**2 \n",
    "                                 + row['B15002_014_err']**2 + row['B15002_015_err']**2 + row['B15002_016_err']**2 \n",
    "                                 + row['B15002_017_err']**2 + row['B15002_018_err']**2 + \n",
    "                                 + row['B15002_028_err']**2 + row['B15002_029_err']**2 + row['B15002_030_err']**2 \n",
    "                                 + row['B15002_031_err']**2 + row['B15002_032_err']**2 + row['B15002_033_err']**2 \n",
    "                                 + row['B15002_034_err']**2 + row['B15002_035_err']**2\n",
    "                                ) / 1.645, errors='coerce')\n",
    "\n",
    "def find_serr_householders(row):\n",
    "    return pandas.to_numeric(np.sqrt(row['B11001_002_err']**2 + row['B11001_007_err']**2 \n",
    "                                ) / 1.645, errors='coerce')\n",
    "\n",
    "def find_serr_pop25plus(row):\n",
    "    return pandas.to_numeric(np.sqrt(row['B01001_011_err']**2 + row['B01001_012_err']**2 + row['B01001_013_err']**2 \n",
    "                                     + row['B01001_014_err']**2 + row['B01001_015_err']**2 + row['B01001_016_err']**2 \n",
    "                                     + row['B01001_017_err']**2 + row['B01001_018_err']**2 + row['B01001_019_err']**2 \n",
    "                                     + row['B01001_020_err']**2 + row['B01001_021_err']**2 + row['B01001_022_err']**2 \n",
    "                                     + row['B01001_023_err']**2 + row['B01001_024_err']**2 + row['B01001_025_err']**2 \n",
    "                                     + row['B01001_035_err']**2 + row['B01001_036_err']**2 + row['B01001_037_err']**2 \n",
    "                                     + row['B01001_038_err']**2 + row['B01001_039_err']**2 + row['B01001_040_err']**2 \n",
    "                                     + row['B01001_041_err']**2 + row['B01001_042_err']**2 + row['B01001_043_err']**2 \n",
    "                                     + row['B01001_044_err']**2 + row['B01001_045_err']**2 + row['B01001_046_err']**2 \n",
    "                                     + row['B01001_047_err']**2 + row['B01001_048_err']**2 + row['B01001_049_err']**2 \n",
    "                                    ) / 1.645, errors='coerce')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Defined standard-error-calculating functions!')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "calculating and renaming margins of error columns for IVs...\n",
      "...margins for race, owner-occupied units, mfi...\n",
      "...standard errors for hs graduates 25 and older (using custom serr-finding function...\n",
      "...margins of error for householder sex & race, unempoyment, poverty, home value, home age...\n",
      "MFI & median home value: substituting \".\" with np.nan, converting to numeric...\n",
      "\n",
      "calculating and renaming margins of error for comparison variables...\n",
      "...race, owner-occupied units, mfi...\n",
      "...population 25plus...\n",
      "...total householders...\n",
      "...labor force, poverty status known...\n",
      "dropping columns we do not care about...\n",
      "Calculated errors for all columns!\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('\\ncalculating and renaming margins of error columns for IVs...')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...margins for race, owner-occupied units, mfi...')\n",
    "data_gdf = data_gdf.rename(columns = {\n",
    "    'B02001_002_err': 'pop_white_err',\n",
    "    'B02001_003_err': 'pop_black_err',\n",
    "    'B25003_002_err': 'owner_occ_housing_units_err',\n",
    "    'B19113_001_err': 'mfi_err'    \n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...standard errors for hs graduates 25 and older (using custom serr-finding function...')\n",
    "data_gdf = data_gdf.assign(hs_grad_25plus_serr = pandas.to_numeric(data_gdf.apply(lambda row: find_serr_hsgrad25plus(row), axis=1), errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...margins of error for householder sex & race, unempoyment, poverty, home value, home age...')\n",
    "data_gdf = data_gdf.rename(columns = {     \n",
    "    'B11001_006_err': 'female_householder_err',\n",
    "    'B11001A_001_err': 'black_householder_err',\n",
    "    'B11001B_001_err': 'white_householder_err',\n",
    "    'B23025_005_err': 'unemployed_16plus_err',\n",
    "    'B17001_002_err': 'poverty_past_12_months_err',\n",
    "    'B25077_001_err': 'median_home_value_err',\n",
    "    'B25035_001_err': 'median_year_built_err'\n",
    "})\n",
    "\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('MFI & median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "data_gdf.loc[data_gdf['mfi_err'] == '.', 'mfi_err'] = pandas.to_numeric(data_gdf['mfi_err'][data_gdf['mfi_err'] == '.'], errors='coerce')\n",
    "data_gdf.loc[data_gdf['median_home_value_err'] == '.', 'median_home_value_err'] = pandas.to_numeric(data_gdf['median_home_value_err'][data_gdf['median_home_value_err'] == '.'], errors='coerce')\n",
    "\n",
    "\n",
    "print('\\ncalculating and renaming margins of error for comparison variables...')\n",
    "if (debug >= 1):\n",
    "    print('...race, owner-occupied units, mfi...')\n",
    "data_gdf = data_gdf.rename(columns = {\n",
    "    'B01001_001_err': 'pop_total_err',\n",
    "    'B17001_001_err': 'poverty_status_known_err'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...population 25plus...')\n",
    "data_gdf = data_gdf.assign(pop_25plus_serr = pandas.to_numeric(data_gdf.apply(lambda row: find_serr_pop25plus(row), axis=1), errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...total householders...')\n",
    "data_gdf = data_gdf.assign(total_householders_serr = pandas.to_numeric(data_gdf.apply(lambda row: find_serr_householders(row), axis=1), errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...labor force, poverty status known...')\n",
    "data_gdf = data_gdf.rename(columns = {\n",
    "    'B23025_002_err': 'labor_force_16plus_err',\n",
    "    'B17001_001_err': 'poverty_status_known_err'\n",
    "})\n",
    "\n",
    "\n",
    "print('dropping columns we do not care about...')\n",
    "columns_do_not_care = ['B15002_011','B15002_012','B15002_013','B15002_014','B15002_015']\n",
    "columns_do_not_care += ['B15002_016','B15002_017','B15002_018','B15002_028','B15002_029']\n",
    "columns_do_not_care += ['B15002_030','B15002_031','B15002_032','B15002_033','B15002_034']\n",
    "columns_do_not_care += ['B15002_035','B01001_011','B01001_012','B01001_013','B01001_014']\n",
    "columns_do_not_care += ['B01001_015','B01001_016','B01001_017','B01001_018','B01001_019']\n",
    "columns_do_not_care += ['B01001_020','B01001_021','B01001_022','B01001_023','B01001_024']\n",
    "columns_do_not_care += ['B01001_025','B01001_035','B01001_036','B01001_037','B01001_038']\n",
    "columns_do_not_care += ['B01001_039','B01001_040','B01001_041','B01001_042','B01001_043']\n",
    "columns_do_not_care += ['B01001_044','B01001_045','B01001_046','B01001_047','B01001_048']\n",
    "columns_do_not_care += ['B01001_049', 'B11001_002', 'B11001_007']\n",
    "columns_do_not_care += ['B15002_011_err','B15002_012_err','B15002_013_err']\n",
    "columns_do_not_care += ['B15002_014_err','B15002_015_err','B15002_016_err']\n",
    "columns_do_not_care += ['B15002_017_err','B15002_018_err','B15002_028_err']\n",
    "columns_do_not_care += ['B15002_029_err','B15002_030_err','B15002_031_err']\n",
    "columns_do_not_care += ['B15002_032_err','B15002_033_err','B15002_034_err']\n",
    "columns_do_not_care += ['B15002_035_err','B15002_011_err','B15002_012_err']\n",
    "columns_do_not_care += ['B15002_013_err','B15002_014_err','B15002_015_err']\n",
    "columns_do_not_care += ['B15002_016_err','B15002_017_err','B15002_018_err']\n",
    "columns_do_not_care += ['B15002_028_err','B15002_029_err','B15002_030_err']\n",
    "columns_do_not_care += ['B15002_031_err','B15002_032_err','B15002_033_err']\n",
    "columns_do_not_care += ['B15002_034_err','B15002_035_err','B01001_011_err']\n",
    "columns_do_not_care += ['B01001_012_err','B01001_013_err','B01001_014_err']\n",
    "columns_do_not_care += ['B01001_015_err','B01001_016_err','B01001_017_err']\n",
    "columns_do_not_care += ['B01001_018_err','B01001_019_err','B01001_020_err']\n",
    "columns_do_not_care += ['B01001_021_err','B01001_022_err','B01001_023_err']\n",
    "columns_do_not_care += ['B01001_024_err','B01001_025_err','B01001_035_err']\n",
    "columns_do_not_care += ['B01001_036_err','B01001_037_err','B01001_038_err']\n",
    "columns_do_not_care += ['B01001_039_err','B01001_040_err','B01001_041_err']\n",
    "columns_do_not_care += ['B01001_042_err','B01001_043_err','B01001_044_err']\n",
    "columns_do_not_care += ['B01001_045_err','B01001_046_err','B01001_047_err']\n",
    "columns_do_not_care += ['B11001_002_err','B11001_007_err']\n",
    "\n",
    "columns_do_not_care += ['B01001_048_err','B01001_049_err','STATE']\n",
    "data_gdf = data_gdf.drop(columns_do_not_care, axis=1)\n",
    "\n",
    "\n",
    "print('Calculated errors for all columns!')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "#data_gdf.sample(2)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "vars_for_percentification = ['pop_white', 'pop_black', 'black_householder', 'white_householder']\n",
    "vars_for_percentification += ['owner_occ_housing_units', 'hs_grad_25plus', 'female_householder']\n",
    "vars_for_percentification += ['unemployed_16plus', 'poverty_past_12_months']\n",
    "\n",
    "vars_for_percentification += ['pop_white_err', 'pop_black_err', 'black_householder_err', 'white_householder_err']\n",
    "vars_for_percentification += ['owner_occ_housing_units_err', 'hs_grad_25plus_serr', 'female_householder_err']\n",
    "vars_for_percentification += ['unemployed_16plus_err', 'poverty_past_12_months_err']\n",
    "\n",
    "vars_for_percentification += ['pop_total', 'total_householders', 'pop_25plus', 'labor_force_16plus']\n",
    "vars_for_percentification += ['poverty_status_known']\n",
    "\n",
    "vars_for_percentification += ['pop_total_err', 'total_householders_serr', 'pop_25plus_serr', 'labor_force_16plus_err']\n",
    "vars_for_percentification += ['poverty_status_known_err']\n",
    "#vars_for_percentification\n",
    "#city_tracts_years_df[vars_for_percentification].columns.tolist()\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate percentages for needed demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "#[x for x in vars_for_percentification if \"_err\" not in x]\n",
    "\n",
    "data_gdf = data_gdf.assign(pct_white = pandas.to_numeric((data_gdf['pop_white'] / data_gdf['pop_total']), errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_black = pandas.to_numeric((data_gdf['pop_black'] / data_gdf['pop_total']), errors='coerce'))\n",
    "\n",
    "data_gdf = data_gdf.assign(pct_white_householders = pandas.to_numeric((data_gdf['white_householder'] / data_gdf['total_householders']), errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_black_householders = pandas.to_numeric((data_gdf['black_householder'] / data_gdf['total_householders']), errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_female_householders = pandas.to_numeric((data_gdf['female_householder'] / data_gdf['total_householders']), errors='coerce'))\n",
    "\n",
    "data_gdf = data_gdf.assign(pct_hs_grad = pandas.to_numeric(data_gdf['hs_grad_25plus'], errors='coerce') / pandas.to_numeric(data_gdf['pop_25plus'], errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_unemployed = pandas.to_numeric(data_gdf['unemployed_16plus'], errors='coerce') / pandas.to_numeric(data_gdf['labor_force_16plus'], errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_poverty = pandas.to_numeric(data_gdf['poverty_past_12_months'], errors='coerce') / pandas.to_numeric(data_gdf['poverty_status_known'], errors='coerce'))\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')\n",
    "#data_gdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate errors in percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined functions to calculate standard errors in percentages!\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#Guide on how to do this:\n",
    "#### https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch08.pdf\n",
    "\n",
    "# X and Y are the measured values (not the errors) - X for the subsgroup and Y for the whole sample\n",
    "# Let P = X/Y  (the proportion we calculated in the last step)\n",
    "# dX and dY are the measured errors\n",
    "# dP = (1/Y) * np.sqrt(dX**2 - (P**2 * dY**2))\n",
    "# Standard error of P is dP/1.645\n",
    "#### this calculation is done verbosely in fnid_pop_white_serr, quickly in other functions\n",
    "\n",
    "s = time.time()\n",
    "def find_errors_in_pct(X, Y, dX, dY, verboselevel = 0):\n",
    "    try:\n",
    "        P = X / Y\n",
    "        oneoverY = 1 / Y\n",
    "        dXsq = dX**2\n",
    "        dYsq = dY**2\n",
    "        Psq = P**2\n",
    "        PsqdYsq = Psq * dYsq\n",
    "        if (PsqdYsq <= dXsq):\n",
    "            underroot = dXsq - PsqdYsq\n",
    "        else:\n",
    "            underroot = dXsq + PsqdYsq\n",
    "        rooty = np.sqrt(underroot)\n",
    "        dP = oneoverY * rooty\n",
    "        SE = dP / 1.645\n",
    "        if (verboselevel >= 2):\n",
    "#            print('X = pop_white, Y = pop_total')\n",
    "            print('X = {0:.0f}, dX = {1:.0f} ({2:.1%} error)'.format(X, dX, dX/X))\n",
    "            print('Y = {0:.0f}, dY = {1:.0f} ({2:.1%} error)'.format(Y, dY, dY/Y))\n",
    "        if (verboselevel >= 3):\n",
    "            print('P = {0:.3f}'.format(P))\n",
    "            print('dXsq = {0:.0f}, dYsq = {1:.0f}, Psq = {2:.3f}'.format(dXsq, dYsq, Psq))\n",
    "            print('PsqdYsq = {0:.0f}, underroot = {1:.0f}, rooty = {2:.3f}'.format(PsqdYsq, underroot, rooty))\n",
    "            print('dP = {0:.3f}'.format(dP))\n",
    "            print('SE = {0:.3f}'.format(SE))\n",
    "        if (verboselevel >= 2):\n",
    "            print('RESULT: {0:.2%} +/- {1:.2%}'.format(P, SE)) \n",
    "            print('\\n')\n",
    "        return pandas.to_numeric(SE, errors='coerce')\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "    \n",
    "e = time.time()\n",
    "g = g + (e-s)    \n",
    "print('Defined functions to calculate standard errors in percentages!')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate errors in percentage values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating errors in percentages...\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "verboselevel = 0\n",
    "s = time.time()\n",
    "\n",
    "print('Calculating errors in percentages...')\n",
    "data_gdf = data_gdf.assign(pct_white_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_black_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_white_householders_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_black_householders_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_female_householders_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_hs_grad_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_unemployed_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_poverty_serr = np.nan)\n",
    "\n",
    "#data_gdf.loc[:, \n",
    "#              'poverty_status_known_last12months_total_err'] = pandas.to_numeric(data_gdf['poverty_status_known_last12months_total_err'], errors='coerce')\n",
    "\n",
    "\n",
    "for ix, thisrow in data_gdf.iterrows():\n",
    "    if (verboselevel >= 2):\n",
    "        print('Census tract {0:}...'.format(ix))\n",
    "    #print('pct_white_serr...')\n",
    "    data_gdf.loc[ix, 'pct_white_serr'] = find_errors_in_pct(thisrow['pop_white'], thisrow['pop_total'], thisrow['pop_white_err'], thisrow['pop_total_err'], verboselevel)\n",
    "    #print('pct_black_serr...')\n",
    "    data_gdf.loc[ix, 'pct_black_serr'] = find_errors_in_pct(thisrow['pop_black'], thisrow['pop_total'], thisrow['pop_black_err'], thisrow['pop_total_err'], verboselevel)\n",
    "    #print('pct_white_householders_serr...')\n",
    "    data_gdf.loc[ix, 'pct_white_householders_serr'] = find_errors_in_pct(thisrow['white_householder'], thisrow['total_householders'], thisrow['white_householder_err'], thisrow['total_householders_serr'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_black_householders_serr'] = find_errors_in_pct(thisrow['white_householder'], thisrow['total_householders'], thisrow['white_householder_err'], thisrow['total_householders_serr'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_female_householders_serr'] = find_errors_in_pct(thisrow['white_householder'], thisrow['total_householders'], thisrow['white_householder_err'], thisrow['total_householders_serr'], verboselevel)\n",
    "    \n",
    "    data_gdf.loc[ix, 'pct_hs_grad_serr'] = find_errors_in_pct(thisrow['hs_grad_25plus'], thisrow['pop_25plus'], thisrow['hs_grad_25plus_serr'], thisrow['pop_25plus_serr'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_unemployed_serr'] = find_errors_in_pct(thisrow['unemployed_16plus'], thisrow['labor_force_16plus'], thisrow['hs_grad_25plus_serr'], thisrow['labor_force_16plus_err'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_poverty_serr'] = find_errors_in_pct(thisrow['poverty_past_12_months'], thisrow['poverty_status_known'], thisrow['poverty_past_12_months_err'], thisrow['poverty_status_known_err'], verboselevel)\n",
    "\n",
    "if (verboselevel >= 1):\n",
    "    for ix, thisrow in data_gdf.iterrows():\n",
    "        print('Census tract {0:,.0f}'.format(ix))\n",
    "        print('{0:,.0f} +/- {1:,.0f} white'.format(\n",
    "            thisrow['pop_white'], thisrow['pop_white_err']\n",
    "        ))\n",
    "        print('{0:,.0f} +/- {1:,.0f} total'.format(\n",
    "            thisrow['pop_total'], thisrow['pop_total_err']\n",
    "        ))\n",
    "        print('{0:.1%} +/- {1:.1%}'.format(\n",
    "            thisrow['pct_white'], thisrow['pct_white_serr']\n",
    "        ))\n",
    "        print('\\n')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct for inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "# I don't know why this didn't work earlier, but what the hell...\n",
    "data_gdf.loc[:, 'mfi'] = pandas.to_numeric(data_gdf['mfi'], errors='coerce')\n",
    "data_gdf.loc[:, 'median_home_value'] = pandas.to_numeric(data_gdf['median_home_value'], errors='coerce')\n",
    "data_gdf.loc[:, 'mfi_err'] = pandas.to_numeric(data_gdf['mfi_err'], errors='coerce')\n",
    "data_gdf.loc[:, 'median_home_value_err'] = pandas.to_numeric(data_gdf['median_home_value_err'], errors='coerce')\n",
    "\n",
    "money_columns = []\n",
    "money_columns += ['amtLoans1', 'amtLoans100k', 'amtLoans250k', 'amtLoansToSmallest']\n",
    "money_columns += ['avgSmallLoan', 'amtLoans', 'amtWorkingLoans']\n",
    "money_columns += ['amtLoans1_per_totaljob', 'amtLoans100k_per_totaljob', 'amtLoans250k_per_totaljob']\n",
    "money_columns += ['amtLoansToSmallest_per_totaljob', 'amtLoans_per_totaljob', 'amtWorkingLoans_per_totaljob']\n",
    "money_columns += ['amtLoans1_per_sbjob', 'amtLoans100k_per_sbjob', 'amtLoans250k_per_sbjob']\n",
    "money_columns += ['amtLoansToSmallest_per_sbjob', 'amtLoans_per_sbjob', 'amtWorkingLoans_per_sbjob']\n",
    "money_columns += ['mfi', 'median_home_value', 'mfi_err', 'median_home_value_err']\n",
    "\n",
    "raw_inflation_df = pandas.read_csv(inflation_dir+'cpi-1913-2017.csv', encoding='utf-8', low_memory=False, index_col='Year')\n",
    "raw_inflation_df.index.name = 'year'\n",
    "\n",
    "for thisyear in years:\n",
    "    inflation_factor = 1 / (raw_inflation_df['Jan'].loc[thisyear] / raw_inflation_df['Jan'].loc[2017])\n",
    "    for thiscol in money_columns:\n",
    "        varname = thiscol + '_adj'\n",
    "        data_gdf[varname] = data_gdf[thiscol].apply(lambda x: x * inflation_factor)\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add community statistical areas (for Baltimore)\n",
    "\n",
    "This will make sense only for Baltimore. Otherwise, the variable CSA2010 will always be np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "tract_to_csa_df = pandas.read_csv(code_lookup_dir+'census_tract_to_neighborhood.csv', index_col='NAME10')\n",
    "\n",
    "data_gdf = data_gdf.assign(CSA2010 = data_gdf.reset_index()[(data_gdf.reset_index()['COUNTYFP'] == 510)].merge(tract_to_csa_df.reset_index(), how='left', left_on=['NAME'], right_on=['NAME10'])['CSA2010'])\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get only the columns we need, in the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "# Create new columns with renamings of total jobs (C000) and sb_jobs (CFS01)\n",
    "data_gdf = data_gdf.assign(total_jobs = data_gdf['C000'])\n",
    "data_gdf = data_gdf.assign(sb_jobs = data_gdf['CFS01'])\n",
    "new_columns = []\n",
    "new_columns += data_gdf.columns.tolist()[9:10]  # city name\n",
    "new_columns += data_gdf.columns.tolist()[17:18]  # metropolitan statistical area (MSA) number\n",
    "new_columns += data_gdf.columns.tolist()[14:16]  # loan type (always 4 for business) and action taken (always 1 for origination)\n",
    "new_columns += data_gdf.columns.tolist()[20:21]  # income group code (1 to 14 or 101 to 105)\n",
    "new_columns += data_gdf.columns.tolist()[29:31]  # income group and CRA level (human-readable)\n",
    "new_columns += data_gdf.columns.tolist()[21:29]  # number and amount of loans (directly from CRA data)\n",
    "new_columns += data_gdf.columns.tolist()[31:36]  # calculated total and working loans\n",
    "new_columns += data_gdf.columns.tolist()[195:197]  # total and SB jobs\n",
    "\n",
    "\n",
    "new_columns += data_gdf.columns.tolist()[38:89] # all jobs columns under original names\n",
    "new_columns += data_gdf.columns.tolist()[89:101] # loans per total job in tract\n",
    "new_columns += data_gdf.columns.tolist()[101:113]  # loans per small business job\n",
    "new_columns += data_gdf.columns.tolist()[118:119]   # total population in census tract\n",
    "new_columns += data_gdf.columns.tolist()[120:122]   # white and black population\n",
    "new_columns += data_gdf.columns.tolist()[122:124]  # white and black householders\n",
    "new_columns += data_gdf.columns.tolist()[126:127]  # female householders\n",
    "new_columns += data_gdf.columns.tolist()[149:150]  # high school graduates (ages 25+)\n",
    "new_columns += data_gdf.columns.tolist()[127:129]  # unemployed (age 16+) in poverty\n",
    "new_columns += data_gdf.columns.tolist()[125:126]  # tract median family income (MFI)\n",
    "new_columns += data_gdf.columns.tolist()[129:131]  # tract median home value and median year built\n",
    "new_columns += data_gdf.columns.tolist()[150:152] # comparison variables: total householders, and population age 25+\n",
    "new_columns += data_gdf.columns.tolist()[131:133] # comparison variables: labor force (age 16+) and population whose poverty status is known\n",
    "\n",
    "new_columns += data_gdf.columns.tolist()[133:134]  # error in total population\n",
    "new_columns += data_gdf.columns.tolist()[135:137]  # error in black and white populations\n",
    "new_columns += data_gdf.columns.tolist()[137:139]  # error in black and white householders\n",
    "new_columns += data_gdf.columns.tolist()[141:142]  # error in female householders\n",
    "\n",
    "new_columns += data_gdf.columns.tolist()[152:153]  # (standard) error in high shchool graduates (age 25+) \n",
    "new_columns += data_gdf.columns.tolist()[142:144]  # error in population unemployed and in poverty\n",
    "new_columns += data_gdf.columns.tolist()[140:141]  # error in tract MFI\n",
    "new_columns += data_gdf.columns.tolist()[144:146]  # error in tract median home value and median year built\n",
    "new_columns += data_gdf.columns.tolist()[154:155]  #(standard) error in comparison variable: total householders\n",
    "new_columns += data_gdf.columns.tolist()[153:154]  #(standard) error in comparison variable: population age 25+\n",
    "new_columns += data_gdf.columns.tolist()[146:148]  #(standard) error in comparison variables: labor force (age 16+) and population whose poverty status is known\n",
    "\n",
    "new_columns += data_gdf.columns.tolist()[155:163]  # percentages: white/black population and householders, female householders, HS grads (age 25+), unemployed (age 16+), in poverty\n",
    "new_columns += data_gdf.columns.tolist()[163:171]  # standard errors in percentages: white/black population and householders, female householders, HS grads (age 25+), unemployed (age 16+), in poverty\n",
    "\n",
    "new_columns += data_gdf.columns.tolist()[171:194]  # money values adjusted for inflation\n",
    "\n",
    "new_columns += data_gdf.columns.tolist()[0:9]   # geographic information about the census tract\n",
    "new_columns += data_gdf.columns.tolist()[18:20]  # split county indicator and population classification from CRA data, probably not useful\n",
    "new_columns += data_gdf.columns.tolist()[115:118] # geodata from ACS5\n",
    "new_columns += data_gdf.columns.tolist()[124:125]  # owner-occupied housing units (we don't use this yet)\n",
    "new_columns += data_gdf.columns.tolist()[194:195]  # community statistical area (useful only for Baltimore)\n",
    "#\n",
    "new_columns += data_gdf.columns.tolist()[10:11]   # as is traditional, geometry goes last\n",
    "\n",
    "#rint(data_gdf[new_columns].columns.tolist())\n",
    "#data_gdf.groupby('cra_level').size().sort_index()\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done')\n",
    "\n",
    "#new_columns\n",
    "#data_gdf.columns.tolist()[155:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing out...\n",
      "Done in 0 minutes 0 seconds!\n",
      "GRAND TOTAL TIME: 4 minutes 41 seconds!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "print('Writing out...')\n",
    "output_gdf = data_gdf[new_columns]\n",
    "output_gdf = output_gdf.reset_index()\n",
    "\n",
    "column_names_df = pandas.DataFrame(output_gdf.columns.tolist(), columns=['variable_name'])\n",
    "#column_names_df = column_names_df.set_index('variable_name')\n",
    "column_names_df.reset_index().to_csv(output_data_dir+'column_names.csv', encoding='utf-8')\n",
    "\n",
    "output_gdf.columns = [str(x) for x in list(range(0,188,1))]\n",
    "output_gdf = output_gdf.set_geometry('187')\n",
    "output_gdf.to_file(output_data_dir+'{0:}_alldata.shp'.format(city.lower()))\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('Done in {0:,.0f} minutes {1:.0f} seconds!'.format(np.floor((e-s)/60), (e-s)%60))\n",
    "print('GRAND TOTAL TIME: {0:,.0f} minutes {1:.0f} seconds!'.format(np.floor(g/60), g%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ggdf = geopandas.read_file(output_data_dir+'{0:}_alldata.shp'.format(city.lower()))\n",
    "#ggdf.plot(edgecolor='white')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COUNTYFP',\n",
       " 'NAME',\n",
       " 'year',\n",
       " 'city_name',\n",
       " 'msa',\n",
       " 'loan_type',\n",
       " 'action_taken_type',\n",
       " 'income_group_code',\n",
       " 'income_group',\n",
       " 'cra_level',\n",
       " 'nLoans1',\n",
       " 'amtLoans1',\n",
       " 'nLoans100k',\n",
       " 'amtLoans100k',\n",
       " 'nLoans250k',\n",
       " 'amtLoans250k',\n",
       " 'nLoansToSmallest',\n",
       " 'amtLoansToSmallest',\n",
       " 'nLoans',\n",
       " 'amtLoans',\n",
       " 'avgSmallLoan',\n",
       " 'nWorkingLoans',\n",
       " 'amtWorkingLoans',\n",
       " 'total_jobs',\n",
       " 'sb_jobs',\n",
       " 'C000',\n",
       " 'CA01',\n",
       " 'CA02',\n",
       " 'CA03',\n",
       " 'CE01',\n",
       " 'CE02',\n",
       " 'CE03',\n",
       " 'CNS01',\n",
       " 'CNS02',\n",
       " 'CNS03',\n",
       " 'CNS04',\n",
       " 'CNS05',\n",
       " 'CNS06',\n",
       " 'CNS07',\n",
       " 'CNS08',\n",
       " 'CNS09',\n",
       " 'CNS10',\n",
       " 'CNS11',\n",
       " 'CNS12',\n",
       " 'CNS13',\n",
       " 'CNS14',\n",
       " 'CNS15',\n",
       " 'CNS16',\n",
       " 'CNS17',\n",
       " 'CNS18',\n",
       " 'CNS19',\n",
       " 'CNS20',\n",
       " 'CR01',\n",
       " 'CR02',\n",
       " 'CR03',\n",
       " 'CR04',\n",
       " 'CR05',\n",
       " 'CR07',\n",
       " 'CT01',\n",
       " 'CT02',\n",
       " 'CD01',\n",
       " 'CD02',\n",
       " 'CD03',\n",
       " 'CD04',\n",
       " 'CS01',\n",
       " 'CS02',\n",
       " 'CFA01',\n",
       " 'CFA02',\n",
       " 'CFA03',\n",
       " 'CFA04',\n",
       " 'CFA05',\n",
       " 'CFS01',\n",
       " 'CFS02',\n",
       " 'CFS03',\n",
       " 'CFS04',\n",
       " 'CFS05',\n",
       " 'nLoans1_per_totaljob',\n",
       " 'amtLoans1_per_totaljob',\n",
       " 'nLoans100k_per_totaljob',\n",
       " 'amtLoans100k_per_totaljob',\n",
       " 'nLoans250k_per_totaljob',\n",
       " 'amtLoans250k_per_totaljob',\n",
       " 'nLoansToSmallest_per_totaljob',\n",
       " 'amtLoansToSmallest_per_totaljob',\n",
       " 'nLoans_per_totaljob',\n",
       " 'amtLoans_per_totaljob',\n",
       " 'nWorkingLoans_per_totaljob',\n",
       " 'amtWorkingLoans_per_totaljob',\n",
       " 'nLoans1_per_sbjob',\n",
       " 'amtLoans1_per_sbjob',\n",
       " 'nLoans100k_per_sbjob',\n",
       " 'amtLoans100k_per_sbjob',\n",
       " 'nLoans250k_per_sbjob',\n",
       " 'amtLoans250k_per_sbjob',\n",
       " 'nLoansToSmallest_per_sbjob',\n",
       " 'amtLoansToSmallest_per_sbjob',\n",
       " 'nLoans_per_sbjob',\n",
       " 'amtLoans_per_sbjob',\n",
       " 'nWorkingLoans_per_sbjob',\n",
       " 'amtWorkingLoans_per_sbjob',\n",
       " 'pop_total',\n",
       " 'pop_white',\n",
       " 'pop_black',\n",
       " 'white_householder',\n",
       " 'black_householder',\n",
       " 'female_householder',\n",
       " 'hs_grad_25plus',\n",
       " 'unemployed_16plus',\n",
       " 'poverty_past_12_months',\n",
       " 'mfi',\n",
       " 'median_home_value',\n",
       " 'median_year_built',\n",
       " 'total_householders',\n",
       " 'pop_25plus',\n",
       " 'labor_force_16plus',\n",
       " 'poverty_status_known',\n",
       " 'pop_total_err',\n",
       " 'pop_white_err',\n",
       " 'pop_black_err',\n",
       " 'black_householder_err',\n",
       " 'white_householder_err',\n",
       " 'female_householder_err',\n",
       " 'hs_grad_25plus_serr',\n",
       " 'unemployed_16plus_err',\n",
       " 'poverty_past_12_months_err',\n",
       " 'mfi_err',\n",
       " 'median_home_value_err',\n",
       " 'median_year_built_err',\n",
       " 'total_householders_serr',\n",
       " 'pop_25plus_serr',\n",
       " 'labor_force_16plus_err',\n",
       " 'poverty_status_known_err',\n",
       " 'pct_white',\n",
       " 'pct_black',\n",
       " 'pct_white_householders',\n",
       " 'pct_black_householders',\n",
       " 'pct_female_householders',\n",
       " 'pct_hs_grad',\n",
       " 'pct_unemployed',\n",
       " 'pct_poverty',\n",
       " 'pct_white_serr',\n",
       " 'pct_black_serr',\n",
       " 'pct_white_householders_serr',\n",
       " 'pct_black_householders_serr',\n",
       " 'pct_female_householders_serr',\n",
       " 'pct_hs_grad_serr',\n",
       " 'pct_unemployed_serr',\n",
       " 'pct_poverty_serr',\n",
       " 'amtLoans1_adj',\n",
       " 'amtLoans100k_adj',\n",
       " 'amtLoans250k_adj',\n",
       " 'amtLoansToSmallest_adj',\n",
       " 'avgSmallLoan_adj',\n",
       " 'amtLoans_adj',\n",
       " 'amtWorkingLoans_adj',\n",
       " 'amtLoans1_per_totaljob_adj',\n",
       " 'amtLoans100k_per_totaljob_adj',\n",
       " 'amtLoans250k_per_totaljob_adj',\n",
       " 'amtLoansToSmallest_per_totaljob_adj',\n",
       " 'amtLoans_per_totaljob_adj',\n",
       " 'amtWorkingLoans_per_totaljob_adj',\n",
       " 'amtLoans1_per_sbjob_adj',\n",
       " 'amtLoans100k_per_sbjob_adj',\n",
       " 'amtLoans250k_per_sbjob_adj',\n",
       " 'amtLoansToSmallest_per_sbjob_adj',\n",
       " 'amtLoans_per_sbjob_adj',\n",
       " 'amtWorkingLoans_per_sbjob_adj',\n",
       " 'mfi_adj',\n",
       " 'median_home_value_adj',\n",
       " 'mfi_err_adj',\n",
       " 'median_home_value_err_adj',\n",
       " 'STATEFP',\n",
       " 'TRACTCE',\n",
       " 'NAMELSAD',\n",
       " 'MTFCC',\n",
       " 'FUNCSTAT',\n",
       " 'ALAND',\n",
       " 'AWATER',\n",
       " 'INTPTLAT',\n",
       " 'INTPTLON',\n",
       " 'split_county_indicator',\n",
       " 'population_classification',\n",
       " 'GEOID',\n",
       " 'STUSAB',\n",
       " 'LOGRECNO',\n",
       " 'owner_occ_housing_units',\n",
       " 'CSA2010',\n",
       " 'geometry']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_gdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
