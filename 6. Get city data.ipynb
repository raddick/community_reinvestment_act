{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n",
      "Now in directory: /home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "debug = 1\n",
    "city = \"Richmond\" # Possible values: Baltimore, Washington DC, Detroit, Newark, St. Louis, Richmond, San Francisco, Cleveland, Philadelphia, Pittsburgh\n",
    "g = 0\n",
    "\n",
    "latest_year = 2019\n",
    "earliest_year = 2010\n",
    "\n",
    "check_tract_and_city_boundaries = True\n",
    "check_tract_consistency = True\n",
    "show_water = False\n",
    "\n",
    "print('Importing packages...')\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import time\n",
    "#import zipfile\n",
    "import geopandas\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.patches as mpatches\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "pandas.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# Directories to look in\n",
    "thisdir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/'\n",
    "data_dir = '/home/idies/workspace/Temporary/raddick/cra_scratch/'\n",
    "jobs_dir = data_dir + 'lodes_wac/'\n",
    "\n",
    "#census_dir = data_dir + 'acs5/'\n",
    "\n",
    "output_data_dir = thisdir + 'final_data/'\n",
    "#baltimore_dir = thisdir + 'baltimore/'\n",
    "\n",
    "census_shapefile_tiger_basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/shapefiles/'\n",
    "\n",
    "#shapefile_dir = census_shapefile_tiger_basedir + '{0:.0f}/TRACT/'.format(thisyear)\n",
    "\n",
    "acs5_basedir = '/home/idies/workspace/Temporary/raddick/census_scratch/acs5/'\n",
    "\n",
    "code_lookup_dir = thisdir + 'code_guide_lookups/'\n",
    "inflation_dir = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/datasets/inflation/'\n",
    "extrasdir = '/home/idies/workspace/Storage/raddick/census/extras/'\n",
    "\n",
    "city_data_dir = thisdir + 'city_data/'\n",
    "scale = 1\n",
    "\n",
    "equal_area_epsg = 5070\n",
    "\n",
    "os.chdir(thisdir)\n",
    "g = 0 # global time\n",
    "\n",
    "print('Now in directory: {0:}'.format(os.getcwd()))\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Richmond!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "years = np.arange(latest_year, earliest_year-1, -1)\n",
    "\n",
    "if (city == 'Baltimore'):\n",
    "    thestate = 24\n",
    "    state_abbrev = 'md'\n",
    "    citycode = 4000\n",
    "    baltimore_shapefile_dir = '/home/idies/workspace/Storage/raddick/Baltimore/shapefiles/'\n",
    "    plotlimits = {'N': 39.38, 'S': 39.195, 'W': -76.745, 'E': -76.505}\n",
    "\n",
    "elif (city == 'Washington DC'):\n",
    "    thestate = 11  \n",
    "    state_abbrev = 'dc'\n",
    "    citycode = 50000\n",
    "    plotlimits = {'N': 39.01, 'S': 38.79, 'E': -76.9, 'W': -77.13}\n",
    "\n",
    "elif (city == 'Detroit'):\n",
    "    thestate = 26   # state_codes_df[state_codes_df['STATE_NAME'] == 'Missouri']\n",
    "    state_abbrev = 'mi'\n",
    "    citycode = 22000\n",
    "    plotlimits = {'N': 42.5, 'S': 42.2, 'E': -82.8, 'W': -83.3}\n",
    "    \n",
    "elif (city == 'Newark'):\n",
    "    thestate = 34\n",
    "    state_abbrev = 'nj'\n",
    "    citycode = 51000\n",
    "    plotlimits = {'N': 40.8, 'S': 40.6, 'E': -74.1, 'W': -74.3}\n",
    "\n",
    "elif (city == 'St. Louis'):\n",
    "    thestate = 29   # state_codes_df[state_codes_df['STATE_NAME'] == 'Missouri']\n",
    "    state_abbrev = 'mo'\n",
    "    citycode = 65000\n",
    "    plotlimits = {'N': 38.8, 'S': 38.5, 'E': -90.1, 'W': -90.4}\n",
    "#    cityname_file = 'st_louis'\n",
    "    \n",
    "elif (city == 'Richmond'):\n",
    "    thestate = 47   # state_codes_df[state_codes_df['STATE_NAME'] == 'Missouri']\n",
    "    state_abbrev = 'va'\n",
    "    citycode = 67000\n",
    "    #thecounty = 760\n",
    "#    cityname_file = 'richmond'\n",
    "    \n",
    "elif (city == 'San Francisco'):\n",
    "    thestate = 5   # state_codes_df[state_codes_df['STATE_NAME'] == 'California']\n",
    "    state_abbrev = 'ca'\n",
    "    #thecounty = 75\n",
    "    citycode = 67000\n",
    "    cityname_file = 'san_francisco'\n",
    "    \n",
    "elif (city == 'Pittsburgh'):\n",
    "    thestate = 42\n",
    "    state_abbrev = 'pa'\n",
    "    citycode = 61000\n",
    "    plotlimits = {'N': 40.55, 'S': 40.3, 'E': -79.85, 'W': -80.15}\n",
    "\n",
    "elif (city == 'Cleveland'):\n",
    "    thestate = 39\n",
    "    state_abbrev = 'oh'\n",
    "    citycode = 16000\n",
    "    plotlimits = {'N': 41.65, 'S': 41.35, 'E': -81.5, 'W': -81.9}\n",
    "    \n",
    "elif (city == 'Philadelphia'):\n",
    "    thestate = 42\n",
    "    state_abbrev = 'pa'\n",
    "    #thecounty = 101\n",
    "    citycode = 60000\n",
    "    plotlimits = {'N': 40.15, 'S': 39.85, 'E': -74.93, 'W': -75.3}\n",
    "else:\n",
    "    print('ERROR: Select city from list!')\n",
    "\n",
    "cityname_file = city.lower().replace(' ','_')\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Selected {0:}!'.format(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do city boundaries line up with census tract boundaries?\n",
    "\n",
    "Baltimore: Yes\n",
    "Washington, DC: Yes\n",
    "Detroit: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 2019 census tracts and places in va...\n",
      "wtf, this city has more than one geometry included?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'city_year_geo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f09b3836a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#         if (np.mod(cnt,100) == 0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#             print('\\t\\tMatching tract {0:,.0f} of {1:,.0f}...'.format(cnt, ntracts_this_state_year))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthisrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_year_geo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mintersection_geo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_tracts_this_year_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_year_geo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mpct_overlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersection_geo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mthisrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'city_year_geo' is not defined"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "show_year = 2010\n",
    "city_tracts_years_gdf = geopandas.GeoDataFrame()#data=None, columns=state_tracts_this_year_gdf, crs=state_tracts_this_year_gdf.crs, geometry='geometry')\n",
    "\n",
    "for thisyear in years:\n",
    "#     print(thisyear)\n",
    "#     print('\\n')\n",
    "#     pprint([x for x in \n",
    "#         os.listdir(\n",
    "#             census_shapefile_tiger_basedir +'{0:.0f}/TRACT/'.format(thisyear)\n",
    "#             ) if ('_11_' in x) and (x[-4:] == '.shp')]\n",
    "#         )\n",
    "#     print('\\n')\n",
    "    print('Parsing {0:.0f} census tracts and places in {1:}...'.format(thisyear, state_abbrev))\n",
    "    if (thisyear == 2010):\n",
    "        state_tracts_this_year_gdf = geopandas.read_file(census_shapefile_tiger_basedir +'{0:.0f}/TRACT/tl_{0:.0f}_{1:02d}_tract10.shp'.format(thisyear, thestate))#.set_index('GEOID')\n",
    "        for x in state_tracts_this_year_gdf.columns[:-1]:\n",
    "            state_tracts_this_year_gdf = state_tracts_this_year_gdf.rename(columns = {x: x[:-2]})\n",
    "        state_tracts_this_year_gdf = state_tracts_this_year_gdf.set_index('GEOID')\n",
    "    else:\n",
    "        state_tracts_this_year_gdf = geopandas.read_file(census_shapefile_tiger_basedir +'{0:.0f}/TRACT/tl_{0:.0f}_{1:02d}_tract.shp'.format(thisyear, thestate)).set_index('GEOID')\n",
    "#     print('{0:,.0f} tracts found...'.format(len(state_tracts_this_year_gdf)))\n",
    "#     print(state_tracts_this_year_gdf.geometry.apply(lambda x: x.area).sort_values(ascending=False).head(3))\n",
    "#     print('\\n\\n\\n')\n",
    "\n",
    "    if (check_tract_and_city_boundaries):\n",
    "        if (thisyear == show_year):    \n",
    "            state_tracts_show_year_gdf = state_tracts_this_year_gdf\n",
    "        \n",
    "    ntracts_this_state_year = len(state_tracts_this_year_gdf)    \n",
    "    city_tracts_this_year_gdf = geopandas.GeoDataFrame(data=None, columns=state_tracts_this_year_gdf.columns, crs=state_tracts_this_year_gdf.crs, geometry='geometry')\n",
    "    if (thisyear == 2010):\n",
    "        state_places_this_year_gdf = geopandas.read_file(census_shapefile_tiger_basedir + '{0:.0f}/PLACE/tl_{0:.0f}_{1:.0f}_place10.shp'.format(thisyear, thestate))#.set_index('GEOID')\n",
    "        for x in state_places_this_year_gdf.columns[:-1]:\n",
    "            state_places_this_year_gdf = state_places_this_year_gdf.rename(columns = {x: x[:-2]})\n",
    "        state_places_this_year_gdf = state_places_this_year_gdf.set_index('GEOID')\n",
    "    else:\n",
    "        state_places_this_year_gdf = geopandas.read_file(census_shapefile_tiger_basedir + '{0:.0f}/PLACE/tl_{0:.0f}_{1:.0f}_place.shp'.format(thisyear, thestate)).set_index('GEOID')\n",
    "    \n",
    "#     print('{0:,.0f} places found...'.format(len(state_places_this_year_gdf)))\n",
    "#     print('------\\n')\n",
    "\n",
    "    if (check_tract_and_city_boundaries):\n",
    "        if (thisyear == show_year):    \n",
    "            state_places_show_year_gdf = state_places_this_year_gdf\n",
    "    \n",
    "    if (len(state_places_this_year_gdf[state_places_this_year_gdf['PLACEFP'] == '{0:05d}'.format(citycode)].geometry.values) == 1):\n",
    "        city_year_geo = state_places_this_year_gdf[state_places_this_year_gdf['PLACEFP'] == '{0:05d}'.format(citycode)].geometry.values[0]\n",
    "        city_year_area_sq_m = state_places_this_year_gdf[state_places_this_year_gdf['PLACEFP'] == '{0:05d}'.format(citycode)].to_crs(epsg=equal_area_epsg).geometry.area.values[0]\n",
    "        print('\\tMatching {0:} {1:.0f} data (city area = {2:,.1f} square km)...'.format(city, thisyear, city_year_area_sq_m/(1000**2)))\n",
    "    else:\n",
    "        print('wtf, this city has more than one geometry included?')\n",
    "\n",
    "    cnt = 0\n",
    "    for ix, thisrow in state_tracts_this_year_gdf.iterrows():\n",
    "#         if (np.mod(cnt,100) == 0):\n",
    "#             print('\\t\\tMatching tract {0:,.0f} of {1:,.0f}...'.format(cnt, ntracts_this_state_year))\n",
    "        if (thisrow.geometry.intersects(city_year_geo)):        \n",
    "            intersection_geo = state_tracts_this_year_gdf.loc[ix].geometry.intersection(city_year_geo)\n",
    "            pct_overlap = intersection_geo.area / thisrow.geometry.area\n",
    "            if (pct_overlap >= 0.995):\n",
    "                city_tracts_this_year_gdf = pandas.concat((city_tracts_this_year_gdf, state_tracts_this_year_gdf[state_tracts_this_year_gdf.index == ix]), axis=0)\n",
    "            elif (pct_overlap >= 0.01):\n",
    "                intersection_gdf = geopandas.GeoDataFrame(data=[[intersection_geo]], columns=['geometry'], crs=state_tracts_this_year_gdf.crs, geometry='geometry')\n",
    "                tract_total_area_sq_m = state_tracts_this_year_gdf.to_crs(epsg=equal_area_epsg).loc[ix].geometry.area\n",
    "                intersection_area_sq_m = intersection_gdf.to_crs(epsg=equal_area_epsg).geometry.apply(lambda x: x.area).values[0]\n",
    "                pct_overlap_sq_m = intersection_area_sq_m / tract_total_area_sq_m        \n",
    "                print('PARTIAL overlap found: County {0:} Tract {1:}\\n\\tTotal area {2:,.1f} sq km, overlap area {3:,.1f} sq km ({4:})'.format(thisrow['COUNTYFP'], thisrow['TRACTCE'], tract_total_area_sq_m/1000000, intersection_area_sq_m/1000000, pct_overlap_sq_m))\n",
    "        cnt += 1\n",
    "    city_tracts_this_year_gdf = city_tracts_this_year_gdf.assign(year = thisyear)\n",
    "    city_tracts_years_gdf = pandas.concat((city_tracts_years_gdf, city_tracts_this_year_gdf), axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "city_tracts_years_gdf.loc[:, 'STATEFP'] = pandas.to_numeric(city_tracts_years_gdf['STATEFP'], errors='coerce')\n",
    "city_tracts_years_gdf.loc[:, 'COUNTYFP'] = pandas.to_numeric(city_tracts_years_gdf['COUNTYFP'], errors='coerce')\n",
    "city_tracts_years_gdf.loc[:, 'TRACTCE'] = pandas.to_numeric(city_tracts_years_gdf['TRACTCE'].apply(lambda x: '{0:}.{1:}'.format(x[0:-2],x[-2:])), errors='coerce')\n",
    "for x in ['ALAND', 'AWATER', 'INTPTLAT','INTPTLON']:\n",
    "    city_tracts_years_gdf.loc[:, x] = pandas.to_numeric(city_tracts_years_gdf[x], errors='coerce')\n",
    "\n",
    "city_tracts_years_gdf = city_tracts_years_gdf.rename(columns={'COUNTYFP': 'county', 'TRACTCE': 'census_tract'})\n",
    "city_tracts_years_gdf.index.name = 'GEOID'\n",
    "city_tracts_years_gdf = city_tracts_years_gdf.reset_index()\n",
    "city_tracts_years_gdf.loc[:, 'GEOID'] = city_tracts_years_gdf['GEOID'].apply(lambda x: '14000US'+x)\n",
    "city_tracts_years_gdf = city_tracts_years_gdf.set_index(['GEOID','year'])\n",
    "\n",
    "if (check_tract_and_city_boundaries):\n",
    "    print('plotting tract vs city for {0:.0f}...'.format(show_year))\n",
    "    fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
    "    state_tracts_show_year_gdf.plot(color='none', edgecolor='black', ax=ax)    \n",
    "    state_places_show_year_gdf[state_places_show_year_gdf['PLACEFP'] == '{0:05d}'.format(citycode)].plot(ax=ax, color='none', edgecolor='yellow', lw=5)\n",
    "\n",
    "    city_tracts_years_gdf.xs(show_year, level='year').plot(ax=ax, color='pink', alpha=0.5)\n",
    "    if (show_water):\n",
    "        print('\\treading water files...')\n",
    "        water_dir = census_shapefile_tiger_basedir+\"{0:.0f}/AREAWATER/\".format(show_year)\n",
    "        water_files = [water_dir+x for x in os.listdir(water_dir) if ('tl_{0:.0f}_{1:02d}'.format(show_year,thestate) in x) and (x[-3:] == 'shp')]\n",
    "        water_gdf = geopandas.GeoDataFrame()\n",
    "        for i in range(0, len(water_files)):\n",
    "            if ((np.mod(i,10) == 0) | (i == len(water_files)-1)):\n",
    "                print('\\t\\treading water file {0:,.0f} of {1:,.0f}...'.format(i+1, len(water_files)))\n",
    "            water_gdf = pandas.concat((water_gdf, geopandas.read_file(water_files[i])))\n",
    "        water_gdf.plot(ax=ax, color='blue')\n",
    "\n",
    "    plt.xlim([plotlimits['W'], plotlimits['E']])\n",
    "    plt.ylim([plotlimits['S'], plotlimits['N']])\n",
    "    plt.title('{0:.0f}'.format(show_year), size=24)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#city_tracts_years_gdf\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Found {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(city_tracts_years_gdf), e-s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure that tract boundaries remain constant throughout the years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicated or unique names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = time.time()\n",
    "\n",
    "\n",
    "# test_year = 2018\n",
    "# test_color = 'cyan'\n",
    "# control_year = 2019\n",
    "# control_edge_color = 'red'\n",
    "\n",
    "# legend_location = 'lower left'\n",
    "\n",
    "# # print('getting from backup...')\n",
    "# # city_tracts_years_gdf = city_tracts_years_gdf_bk\n",
    "\n",
    "# counties_list = city_tracts_years_gdf['county'].drop_duplicates().tolist()\n",
    "\n",
    "# #print('{0:} covers {1:,.0f} county...'.format(city, len(counties_list)))\n",
    "# #city_tracts_years_gdf.head(1).T\n",
    "# for x in counties_list:\n",
    "#     print('County {0:,.0f} contains {1:,.0f} tract-years:'.format(x, len(city_tracts_years_gdf.reset_index()[city_tracts_years_gdf.reset_index()['county'] == x])))\n",
    "#     for i in years:\n",
    "#         print('{0:.0f}: {1:,.0f} tracts'.format(\n",
    "#             i, \n",
    "#             len(city_tracts_years_gdf.reset_index()[\n",
    "#                 (city_tracts_years_gdf.reset_index()['county'] == x) \n",
    "#                 & (city_tracts_years_gdf.reset_index()['year'] == i)])\n",
    "#         ))\n",
    "#         county_x_year_control_census_tracts_list = city_tracts_years_gdf.reset_index()[\n",
    "#             (city_tracts_years_gdf.reset_index()['county'] == x) \n",
    "#             & (city_tracts_years_gdf.reset_index()['year'] == control_year)\n",
    "#         ]['NAME'].tolist()\n",
    "        \n",
    "#         county_x_year_i_census_tracts_list = city_tracts_years_gdf.reset_index()[\n",
    "#             (city_tracts_years_gdf.reset_index()['county'] == x) \n",
    "#             & (city_tracts_years_gdf.reset_index()['year'] == i)\n",
    "#         ]['NAME'].tolist()\n",
    "#         #print(county_x_year_i_census_tracts_list)\n",
    "        \n",
    "#         new_census_tracts = []\n",
    "#         missing_census_tracts = []\n",
    "        \n",
    "#         for n in county_x_year_i_census_tracts_list:\n",
    "#             if (n not in county_x_year_control_census_tracts_list):\n",
    "#                 new_census_tracts.append(n)\n",
    "#             for p in county_x_year_control_census_tracts_list:\n",
    "#                 if (p not in county_x_year_i_census_tracts_list):\n",
    "#                     missing_census_tracts.append(n)\n",
    "#         for y in new_census_tracts:\n",
    "#             print('Found census tract in {0:.0f} that is not present in 2018: {1:}'.format(i,y))\n",
    "#         for z in missing_census_tracts:\n",
    "#             print('Found census tract from 2018 that was not present in {0:.0f}: {1:}'.format(i,z))\n",
    "#         if (len(new_census_tracts) + len(missing_census_tracts) == 0):\n",
    "#             print('\\tAll census tracts have the same names!')\n",
    "#         else:\n",
    "#             print('\\n')\n",
    "# legend_list = []\n",
    "# # if (check_tract_consistency):\n",
    "# #     print('Plotting tracts for potential border differences...')\n",
    "# #     fig, ax = plt.subplots(1,1, figsize=(48*scale,48*scale))\n",
    "    \n",
    "# #     city_tracts_years_gdf.xs(test_year, level='year').plot(ax=ax, color=test_color, edgecolor='black')\n",
    "# #     city_tracts_years_gdf.xs(control_year, level='year').plot(ax=ax, color='none', edgecolor=control_edge_color, lw=3*scale)\n",
    "    \n",
    "    \n",
    "# #     legend_list.append(mpatches.Patch(facecolor=test_color, edgecolor='black', lw=3*scale, label='{0:}'.format(test_year)))\n",
    "# #     legend_list.append(mpatches.Patch(facecolor='none', edgecolor=control_edge_color, lw=3*scale, label='{0:}'.format(control_year)))\n",
    "    \n",
    "# # #     if (show_water):\n",
    "# # #         print('\\treading water files...')\n",
    "# # #         water_dir = census_shapefile_tiger_basedir+\"{0:.0f}/AREAWATER/\".format(show_year)\n",
    "# # #         water_files = [water_dir+x for x in os.listdir(water_dir) if ('tl_{0:.0f}_{1:02d}'.format(show_year,thestate) in x) and (x[-3:] == 'shp')]\n",
    "# # #         water_gdf = geopandas.GeoDataFrame()\n",
    "# # #         for i in range(0, len(water_files)):\n",
    "# # #             if ((np.mod(i,10) == 0) | (i == len(water_files)-1)):\n",
    "# # #                 print('\\t\\treading water file {0:,.0f} of {1:,.0f}...'.format(i+1, len(water_files)))\n",
    "# # #             water_gdf = pandas.concat((water_gdf, geopandas.read_file(water_files[i])))\n",
    "# # #         water_gdf.plot(ax=ax, color='blue')\n",
    "    \n",
    "# # #     # #city_tracts_years_gdf.loc[[2012, 35, 1711.03]].geometry.plot()#.plot(color='red')\n",
    "# # #     for ix, thisrow in city_tracts_years_gdf.xs(otheryear, level=-1).iterrows():\n",
    "# # #         if (ix[1] in new_census_tracts):\n",
    "# # #             annotator = ix[1]\n",
    "# # #             ax.annotate(annotator, \n",
    "# # #                        xy=(thisrow.geometry.centroid.x, thisrow.geometry.centroid.y), \n",
    "# # #                        xytext=(thisrow.geometry.centroid.x,#+0.01*np.random.rand(), \n",
    "# # #                                thisrow.geometry.centroid.y),#+0.01*np.random.rand()), \n",
    "# # #                        backgroundcolor = 'white', horizontalalignment='center', verticalalignment='center',\n",
    "# # #                        fontsize=24*scale)\n",
    "\n",
    "    \n",
    "# #     ax.legend(handles=legend_list, fontsize=48*scale, loc=legend_location)\n",
    "# #     plt.title('{0:.0f} compared to {1:.0f}'.format(test_year, control_year), fontsize=48*scale)\n",
    "# #     ax.tick_params(axis='both', which='major', labelsize=24*scale)\n",
    "# #     plt.xlim([plotlimits['W'], plotlimits['E']])\n",
    "# #     plt.ylim([plotlimits['S'], plotlimits['N']])\n",
    "\n",
    "# #     plt.show()\n",
    "\n",
    "# e = time.time()\n",
    "# g = g + (e-s)\n",
    "# print('Checked consistency of {0:,.0f} tract-years in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(city_tracts_years_gdf), np.floor((e-s)/60), (e-s)%60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Have the centroids or areas of any tracts changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# s = time.time()\n",
    "\n",
    "# test_year = 2010\n",
    "# test_color = 'cyan'\n",
    "# control_year = 2011\n",
    "# control_edge_color = 'red'\n",
    "\n",
    "# area_diff_tolerance = 0.01\n",
    "\n",
    "# legend_location = 'lower left'\n",
    "\n",
    "# print('Comparing areas of census tracts in {0:.0f} vs {1:.0f}...'.format(test_year, control_year))\n",
    "\n",
    "# if (len(new_census_tracts) + len(missing_census_tracts) > 0):\n",
    "#     print('need to match up the geographies of the census tract that changed names...')\n",
    "\n",
    "# changed_tract_list = []\n",
    "# # print('getting from backup...')\n",
    "# # city_tracts_years_gdf = city_tracts_years_gdf_bk\n",
    "\n",
    "# #counties_list = city_tracts_years_gdf['county'].drop_duplicates().tolist()\n",
    "\n",
    "# #print('{0:} covers {1:,.0f} county...'.format(city, len(counties_list)))\n",
    "# #city_tracts_years_gdf.head(1).T\n",
    "# nrows = len(city_tracts_years_gdf.xs(control_year, level='year'))\n",
    "# cnt = 0\n",
    "# for ix, thisrow in city_tracts_years_gdf.xs(control_year, level='year').iterrows():\n",
    "#     #if ()\n",
    "#     #print('exploring tract {0:.0f} of {1:.0f}...'.format(cnt, nrows))\n",
    "#     control_area = thisrow.geometry.area\n",
    "#     test_area = city_tracts_years_gdf.loc[ix, test_year].geometry.area\n",
    "#     if ((test_area - control_area)/control_area > area_diff_tolerance):\n",
    "#         changed_tract_list.append(ix)\n",
    "#     #cnt += 1\n",
    "    \n",
    "    \n",
    "# if (check_tract_consistency):\n",
    "#     legend_list = []\n",
    "    \n",
    "#     for ix in changed_tract_list:\n",
    "#         control_area = city_tracts_years_gdf.to_crs(epsg=equal_area_epsg).loc[ix, control_year].geometry.area\n",
    "#         test_area = city_tracts_years_gdf.to_crs(epsg=equal_area_epsg).loc[ix, test_year].geometry.area\n",
    "#         print('{0:}: area in {1:.0f}: {2:,.0f} m^2'.format(city_tracts_years_gdf.loc[ix, control_year]['NAME'], control_year, control_area))\n",
    "#         print('{0:}: area in {1:.0f}: {2:,.0f} m^2'.format(city_tracts_years_gdf.loc[ix, test_year]['NAME'], test_year, test_area))\n",
    "#         print('\\t{0:}: area difference {1:,.0f} m^2 (which is {2:.1%})'.format(ix, np.abs(test_area-control_area), np.abs(test_area-control_area)/control_area))\n",
    "#         print('\\n')\n",
    "\n",
    "#     print('Plotting tracts for potential border differences...')\n",
    "#     fig, ax = plt.subplots(1,1, figsize=(48*scale,48*scale))\n",
    "    \n",
    "#     city_tracts_years_gdf.xs(test_year, level='year').plot(ax=ax, color=test_color, edgecolor='black')\n",
    "#     city_tracts_years_gdf.xs(control_year, level='year').plot(ax=ax, color='none', edgecolor=control_edge_color, lw=3*scale)\n",
    "    \n",
    "#     legend_list.append(mpatches.Patch(facecolor=test_color, edgecolor='black', lw=3*scale, label='{0:}'.format(test_year)))\n",
    "#     legend_list.append(mpatches.Patch(facecolor='none', edgecolor=control_edge_color, lw=3*scale, label='{0:}'.format(control_year)))\n",
    "\n",
    "# #     if (show_water):\n",
    "# #         print('\\treading water files...')\n",
    "# #         water_dir = census_shapefile_tiger_basedir+\"{0:.0f}/AREAWATER/\".format(show_year)\n",
    "# #         water_files = [water_dir+x for x in os.listdir(water_dir) if ('tl_{0:.0f}_{1:02d}'.format(show_year,thestate) in x) and (x[-3:] == 'shp')]\n",
    "# #         water_gdf = geopandas.GeoDataFrame()\n",
    "# #         for i in range(0, len(water_files)):\n",
    "# #             if ((np.mod(i,10) == 0) | (i == len(water_files)-1)):\n",
    "# #                 print('\\t\\treading water file {0:,.0f} of {1:,.0f}...'.format(i+1, len(water_files)))\n",
    "# #             water_gdf = pandas.concat((water_gdf, geopandas.read_file(water_files[i])))\n",
    "# #         water_gdf.plot(ax=ax, color='blue')\n",
    "    \n",
    "#     for ix in changed_tract_list:\n",
    "#         annotator_test_year = '{0:} ({1:.0f})'.format(city_tracts_years_gdf.loc[ix, test_year]['NAME'], test_year)\n",
    "#         ax.annotate(annotator_test_year, \n",
    "#                     (city_tracts_years_gdf.loc[ix, test_year].geometry.centroid.x, city_tracts_years_gdf.loc[ix, test_year].geometry.centroid.y), \n",
    "#                     (city_tracts_years_gdf.loc[ix, test_year].geometry.centroid.x, city_tracts_years_gdf.loc[ix, test_year].geometry.centroid.y), \n",
    "#                     color='red', backgroundcolor='white', fontsize=32*scale, ha='center', va='center')\n",
    "# #     # #city_tracts_years_gdf.loc[[2012, 35, 1711.03]].geometry.plot()#.plot(color='red')\n",
    "# #     for ix, thisrow in city_tracts_years_gdf.xs(otheryear, level=-1).iterrows():\n",
    "# #         if (ix[1] in new_census_tracts):\n",
    "# #             annotator = ix[1]\n",
    "# #             ax.annotate(annotator, \n",
    "# #                        xy=(thisrow.geometry.centroid.x, thisrow.geometry.centroid.y), \n",
    "# #                        xytext=(thisrow.geometry.centroid.x,#+0.01*np.random.rand(), \n",
    "# #                                thisrow.geometry.centroid.y),#+0.01*np.random.rand()), \n",
    "# #                        backgroundcolor = 'white', horizontalalignment='center', verticalalignment='center',\n",
    "# #                        fontsize=24*scale)\n",
    "\n",
    "    \n",
    "#     ax.legend(handles=legend_list, fontsize=48*scale, loc=legend_location)\n",
    "#     plt.title('{0:.0f} compared to {1:.0f}'.format(test_year, control_year), fontsize=48*scale)\n",
    "#     ax.tick_params(axis='both', which='major', labelsize=24*scale)\n",
    "#     plt.xlim([plotlimits['W'], plotlimits['E']])\n",
    "#     plt.ylim([plotlimits['S'], plotlimits['N']])\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend_list = []\n",
    "# if (check_tract_consistency):\n",
    "#     print('Plotting tracts for potential border differences...')\n",
    "#     fig, ax = plt.subplots(1,1, figsize=(48*scale,48*scale))\n",
    "    \n",
    "#     city_tracts_years_gdf.xs(test_year, level='year').plot(ax=ax, color=test_color, edgecolor='black')\n",
    "#     city_tracts_years_gdf.xs(control_year, level='year').plot(ax=ax, color='none', edgecolor=control_edge_color, lw=3*scale)\n",
    "    \n",
    "    \n",
    "#     legend_list.append(mpatches.Patch(facecolor=test_color, edgecolor='black', lw=3*scale, label='{0:}'.format(test_year)))\n",
    "#     legend_list.append(mpatches.Patch(facecolor='none', edgecolor=control_edge_color, lw=3*scale, label='{0:}'.format(control_year)))\n",
    "    \n",
    "#     if (show_water):\n",
    "#         print('\\treading water files...')\n",
    "#         water_dir = census_shapefile_tiger_basedir+\"{0:.0f}/AREAWATER/\".format(show_year)\n",
    "#         water_files = [water_dir+x for x in os.listdir(water_dir) if ('tl_{0:.0f}_{1:02d}'.format(show_year,thestate) in x) and (x[-3:] == 'shp')]\n",
    "#         water_gdf = geopandas.GeoDataFrame()\n",
    "#         for i in range(0, len(water_files)):\n",
    "#             if ((np.mod(i,10) == 0) | (i == len(water_files)-1)):\n",
    "#                 print('\\t\\treading water file {0:,.0f} of {1:,.0f}...'.format(i+1, len(water_files)))\n",
    "#             water_gdf = pandas.concat((water_gdf, geopandas.read_file(water_files[i])))\n",
    "#         water_gdf.plot(ax=ax, color='blue')\n",
    "    \n",
    "# #     # #city_tracts_years_gdf.loc[[2012, 35, 1711.03]].geometry.plot()#.plot(color='red')\n",
    "# #     for ix, thisrow in city_tracts_years_gdf.xs(otheryear, level=-1).iterrows():\n",
    "# #         if (ix[1] in new_census_tracts):\n",
    "# #             annotator = ix[1]\n",
    "# #             ax.annotate(annotator, \n",
    "# #                        xy=(thisrow.geometry.centroid.x, thisrow.geometry.centroid.y), \n",
    "# #                        xytext=(thisrow.geometry.centroid.x,#+0.01*np.random.rand(), \n",
    "# #                                thisrow.geometry.centroid.y),#+0.01*np.random.rand()), \n",
    "# #                        backgroundcolor = 'white', horizontalalignment='center', verticalalignment='center',\n",
    "# #                        fontsize=24*scale)\n",
    "\n",
    "    \n",
    "#     ax.legend(handles=legend_list, fontsize=48*scale, loc=legend_location)\n",
    "#     plt.title('{0:.0f} compared to {1:.0f}'.format(test_year, control_year), fontsize=48*scale)\n",
    "#     ax.tick_params(axis='both', which='major', labelsize=24*scale)\n",
    "#     plt.xlim([plotlimits['W'], plotlimits['E']])\n",
    "#     plt.ylim([plotlimits['S'], plotlimits['N']])\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# e = time.time()\n",
    "# g = g + (e-s)\n",
    "# print('Checked consistency of {0:,.0f} tract-years in {1:,.0f} minutes {2:,.0f} seconds!'.format(len(city_tracts_years_gdf), np.floor((e-s)/60), (e-s)%60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for this_oneyear_index in tracts_to_label:\n",
    "#     print('county {0:.0f}, tract {1:.2f}:'.format(this_oneyear_index[0],this_oneyear_index[1]))\n",
    "#     print('\\tcentroids differ by {0:.2e}'.format(city_tracts_years_gdf.xs(oneyear, level=-1).loc[this_oneyear_index].geometry.centroid.distance(city_tracts_years_gdf.xs(otheryear, level=-1).loc[this_oneyear_index].geometry.centroid)))\n",
    "#     print('\\tareas differ by {0:.2e}'.format(np.abs(city_tracts_years_gdf.xs(oneyear, level=-1).loc[this_oneyear_index].geometry.area - city_tracts_years_gdf.xs(otheryear, level=-1).loc[this_oneyear_index].geometry.area)))\n",
    "#     print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# fig, ax = plt.subplots(1,1, figsize=(48*scale, 48*scale))\n",
    "# city_tracts_years_gdf.xs(oneyear, level=-1).plot(ax=ax, color='none', edgecolor='black')\n",
    "# city_tracts_years_gdf.xs(otheryear, level=-1).plot(ax=ax, color='none', edgecolor='red')\n",
    "# # if (show_water):\n",
    "# #     print('reading water files...')\n",
    "# #     water_dir = census_shapefile_tiger_basedir+\"{0:.0f}/AREAWATER/\".format(show_year)\n",
    "# #     water_files = [water_dir+x for x in os.listdir(water_dir) if ('tl_{0:.0f}_{1:02d}'.format(show_year,thestate) in x) and (x[-3:] == 'shp')]\n",
    "# #     water_gdf = geopandas.GeoDataFrame()\n",
    "# #     for i in range(0, len(water_files)):\n",
    "# #         if ((np.mod(i,10) == 0) | (i == len(water_files)-1)):\n",
    "# #             print('\\treading water file {0:,.0f} of {1:,.0f}...'.format(i+1, len(water_files)))\n",
    "# #     water_gdf = pandas.concat((water_gdf, geopandas.read_file(water_files[i])))\n",
    "# #     water_gdf.plot(ax=ax)\n",
    "\n",
    "# # for ix, thisrow in city_tracts_years_gdf.xs(oneyear, level=-1).loc[tracts_to_label].iterrows():\n",
    "# #     annotator = ix\n",
    "# #     ax.annotate(annotator, \n",
    "# #                 xy=(thisrow.geometry.centroid.x, thisrow.geometry.centroid.y), \n",
    "# #                 xytext=(thisrow.geometry.centroid.x,#+0.01*np.random.rand(), \n",
    "# #                         thisrow.geometry.centroid.y),#+0.01*np.random.rand()), \n",
    "# #                 backgroundcolor = 'white', horizontalalignment='center', verticalalignment='center',\n",
    "# #                 fontsize=24*scale)\n",
    "\n",
    "# plt.xlim([plotlimits['W'], plotlimits['E']])\n",
    "# plt.ylim([plotlimits['S'], plotlimits['N']])\n",
    "# plt.title('{0:.0f} (black) vs {1:.0f} (red)'.format(oneyear, otheryear), size=48*scale)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('finding tracts with different centroids and/or areas...')\n",
    "\n",
    "# centroid_tolerance_meters = 100\n",
    "# area_tolerance_square__meters = 1000\n",
    "\n",
    "# tracts_to_label = []\n",
    "# cnt = 1\n",
    "# for this_oneyear_index in city_tracts_years_gdf.xs(oneyear, level=-1).index.values:\n",
    "#     if (np.mod(cnt-1,50) == 0):\n",
    "#         print('Checking tract {0:.0f} of {1:.0f}...'.format(cnt, len(city_tracts_years_gdf.xs(oneyear, level=-1).index.values)))\n",
    "#     centroid_offset_meters = city_tracts_years_gdf.xs(oneyear, level=-1).to_crs(epsg=2272).loc[this_oneyear_index].geometry.centroid.distance(city_tracts_years_gdf.xs(otheryear, level=-1).to_crs(epsg=2272).loc[this_oneyear_index].geometry.centroid)\n",
    "#     area_difference_square_meters = np.abs(city_tracts_years_gdf.xs(oneyear, level=-1).to_crs(epsg=2272).loc[this_oneyear_index].geometry.area - city_tracts_years_gdf.xs(otheryear, level=-1).to_crs(epsg=2272).loc[this_oneyear_index].geometry.area)\n",
    "#     #print(this_oneyear_index)\n",
    "#     if ((centroid_offset_meters >= centroid_tolerance_meters) | (area_difference_square_meters >= area_tolerance_square__meters)):\n",
    "#         print('\\tCounty {0:} tract {1:} ({2:} vs. {3:}): centroid offset is {4:.1f} meters; area difference is {5:,.0f} square meters..'.format(this_oneyear_index[0], this_oneyear_index[1], oneyear, otheryear, centroid_offset_meters, area_difference_square_meters))\n",
    "#     cnt = cnt + 1\n",
    "# #     oneyear_tract_centroid = city_tracts_years_gdf.xs(oneyear, level=-1).loc[this_oneyear_index].geometry.centroid\n",
    "# #     otheryear_tract_centroid = city_tracts_years_gdf.xs(otheryear, level=-1).loc[this_oneyear_index].geometry.centroid\n",
    "# #     oneyear_tract_area = city_tracts_years_gdf.xs(oneyear, level=-1).loc[this_oneyear_index].geometry.area\n",
    "# #     otheryear_tract_area = city_tracts_years_gdf.xs(otheryear, level=-1).loc[this_oneyear_index].geometry.area\n",
    "# #     if ((oneyear_tract_centroid.distance(otheryear_tract_centroid) > centroid_tolerance) & (np.abs(oneyear_tract_area - otheryear_tract_area) > area_tolerance)):\n",
    "# #         if (this_oneyear_index not in tracts_to_label):\n",
    "# #             tracts_to_label.append(this_oneyear_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get geo-aggregated loans for this city\n",
    "\n",
    "and join the tract data onto the loans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# city_tracts_years_gdf = city_tracts_years_gdf_bk\n",
    "\n",
    "print('reading nationwide data...')\n",
    "agg_loans_all_df = pandas.read_csv(data_dir+'agg_loans.csv', encoding='utf-8', low_memory=False, index_col='rownumber', keep_default_na=False)\n",
    "e = time.time()\n",
    "print('Read {0:,.0f} nationwide tract-years in {1:,.0f} seconds...'.format(len(agg_loans_all_df), e-s))\n",
    "\n",
    "# Keep only business loans\n",
    "agg_loans_all_df = agg_loans_all_df[agg_loans_all_df['loan_type'] == 4]\n",
    "# Keep only loan originations\n",
    "agg_loans_all_df = agg_loans_all_df[agg_loans_all_df['action_taken_type'] == 1]\n",
    "# Keep only this state\n",
    "agg_loans_all_df = agg_loans_all_df[agg_loans_all_df['state'] == thestate]\n",
    "\n",
    "agg_loans_all_df = agg_loans_all_df.assign(\n",
    "    GEOID = agg_loans_all_df.apply(lambda row: '14000US{0:02d}{1:03d}{2:04d}{3:02d}'.format(\n",
    "        row['state'], \n",
    "        row['county'], \n",
    "        int(str(row['census_tract'])[0:str(row['census_tract']).find('.')]),\n",
    "        int(str(row['census_tract'])[str(row['census_tract']).find('.')+1:])\n",
    "    ), axis=1))\n",
    "\n",
    "agg_loans_all_df.loc[\n",
    "    (agg_loans_all_df['GEOID'] == '14000US11001009801') & (agg_loans_all_df['census_tract'] == 98.1), \n",
    "'GEOID']  = '14000US11001009810'\n",
    "\n",
    "agg_loans_all_df = agg_loans_all_df.rename(columns={'activity_year': 'year'})\n",
    "agg_loans_all_df = agg_loans_all_df.set_index(['GEOID','year'])\n",
    "\n",
    "\n",
    "\n",
    "data_gdf = city_tracts_years_gdf.join(agg_loans_all_df[[x for x in agg_loans_all_df.columns if x not in city_tracts_years_gdf.columns]], how='left')\n",
    "# # data_gdf[['county_geodata','county_loandata']]\n",
    "\n",
    "print('converting columns to numeric...')\n",
    "numeric_columns = []\n",
    "numeric_columns += ['nLoans1', 'amtLoans1', 'nLoans100k', 'amtLoans100k']\n",
    "numeric_columns += ['nLoans250k', 'amtLoans250k', 'nLoansToSmallest', 'amtLoansToSmallest']\n",
    "\n",
    "# for x in numeric_columns:\n",
    "#     data_gdf.loc[:, x] = pandas.to_numeric(x, errors='coerce')\n",
    "print('calculating total loans...')\n",
    "data_gdf = data_gdf.assign(nLoans = data_gdf['nLoans1'] + data_gdf['nLoans100k'] + data_gdf['nLoans250k'])\n",
    "data_gdf = data_gdf.assign(amtLoans = data_gdf['amtLoans1'] + data_gdf['amtLoans100k'] + data_gdf['amtLoans250k'])\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Kept {0:,.0f} tract-years in {1:,.0f} seconds!'.format(len(data_gdf), e-s))\n",
    "#agg_loans_all_df.reset_index().groupby(['GEOID','year']).size().sort_values(ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add income groups, CRA levels, working loans for each tract-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "\n",
    "print('looking up income group names from income_group_total...')\n",
    "\n",
    "data_gdf = data_gdf.rename(columns = {'income_group_total': 'income_group_code'})\n",
    "data_gdf = data_gdf.assign(income_group = np.nan)\n",
    "\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 1, 'income_group'] = '< 10% of Median Family Income (MFI)'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 2, 'income_group'] = '10% to 20% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 3, 'income_group'] = '20% to 30% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 4, 'income_group'] = '30% to 40% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 5, 'income_group'] = '40% to 50% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 6, 'income_group'] = '50% to 60% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 7, 'income_group'] = '60% to 70% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 8, 'income_group'] = '70% to 80% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 9, 'income_group'] = '80% to 90% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 10, 'income_group'] = '90% to 100% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 11, 'income_group'] = '100% to 110% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 12, 'income_group'] = '110% to 120% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 13, 'income_group'] = '> 120% of MFI'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 14, 'income_group'] = 'unknown'\n",
    "\n",
    "print('Adding CRA income levels (low/moderate/middle/upper/unknown)...')\n",
    "# Get levels (low, moderate, middle, upper)\n",
    "data_gdf = data_gdf.assign(cra_level = np.nan)\n",
    "data_gdf.loc[(data_gdf['income_group_code'] >= 1) & (data_gdf['income_group_code'] <= 5), 'cra_level'] = 'low'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] >= 6) & (data_gdf['income_group_code'] <= 8), 'cra_level'] = 'moderate'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] >= 9) & (data_gdf['income_group_code'] <= 12), 'cra_level'] = 'middle'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] == 13), 'cra_level'] = 'upper'\n",
    "data_gdf.loc[(data_gdf['income_group_code'] == 14), 'cra_level'] = 'unknown'\n",
    "\n",
    "print('Getting CRA income levels for tracts where only CRA level was reported...')\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 101, 'cra_level'] = 'low'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 102, 'cra_level'] = 'moderate'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 103, 'cra_level'] = 'middle'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 104, 'cra_level'] = 'upper'\n",
    "data_gdf.loc[data_gdf['income_group_code'] == 105, 'cra_level'] = 'unknown'\n",
    "\n",
    "print('calculating working loans...')\n",
    "data_gdf = data_gdf.assign(avgSmallLoan = data_gdf['amtLoans1'] / data_gdf['nLoans1'])\n",
    "\n",
    "data_gdf = data_gdf.assign(nWorkingLoans = 0)\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] < 10000, \n",
    "                           'nWorkingLoans'] = data_gdf['nLoans'][data_gdf['avgSmallLoan'] < 10000] - data_gdf['nLoans1'][data_gdf['avgSmallLoan'] < 10000]\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] >= 10000, \n",
    "                           'nWorkingLoans'] = data_gdf['nLoans'][data_gdf['avgSmallLoan'] >= 10000]\n",
    "\n",
    "data_gdf = data_gdf.assign(amtWorkingLoans = 0)\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] < 10000, \n",
    "                           'amtWorkingLoans'] = data_gdf['amtLoans'][data_gdf['avgSmallLoan'] < 10000] - data_gdf['amtLoans1'][data_gdf['avgSmallLoan'] < 10000]\n",
    "data_gdf.loc[data_gdf['avgSmallLoan'] >= 10000, \n",
    "                           'amtWorkingLoans'] = data_gdf['amtLoans'][data_gdf['avgSmallLoan'] >= 10000]\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('backing up...')\n",
    "data_gdf_bk = data_gdf\n",
    "print('Kept {0:,.0f} tract-years in {1:} in {2:,.2f} seconds!'.format(len(data_gdf), city, e-s))\n",
    "print(data_gdf.groupby('income_group').size())\n",
    "print(data_gdf.groupby('cra_level').size())\n",
    "#print('\\n')\n",
    "#data_gdf[data_gdf['cra_level'] == 'unknown']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to jobs data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw jobs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# IMPORTANT NOTE ABOUT JOBS VARIABLES\n",
    "# Race, ethnicity, education, sex reported 2009 t.e.m. 2018\n",
    "# Firm age reported 2011 t.e.m. 2017\n",
    "\n",
    "print('reading state jobs data for {0:}...'.format(state_abbrev.upper()))\n",
    "\n",
    "statejobfiles = sorted([jobs_dir+x for x in os.listdir(jobs_dir) if '{0:}'.format(state_abbrev) in x and '{0:}.'.format(state_abbrev) in x])\n",
    "#print(statejobfiles)\n",
    "state_raw_jobs_df = pandas.DataFrame()\n",
    "for thisfile in statejobfiles:\n",
    "    print('Reading {0:}...'.format(thisfile))\n",
    "    state_raw_jobs_df = pandas.concat((state_raw_jobs_df,pandas.read_csv(thisfile, index_col='rownumber', low_memory=False, keep_default_na=False)), axis=0)\n",
    "\n",
    "    \n",
    "print('constructing GEOIDs...')\n",
    "if (thestate < 10):\n",
    "    state_raw_jobs_df = state_raw_jobs_df.assign(GEOID = state_raw_jobs_df['w_geocode'].apply(lambda x: '14000US{0:02d}{1:}'.format(thestate,str(x)[1:10])))\n",
    "else:\n",
    "    state_raw_jobs_df = state_raw_jobs_df.assign(GEOID = state_raw_jobs_df['w_geocode'].apply(lambda x: '14000US{0:02d}{1:}'.format(thestate,str(x)[2:11])))\n",
    "\n",
    "    \n",
    "print('selecting raw jobs from {0:}...'.format(city))\n",
    "\n",
    "city_raw_jobs_df = state_raw_jobs_df[state_raw_jobs_df['GEOID'].isin(data_gdf.index.get_level_values('GEOID').tolist())]\n",
    "\n",
    "# print('backing up...')\n",
    "# city_raw_jobs_df_bk = city_raw_jobs_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Read jobs for {0:,.0f} block-group-years in {1:} in {2:,.0f} minutes {3:,.0f} seconds!'.format(len(city_raw_jobs_df), city, np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "city_raw_jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum jobs over census tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup...')\n",
    "# city_raw_jobs_df = city_raw_jobs_df_bk\n",
    "\n",
    "#sum_columns = [x for x in city_raw_jobs_df.columns if x[0] == 'C']\n",
    "\n",
    "print('finding columns to sum over...')\n",
    "jobs_metadata_df = pandas.read_csv(code_lookup_dir+'wac_jobs_metadata.csv', encoding='utf-8', index_col='varnum')\n",
    "\n",
    "jobs_total_columns = jobs_metadata_df[jobs_metadata_df['variable'] == 'C000']['variable'].tolist()\n",
    "jobs_age_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:2] == 'CA')]['variable'].tolist()\n",
    "jobs_earnings_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:2] == 'CE')]['variable'].tolist()\n",
    "jobs_sector_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:3] == 'CNS')]['variable'].tolist()\n",
    "jobs_race_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:2] == 'CR')]['variable'].tolist()\n",
    "jobs_ethnicity_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:2] == 'CT')]['variable'].tolist()\n",
    "jobs_education_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:2] == 'CD')]['variable'].tolist()\n",
    "jobs_sex_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:2] == 'CS')]['variable'].tolist()\n",
    "jobs_firm_age_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:3] == 'CFA')]['variable'].tolist()\n",
    "jobs_firm_size_columns = jobs_metadata_df[jobs_metadata_df['variable'].apply(lambda x: x[0:3] == 'CFS')]['variable'].tolist()\n",
    "\n",
    "jobs_columns = jobs_total_columns + jobs_age_columns + jobs_earnings_columns + jobs_sector_columns\n",
    "jobs_columns += jobs_race_columns + jobs_ethnicity_columns + jobs_education_columns + jobs_sex_columns\n",
    "jobs_columns += jobs_firm_age_columns + jobs_firm_size_columns\n",
    "\n",
    "jobs_metadata_df = jobs_metadata_df.set_index('variable')\n",
    "\n",
    "print('summing those jobs into census tracts...')\n",
    "city_jobs_df = pandas.DataFrame(data=None, columns=['GEOID','year']+jobs_columns)\n",
    "city_jobs_df = city_jobs_df.set_index(['GEOID','year'])\n",
    "\n",
    "\n",
    "\n",
    "for x in jobs_columns:\n",
    "#    print('summing {0:} across census tracts...'.format(x))\n",
    "    city_jobs_df.loc[:,:][x] = city_raw_jobs_df.groupby(['GEOID', 'year'])[x].sum()\n",
    "\n",
    "    \n",
    "# Which variables appear in which years?    \n",
    "\n",
    "# city_jobs_df.xs(\n",
    "#     city_jobs_df.sample(1).index.get_level_values('GEOID').values[0], level='GEOID'\n",
    "# )[jobs_ethnicity_columns]#.sum()\n",
    "\n",
    "# for thiscol in jobs_firm_size_columns:\n",
    "#     print(thiscol)\n",
    "#     print('----')\n",
    "#     for this_one_year in city_jobs_df.index.get_level_values('year').drop_duplicates().tolist():\n",
    "\n",
    "#         print('{0:.0f}: {1:,.0f}'.format(this_one_year, city_jobs_df.xs(this_one_year, level='year')[thiscol].sum()))\n",
    "#     print('\\n')\n",
    "\n",
    "\n",
    "# e = time.time()\n",
    "# g = g + (e-s)\n",
    "# for x in city_jobs_df[jobs_columns_we_want].columns:\n",
    "#     print('variable: {0:}\\t\\tdescription:{1:}'.format(x, jobs_metadata_df['description'][jobs_metadata_df.index == x].tolist()[0]))\n",
    "\n",
    "print('creating new jobs columns...')\n",
    "city_jobs_df = city_jobs_df.assign(total_jobs = city_jobs_df['C000'])\n",
    "city_jobs_df = city_jobs_df.assign(sb_jobs = city_jobs_df['C000'] - city_jobs_df['CFS05'])\n",
    "\n",
    "\n",
    "jobs_columns_we_want = ['total_jobs', 'sb_jobs']   # later we will focus on total and small business jobs\n",
    "\n",
    "\n",
    "print('Firm age and firm size: copying 2017 jobs into 2018...')\n",
    "city_age_size_jobs_2018_df = city_jobs_df.xs(2017, level='year')#.reset_index()\n",
    "city_age_size_jobs_2018_df = city_age_size_jobs_2018_df.assign(year = 2018).reset_index().set_index(['GEOID', 'year'])\n",
    "\n",
    "\n",
    "for thisgeoid in city_jobs_df.xs(2017, level='year').index.values.tolist():\n",
    "    city_jobs_df.loc[thisgeoid, 2018][jobs_firm_age_columns+jobs_firm_size_columns+['total_jobs','sb_jobs']] = city_age_size_jobs_2018_df.loc[thisgeoid, 2018][jobs_firm_age_columns+jobs_firm_size_columns]\n",
    "\n",
    "print('All columns: copying 2018 jobs into 2019...')\n",
    "city_jobs_2019_df = city_jobs_df.xs(2018, level='year').reset_index()\n",
    "city_jobs_2019_df = city_jobs_2019_df.assign(year = 2019)#.set_index(['GEOID', 'year'])\n",
    "\n",
    "\n",
    "city_jobs_df = city_jobs_df.reset_index()\n",
    "city_jobs_2019_df = city_jobs_2019_df[city_jobs_df.columns.tolist()]\n",
    "city_jobs_df = pandas.concat(\n",
    "    (city_jobs_df, city_jobs_2019_df), axis=0).sort_values(['GEOID', 'year']\n",
    "                                                          ).reset_index(drop=True).set_index(['GEOID', 'year'])\n",
    "\n",
    "\n",
    "# print('backing up...')\n",
    "# city_jobs_df_bk = city_jobs_df\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Summed {0:,.0f} jobs across {1:,.0f} tract-years in {2:,.0f} minutes {3:,.0f} seconds!'.format(len(jobs_columns), len(city_jobs_df), np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "\n",
    "#jobs_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge jobs with rest of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "# city_jobs_df = city_jobs_df_bk\n",
    "\n",
    "\n",
    "#data_gdf.head(1).T #['COUNTYFP','NAME','year']\n",
    "data_gdf = data_gdf.join(city_jobs_df)\n",
    "\n",
    "# for x in jobs_columns_we_want:\n",
    "#     data_gdf.loc[:, x] = data_gdf[x].fillna(0)\n",
    "\n",
    "data_gdf = data_gdf.sort_index()\n",
    "\n",
    "print('done')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Added jobs data for {0:,.0f} tract-years!'.format(len(data_gdf[data_gdf['CFS01'].notnull()])))\n",
    "\n",
    "# print('backing up...')\n",
    "# data_gdf_bk = data_gdf\n",
    "\n",
    "data_gdf.tail(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get loans per job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "#sbjobs_column = jobs_varnames_df[jobs_varnames_df['description'].apply(lambda x: '0-19' in x)].index.values[0]\n",
    "#loans_columns = []\n",
    "#data_gdf[sbjobs_column]\n",
    "print('Calulating loans per job (total and with firm size 0-19)...')\n",
    "\n",
    "data_gdf = data_gdf.assign(nLoans1_per_totaljob = data_gdf['nLoans1'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans1_per_totaljob = data_gdf['amtLoans1'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoans100k_per_totaljob = data_gdf['nLoans100k'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans100k_per_totaljob = data_gdf['amtLoans100k'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoans250k_per_totaljob = data_gdf['nLoans250k'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans250k_per_totaljob = data_gdf['amtLoans250k'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoansToSmallest_per_totaljob = data_gdf['nLoansToSmallest'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoansToSmallest_per_totaljob = data_gdf['amtLoansToSmallest'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoans_per_totaljob = data_gdf['nLoans'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans_per_totaljob = data_gdf['amtLoans'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(nWorkingLoans_per_totaljob = data_gdf['nWorkingLoans'] / data_gdf['total_jobs'])\n",
    "data_gdf = data_gdf.assign(amtWorkingLoans_per_totaljob = data_gdf['amtWorkingLoans'] / data_gdf['total_jobs'])\n",
    "\n",
    "data_gdf = data_gdf.assign(nLoans1_per_sbjob = data_gdf['nLoans1'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans1_per_sbjob = data_gdf['amtLoans1'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoans100k_per_sbjob = data_gdf['nLoans100k'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans100k_per_sbjob = data_gdf['amtLoans100k'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoans250k_per_sbjob = data_gdf['nLoans250k'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans250k_per_sbjob = data_gdf['amtLoans250k'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoansToSmallest_per_sbjob = data_gdf['nLoansToSmallest'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoansToSmallest_per_sbjob = data_gdf['amtLoansToSmallest'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(nLoans_per_sbjob = data_gdf['nLoans'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(amtLoans_per_sbjob = data_gdf['amtLoans'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(nWorkingLoans_per_sbjob = data_gdf['nWorkingLoans'] / data_gdf['sb_jobs'])\n",
    "data_gdf = data_gdf.assign(amtWorkingLoans_per_sbjob = data_gdf['amtWorkingLoans'] / data_gdf['sb_jobs'])\n",
    "\n",
    "\n",
    "per_job_columns = ['nLoans1_per_totaljob', 'amtLoans1_per_totaljob', 'nLoans100k_per_totaljob']\n",
    "per_job_columns += ['amtLoans100k_per_totaljob', 'nLoans250k_per_totaljob', 'amtLoans250k_per_totaljob']\n",
    "per_job_columns += ['nLoansToSmallest_per_totaljob', 'amtLoansToSmallest_per_totaljob']\n",
    "per_job_columns += ['nLoans_per_totaljob', 'amtLoans_per_totaljob', 'nWorkingLoans_per_totaljob']\n",
    "per_job_columns += ['amtWorkingLoans_per_totaljob', 'nLoans1_per_sbjob', 'amtLoans1_per_sbjob']\n",
    "per_job_columns += ['nLoans100k_per_sbjob', 'amtLoans100k_per_sbjob', 'nLoans250k_per_sbjob']\n",
    "per_job_columns += ['amtLoans250k_per_sbjob', 'nLoansToSmallest_per_sbjob', 'amtLoansToSmallest_per_sbjob']\n",
    "per_job_columns += ['nLoans_per_sbjob', 'amtLoans_per_sbjob', 'nWorkingLoans_per_sbjob']\n",
    "per_job_columns += ['amtWorkingLoans_per_sbjob']\n",
    "\n",
    "\n",
    "print('recoding zero/zero values to zero and infinite values to NaN...')\n",
    "for x in data_gdf[per_job_columns]:\n",
    "    data_gdf.loc[:, x] = data_gdf[x].fillna(0)\n",
    "    data_gdf.loc[data_gdf[x] == np.inf, x] = np.nan\n",
    "\n",
    "# print('backing up...')\n",
    "# data_gdf_bk = data_gdf\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done in {0:,.1f} seconds!'.format(e-s)) \n",
    "data_gdf[(data_gdf['nWorkingLoans'] == 0) | (data_gdf['CFS01'] == 0)][\n",
    "    ['nWorkingLoans', 'amtWorkingLoans', 'total_jobs', 'sb_jobs', 'nWorkingLoans_per_totaljob', 'amtWorkingLoans_per_totaljob', 'nWorkingLoans_per_sbjob', 'amtWorkingLoans_per_sbjob']\n",
    "].sample(4).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "\n",
    "acs5_estimates_df = pandas.DataFrame()\n",
    "acs5_margins_of_error_df = pandas.DataFrame()\n",
    "\n",
    "print('Getting ACS 5-year census data...')\n",
    "\n",
    "\n",
    "for thisyear in years:\n",
    "    print('\\t{0:.0f} estimates...'.format(thisyear))\n",
    "    acs5_estimates_this_year_df = pandas.read_csv(acs5_basedir+'{0:.0f}/estimates/estimates_acs{0:.0f}_tract_for_cra_analysis_mac.csv'.format(thisyear), low_memory=False, encoding='utf-8', index_col='GEOID')\n",
    "    acs5_estimates_this_year_df = acs5_estimates_this_year_df.drop([x for x in acs5_estimates_this_year_df.columns.tolist() if 'unnamed' in x.lower()], axis=1)\n",
    "\n",
    "    acs5_estimates_this_year_df = acs5_estimates_this_year_df.assign(year = thisyear)\n",
    "    acs5_estimates_df = pandas.concat((acs5_estimates_df, acs5_estimates_this_year_df), axis=0, sort=False)\n",
    "        \n",
    "    print('\\t{0:.0f} margins of error...'.format(thisyear))\n",
    "    acs5_margins_of_error_this_year_df = pandas.read_csv(acs5_basedir+'{0:.0f}/margins_of_error/margins_of_error_acs{0:.0f}_tract_for_cra_analysis_mac.csv'.format(thisyear), low_memory=False, encoding='utf-8', index_col='GEOID')\n",
    "    if (thisyear <= 2014):\n",
    "        acs5_margins_of_error_this_year_df = acs5_margins_of_error_this_year_df.drop([x for x in acs5_margins_of_error_this_year_df.columns.tolist() if 'unnamed' in x.lower()], axis=1)\n",
    "    acs5_margins_of_error_this_year_df = acs5_margins_of_error_this_year_df.assign(year = thisyear)\n",
    "    acs5_margins_of_error_df = pandas.concat((acs5_margins_of_error_df, acs5_margins_of_error_this_year_df), axis=0, sort=False)\n",
    "\n",
    "    \n",
    "    \n",
    "print('discarding block groups, keeping census tracts...')\n",
    "acs5_estimates_df = acs5_estimates_df.reset_index()\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df.reset_index()\n",
    "\n",
    "acs5_estimates_df = acs5_estimates_df[acs5_estimates_df['GEOID'].apply(lambda x: x[0:3] == '140')]\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df[acs5_margins_of_error_df['GEOID'].apply(lambda x: x[0:3] == '140')]\n",
    "\n",
    "\n",
    "print('converting to numeric...')\n",
    "for x in ['B08013_001', 'B19013_001', 'B19013A_001', 'B19013B_001', 'B19113_001', 'B25077_001', 'B25035_001']:\n",
    "    acs5_estimates_df.loc[:, x] = pandas.to_numeric(acs5_estimates_df[x], errors='coerce')\n",
    "    acs5_margins_of_error_df.loc[:, x] = pandas.to_numeric(acs5_margins_of_error_df[x], errors='coerce')\n",
    "\n",
    "print('renaming error columns...')\n",
    "orig_column_names = acs5_margins_of_error_df.columns.tolist()[6:-6]\n",
    "err_column_names = []\n",
    "for x in orig_column_names:\n",
    "    err_column_names.append(x+'_err')\n",
    "acs5_margins_of_error_df.columns = acs5_margins_of_error_df.columns.tolist()[0:6]+err_column_names+acs5_margins_of_error_df.columns.tolist()[-6:]\n",
    "\n",
    "print('joining to the rest of the data...')\n",
    "acs5_estimates_df = acs5_estimates_df.set_index(['GEOID', 'year'])\n",
    "acs5_margins_of_error_df = acs5_margins_of_error_df.set_index(['GEOID', 'year'])\n",
    "\n",
    "\n",
    "data_gdf = data_gdf.join(acs5_estimates_df[acs5_estimates_df.index.isin(data_gdf.index)][acs5_estimates_df.columns.tolist()[:-5]+acs5_estimates_df.columns.tolist()[-1:]], how='left', lsuffix='_loans_jobs', rsuffix='_acs5')\n",
    "data_gdf = data_gdf.join(acs5_margins_of_error_df[acs5_margins_of_error_df.index.isin(data_gdf.index)][acs5_margins_of_error_df.columns.tolist()[6:-3]+acs5_margins_of_error_df.columns.tolist()[-1:]], how='left', lsuffix='_loans_jobs', rsuffix='_acs5')\n",
    "\n",
    "# print('backing up...')\n",
    "# data_gdf_bk = data_gdf\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('{0:}: Kept {1:,.0f} tract-years in {2:,.0f} minutes {3:.0f} seconds!'.format(city, len(data_gdf), np.floor((e-s)/60), np.floor((e-s)%60)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate composite demographic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acs5_columns = [x for x in data_gdf.columns.tolist() if x[0] == 'B']\n",
    "acs5_metadata_df = pandas.read_csv(acs5_basedir+'2019/variables/variables_acs_5yr_all.csv', encoding='utf-8', low_memory=False, index_col='rownumber')\n",
    "\n",
    "acs5_metadata_df[acs5_metadata_df['variable'].isin(acs5_columns)][['variable','description']][30:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "\n",
    "print('\\ncalculating and renaming estimates columns for IVs...')\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('population 25 and older with bachelors degree or higher (B15003_022 t.e.m. B15003_025...')\n",
    "h = data_gdf['B15003_022'] + data_gdf['B15003_023'] + data_gdf['B15003_024']  + data_gdf['B15003_025']\n",
    "data_gdf = data_gdf.assign(educated = pandas.to_numeric(h, errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...population 25 years and older...')\n",
    "data_gdf = data_gdf.rename(columns={'B15003_001': 'pop_25plus'})\n",
    "\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...householder sex & race, unempoyment, poverty, household type, home value, home age, travel time...')\n",
    "data_gdf = data_gdf.rename(columns = {  \n",
    "    'B11001_006': 'female_householders',\n",
    "    'B11001A_001': 'white_householders',\n",
    "    'B11001B_001': 'black_householders',\n",
    "    'B23025_005': 'unemployed_16plus',\n",
    "    'B17001_002': 'poverty_past_12_months',\n",
    "    'B25077_001': 'median_home_value',\n",
    "    'B25035_001': 'median_year_built',\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...race, owner-occupied units, mfi, vacants...')\n",
    "data_gdf = data_gdf.rename(columns = {    \n",
    "    'B02001_002': 'pop_white',\n",
    "    'B02001_003': 'pop_black',\n",
    "    'B25003_002': 'owner_occ_housing_units',\n",
    "    'B19113_001': 'mfi',\n",
    "    'B25002_002': 'occupied_housing_units',\n",
    "    'B25002_003': 'vacant_housing_units',\n",
    "    'B11001_002': 'household_type_family',\n",
    "    'B11001_007': 'household_type_nonfamily',\n",
    "    'B25003_003': 'tenure_rent',\n",
    "    'B25034_001': 'year_built'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('....comparison variables: total population, total households, poverty status, rentage...')\n",
    "data_gdf = data_gdf.rename(columns = {\n",
    "    'B01001_001': 'pop_total',\n",
    "    'B02001_001': 'pop_by_race_total',\n",
    "    'B11001_001': 'total_households',\n",
    "    'B25002_001': 'total_housing_units',\n",
    "    'B23025_002': 'labor_force_16plus',\n",
    "    'B17001_001': 'poverty_status_known',\n",
    "    'B25003_001': 'tenure_total'\n",
    "})\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('MFI & median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "    print('median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "data_gdf.loc[data_gdf['mfi'] == '.', 'mfi'] = pandas.to_numeric(data_gdf['mfi'][data_gdf['mfi'] == '.'], errors='coerce')\n",
    "data_gdf.loc[data_gdf['median_home_value'] == '.', 'median_home_value'] = pandas.to_numeric(data_gdf['median_home_value'][data_gdf['median_home_value'] == '.'], errors='coerce')\n",
    "\n",
    "\n",
    "# print('backing up...')\n",
    "# data_gdf_bk = data_gdf\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "\n",
    "print('Done in {0:,.0f} seconds!'.format(e-s))\n",
    "\n",
    "\n",
    "# unrenamed_columns = []\n",
    "# for x in data_gdf.columns:\n",
    "#     if ((x[0] == 'B') and ('_err' not in x)):\n",
    "#         unrenamed_columns.append(x)\n",
    "# acs5_metadata_df[acs5_metadata_df['variable'].isin(unrenamed_columns)][-9:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create error calculating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Guide on how to calculate errors in percentages:\n",
    "# https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch08.pdf\n",
    "    \n",
    "## Aggregating Data Across Population Subgroups: add error for each group in quadrature, divide by 1.645 for serr\n",
    "s = time.time()\n",
    "\n",
    "def find_serr_educated(row):\n",
    "\n",
    "    return pandas.to_numeric(np.sqrt(row['B15003_022_err']**2 + row['B15003_023_err']**2 + row['B15003_024_err']**2 + row['B15003_025_err']**2\n",
    "                                ) / 1.645, errors='coerce')\n",
    "\n",
    "# def find_serr_householders(row):\n",
    "#     return pandas.to_numeric(np.sqrt(row['B11001_002_err']**2 + row['B11001_007_err']**2 \n",
    "#                                 ) / 1.645, errors='coerce')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Defined standard-error-calculating functions!')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup..')\n",
    "# data_gdf = data_gdf_bk\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...standard errors for hs graduates 25 and older (using custom serr-finding function...')\n",
    "data_gdf = data_gdf.assign(educated_serr = pandas.to_numeric(data_gdf.apply(lambda row: find_serr_educated(row), axis=1), errors='coerce'))\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('...margins of error for householder sex & race, unempoyment, poverty, home value, home age...')\n",
    "data_gdf = data_gdf.rename(columns = {     \n",
    "    'B11001_001_err': 'total_households_err',\n",
    "    'B11001_006_err': 'female_householders_err',\n",
    "    'B11001A_001_err': 'black_householders_err',\n",
    "    'B11001B_001_err': 'white_householders_err',\n",
    "    'B23025_005_err': 'unemployed_16plus_err',\n",
    "    'B17001_002_err': 'poverty_past_12_months_err',\n",
    "    'B25077_001_err': 'median_home_value_err',\n",
    "    'B25035_001_err': 'median_year_built_err',\n",
    "    'B25003_002_err': 'owner_occ_housing_units_err',\n",
    "    'B19113_001_err': 'mfi_err',\n",
    "    'B25002_002_err': 'occupied_housing_units_err',\n",
    "    'B25002_003_err': 'vacant_housing_units_err',\n",
    "    'B11001_002_err': 'household_type_family_err',\n",
    "    'B11001_007_err': 'household_type_nonfamily_err',\n",
    "    'B25003_003_err': 'tenure_rent_err',\n",
    "    'B15003_001_err': 'pop_25plus_err'\n",
    "})\n",
    "\n",
    "data_gdf = data_gdf.assign(household_type_total = data_gdf['household_type_family'] + data_gdf['household_type_nonfamily'])\n",
    "data_gdf = data_gdf.assign(household_type_total_err = data_gdf['household_type_family_err'] + data_gdf['household_type_nonfamily_err'])\n",
    "\n",
    "if (debug >= 1):\n",
    "    print('MFI & median home value: substituting \".\" with np.nan, converting to numeric...')\n",
    "data_gdf.loc[data_gdf['mfi_err'] == '.', 'mfi_err'] = pandas.to_numeric(data_gdf['mfi_err'][data_gdf['mfi_err'] == '.'], errors='coerce')\n",
    "data_gdf.loc[data_gdf['median_home_value_err'] == '.', 'median_home_value_err'] = pandas.to_numeric(data_gdf['median_home_value_err'][data_gdf['median_home_value_err'] == '.'], errors='coerce')\n",
    "\n",
    "\n",
    "print('\\ncalculating and renaming margins of error for comparison variables...')\n",
    "if (debug >= 1):\n",
    "    print('...race, owner-occupied units, mfi...')\n",
    "data_gdf = data_gdf.rename(columns = {\n",
    "    'B01001_001_err': 'pop_total_err',\n",
    "    'B02001_002_err': 'pop_white_err',\n",
    "    'B02001_003_err': 'pop_black_err',\n",
    "    'B02001_001_err': 'pop_by_race_total',\n",
    "    'B17001_001_err': 'poverty_status_known_err',\n",
    "    'B02001_001_err': 'pop_by_race_total_err',\n",
    "    'B25002_001_err': 'total_housing_units_err',\n",
    "    'B23025_002_err': 'labor_force_16plus_err',\n",
    "    'B25003_001_err': 'tenure_total_err',\n",
    "    \n",
    "    \n",
    "})\n",
    "\n",
    "# if (debug >= 1):\n",
    "#     print('...total householders...')\n",
    "# data_gdf = data_gdf.assign(total_householders_serr = pandas.to_numeric(data_gdf.apply(lambda row: find_serr_householders(row), axis=1), errors='coerce'))\n",
    "\n",
    "\n",
    "#print('dropping columns we do not care about...')\n",
    "# columns_do_not_care = []\n",
    "# columns_do_not_care += ['B11001_002', 'B11001_007']\n",
    "# columns_do_not_care += ['B11001_002_err', 'B11001_007_err']\n",
    "# columns_do_not_care += ['B01001_048_err','B01001_049_err']#,'STATE']\n",
    "#data_gdf = data_gdf.drop(columns_do_not_care, axis=1)\n",
    "\n",
    "\n",
    "# print('Calculated errors for all columns!')\n",
    "# print('backing up...')\n",
    "# data_gdf_bk = data_gdf\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "unrenamed_columns_with_errs = []\n",
    "\n",
    "for x in data_gdf.columns:\n",
    "#    print(x)\n",
    "    if ((x[0] == 'B') and ('_err' in x)):\n",
    "        unrenamed_columns_with_errs.append(x)\n",
    "\n",
    "unrenamed_columns = []        \n",
    "for y in unrenamed_columns_with_errs:\n",
    "    unrenamed_columns.append(y[:-4])\n",
    "#acs5_metadata_df[acs5_metadata_df['variable'].isin(unrenamed_columns)][80:]\n",
    "#print(unrenamed_columns)\n",
    "\n",
    "e = time.time()\n",
    "print('Done in {0:,.0f} seconds!'.format(e-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare to percentify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "vars_for_percentification = ['pop_white', 'pop_black', 'black_householders', 'white_householders']\n",
    "vars_for_percentification += ['owner_occ_housing_units', 'educated', 'female_householders']\n",
    "vars_for_percentification += ['unemployed_16plus', 'poverty_past_12_months']\n",
    "vars_for_percentification += ['tenure_rent', 'household_type_family']\n",
    "\n",
    "vars_for_percentification += ['pop_white_err', 'pop_black_err', 'black_householders_err', 'white_householders_err']\n",
    "vars_for_percentification += ['owner_occ_housing_units_err', 'educated_serr', 'female_householders_err']\n",
    "vars_for_percentification += ['unemployed_16plus_err', 'poverty_past_12_months_err']\n",
    "vars_for_percentification += ['tenure_rent_err', 'household_type_family_err']\n",
    "\n",
    "vars_for_percentification += ['pop_total', 'total_householders', 'pop_by_race_total', 'pop_25plus', 'labor_force_16plus']\n",
    "vars_for_percentification += ['poverty_status_known', 'vacant_housing_units', 'total_housing_units']\n",
    "vars_for_percentification += ['tenure_total', 'household_type_total']\n",
    "\n",
    "vars_for_percentification += ['pop_totatl_err', 'total_householders_serr', 'pop_by_race_total_err', 'pop_25plus_err', 'labor_force_16plus_err']\n",
    "vars_for_percentification += ['poverty_status_known_err', 'vacant_housing_units_err', 'total_housing_units_err']\n",
    "vars_for_percentification += ['tenure_total_err', 'household_type_total_err']\n",
    "#vars_for_percentification\n",
    "#city_tracts_years_df[vars_for_percentification].columns.tolist()\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate percentages for needed demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "data_gdf = data_gdf.assign(pct_white = pandas.to_numeric((data_gdf['pop_white'] / data_gdf['pop_by_race_total']), errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_black = pandas.to_numeric((data_gdf['pop_black'] / data_gdf['pop_by_race_total']), errors='coerce'))\n",
    "\n",
    "data_gdf = data_gdf.assign(pct_white_householders = pandas.to_numeric((data_gdf['white_householders'] / data_gdf['total_households']), errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_black_householders = pandas.to_numeric((data_gdf['black_householders'] / data_gdf['total_households']), errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_female_householders = pandas.to_numeric((data_gdf['female_householders'] / data_gdf['total_households']), errors='coerce'))\n",
    "\n",
    "data_gdf = data_gdf.assign(peducated = pandas.to_numeric(data_gdf['educated'], errors='coerce') / pandas.to_numeric(data_gdf['pop_25plus'], errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_unemployed = pandas.to_numeric(data_gdf['unemployed_16plus'], errors='coerce') / pandas.to_numeric(data_gdf['labor_force_16plus'], errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_poverty = pandas.to_numeric(data_gdf['poverty_past_12_months'], errors='coerce') / pandas.to_numeric(data_gdf['poverty_status_known'], errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_vacant = pandas.to_numeric(data_gdf['vacant_housing_units'], errors='coerce') / pandas.to_numeric(data_gdf['total_housing_units'], errors='coerce'))\n",
    "\n",
    "data_gdf = data_gdf.assign(pct_rent = pandas.to_numeric(data_gdf['tenure_rent'], errors='coerce') / pandas.to_numeric(data_gdf['tenure_total'], errors='coerce'))\n",
    "data_gdf = data_gdf.assign(pct_household_family = pandas.to_numeric(data_gdf['household_type_family'], errors='coerce') / pandas.to_numeric(data_gdf['household_type_total'], errors='coerce'))\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')\n",
    "#data_gdf.columns.tolist()\n",
    "\n",
    "data_gdf.sample(1).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to calculate errors in percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guide on how to do this:\n",
    "#### https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch08.pdf\n",
    "\n",
    "# X and Y are the measured values (not the errors) - X for the subsgroup and Y for the whole sample\n",
    "# Let P = X/Y  (the proportion we calculated in the last step)\n",
    "# dX and dY are the measured errors\n",
    "# dP = (1/Y) * np.sqrt(dX**2 - (P**2 * dY**2))\n",
    "# Standard error of P is dP/1.645\n",
    "#### this calculation is done verbosely in fnid_pop_white_serr, quickly in other functions\n",
    "\n",
    "s = time.time()\n",
    "def find_errors_in_pct(X, Y, dX, dY, verboselevel = 0):\n",
    "    try:\n",
    "        P = X / Y\n",
    "        oneoverY = 1 / Y\n",
    "        dXsq = dX**2\n",
    "        dYsq = dY**2\n",
    "        Psq = P**2\n",
    "        PsqdYsq = Psq * dYsq\n",
    "        if (PsqdYsq <= dXsq):\n",
    "            underroot = dXsq - PsqdYsq\n",
    "        else:\n",
    "            underroot = dXsq + PsqdYsq\n",
    "        rooty = np.sqrt(underroot)\n",
    "        dP = oneoverY * rooty\n",
    "        SE = dP / 1.645\n",
    "        if (verboselevel >= 2):\n",
    "#            print('X = pop_white, Y = pop_total')\n",
    "            print('X = {0:.0f}, dX = {1:.0f} ({2:.1%} error)'.format(X, dX, dX/X))\n",
    "            print('Y = {0:.0f}, dY = {1:.0f} ({2:.1%} error)'.format(Y, dY, dY/Y))\n",
    "        if (verboselevel >= 3):\n",
    "            print('P = {0:.3f}'.format(P))\n",
    "            print('dXsq = {0:.0f}, dYsq = {1:.0f}, Psq = {2:.3f}'.format(dXsq, dYsq, Psq))\n",
    "            print('PsqdYsq = {0:.0f}, underroot = {1:.0f}, rooty = {2:.3f}'.format(PsqdYsq, underroot, rooty))\n",
    "            print('dP = {0:.3f}'.format(dP))\n",
    "            print('SE = {0:.3f}'.format(SE))\n",
    "        if (verboselevel >= 2):\n",
    "            print('RESULT: {0:.2%} +/- {1:.2%}'.format(P, SE)) \n",
    "            print('\\n')\n",
    "        return pandas.to_numeric(SE, errors='coerce')\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "    \n",
    "e = time.time()\n",
    "g = g + (e-s)    \n",
    "print('Defined functions to calculate standard errors in percentages!')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate errors in percntages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboselevel = 0\n",
    "s = time.time()\n",
    "\n",
    "print('Calculating errors in percentages...')\n",
    "data_gdf = data_gdf.assign(pct_white_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_black_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_white_householders_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_black_householders_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_female_householders_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(peducated_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_unemployed_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_poverty_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_vacant_serr = np.nan)\n",
    "\n",
    "\n",
    "data_gdf = data_gdf.assign(pct_rent_serr = np.nan)\n",
    "data_gdf = data_gdf.assign(pct_household_family_serr = np.nan)\n",
    "\n",
    "\n",
    "#data_gdf.loc[:, \n",
    "#              'poverty_status_known_last12months_total_err'] = pandas.to_numeric(data_gdf['poverty_status_known_last12months_total_err'], errors='coerce')\n",
    "\n",
    "\n",
    "for ix, thisrow in data_gdf.iterrows():\n",
    "    if (verboselevel >= 2):\n",
    "        print('Census tract {0:}...'.format(ix))\n",
    "    #print('pct_white_serr...')\n",
    "    data_gdf.loc[ix, 'pct_white_serr'] = find_errors_in_pct(thisrow['pop_white'], thisrow['pop_by_race_total'], thisrow['pop_white_err'], thisrow['pop_by_race_total_err'], verboselevel)\n",
    "    #print('pct_black_serr...')\n",
    "    data_gdf.loc[ix, 'pct_black_serr'] = find_errors_in_pct(thisrow['pop_black'], thisrow['pop_by_race_total'], thisrow['pop_black_err'], thisrow['pop_by_race_total_err'], verboselevel)\n",
    "    #print('pct_white_householders_serr...')\n",
    "    data_gdf.loc[ix, 'pct_white_householders_serr'] = find_errors_in_pct(thisrow['white_householders'], thisrow['total_households'], thisrow['white_householders_err'], thisrow['total_households_err'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_black_householders_serr'] = find_errors_in_pct(thisrow['white_householders'], thisrow['total_households'], thisrow['white_householders_err'], thisrow['total_households_err'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_female_householders_serr'] = find_errors_in_pct(thisrow['white_householders'], thisrow['total_households'], thisrow['white_householders_err'], thisrow['total_households_err'], verboselevel)\n",
    "    \n",
    "    data_gdf.loc[ix, 'peducated_serr'] = find_errors_in_pct(thisrow['educated'], thisrow['pop_25plus'], thisrow['educated_serr'], thisrow['pop_25plus_err'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_unemployed_serr'] = find_errors_in_pct(thisrow['unemployed_16plus'], thisrow['unemployed_16plus_err'], thisrow['labor_force_16plus_err'], thisrow['labor_force_16plus_err'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_poverty_serr'] = find_errors_in_pct(thisrow['poverty_past_12_months'], thisrow['poverty_status_known'], thisrow['poverty_past_12_months_err'], thisrow['poverty_status_known_err'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_vacant_serr'] = find_errors_in_pct(thisrow['vacant_housing_units'], thisrow['total_housing_units'], thisrow['vacant_housing_units_err'], thisrow['total_housing_units_err'], verboselevel)\n",
    "\n",
    "    data_gdf.loc[ix, 'pct_rent_serr'] = find_errors_in_pct(thisrow['tenure_rent'], thisrow['tenure_total'], thisrow['tenure_rent_err'], thisrow['tenure_total_err'], verboselevel)\n",
    "    data_gdf.loc[ix, 'pct_household_family_serr'] = find_errors_in_pct(thisrow['household_type_family'], thisrow['household_type_total'], thisrow['household_type_family_err'], thisrow['household_type_total_err'], verboselevel)\n",
    "\n",
    "if (verboselevel >= 1):\n",
    "    for ix, thisrow in data_gdf.iterrows():\n",
    "        print('Census tract {0:,.0f}'.format(ix))\n",
    "        print('{0:,.0f} +/- {1:,.0f} white'.format(\n",
    "            thisrow['pop_white'], thisrow['pop_white_err']\n",
    "        ))\n",
    "        print('{0:,.0f} +/- {1:,.0f} total'.format(\n",
    "            thisrow['pop_total'], thisrow['pop_total_err']\n",
    "        ))\n",
    "        print('{0:.1%} +/- {1:.1%}'.format(\n",
    "            thisrow['pct_white'], thisrow['pct_white_serr']\n",
    "        ))\n",
    "        print('\\n')\n",
    "\n",
    "# print('backing up...')\n",
    "# data_gdf_bk = data_gdf\n",
    "    \n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct for inflation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get inflation values from Board of Labor Statistics Consumer Price Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "baseline_year = latest_year\n",
    "\n",
    "inflator_df = pandas.DataFrame(data=None, columns=['theyear','themonth','thevalue'])\n",
    "\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = json.dumps({\"seriesid\": ['CUSR0000SA0'],\"startyear\":earliest_year, \"endyear\":latest_year})  # All items in U.S. city average, all urban consumers, seasonally adjusted: https://www.bls.gov/cpi/tables/supplemental-files/historical-cpi-u-202007.pdf\n",
    "p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
    "json_data = json.loads(p.text)\n",
    "\n",
    "if (json_data['status'] == 'REQUEST_NOT_PROCESSED'):\n",
    "    print('Request not processed, setting manually...')\n",
    "    inflator_df.loc[0, ['theyear', 'themonth', 'thevalue']] = [2019, 1, 251.712]\n",
    "    inflator_df.loc[1, ['theyear', 'themonth', 'thevalue']] = [2018, 1, 247.867]\n",
    "    inflator_df.loc[2, ['theyear', 'themonth', 'thevalue']] = [2017, 1, 242.839]\n",
    "    inflator_df.loc[3, ['theyear', 'themonth', 'thevalue']] = [2016, 1, 237.652]\n",
    "    inflator_df.loc[4, ['theyear', 'themonth', 'thevalue']] = [2015, 1, 234.747]\n",
    "    inflator_df.loc[5, ['theyear', 'themonth', 'thevalue']] = [2014, 1, 235.288]\n",
    "    inflator_df.loc[6, ['theyear', 'themonth', 'thevalue']] = [2013, 1, 231.679]\n",
    "    inflator_df.loc[7, ['theyear', 'themonth', 'thevalue']] = [2012, 1, 227.842]\n",
    "    inflator_df.loc[8, ['theyear', 'themonth', 'thevalue']] = [2011, 1, 221.187]\n",
    "    inflator_df.loc[9, ['theyear', 'themonth', 'thevalue']] = [2010, 1, 217.488]\n",
    "else:\n",
    "    cnt = 0\n",
    "    for series in json_data['Results']['series']:\n",
    "        seriesId = series['seriesID']\n",
    "#        print(seriesId)\n",
    "        for item in series['data']:\n",
    "            inflator_df.loc[cnt, ['theyear', 'themonth', 'thevalue']] = [int(item['year']), int(item['period'][1:]), float(item['value'])]\n",
    "            cnt += 1 \n",
    "inflator_df = inflator_df[inflator_df['themonth'] == 1]\n",
    "inflator_df = inflator_df.drop('themonth', axis=1)\n",
    "\n",
    "baseline_value = inflator_df[inflator_df['theyear'] == baseline_year]['thevalue'].values[0]\n",
    "inflator_df = inflator_df.assign(thefactor = baseline_value / inflator_df['thevalue'])\n",
    "inflator_df = inflator_df.set_index('theyear')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "inflator_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "\n",
    "money_columns = []\n",
    "money_columns += ['amtLoans1', 'amtLoans100k', 'amtLoans250k', 'amtLoansToSmallest']\n",
    "money_columns += ['avgSmallLoan', 'amtLoans', 'amtWorkingLoans']\n",
    "money_columns += ['amtLoans1_per_totaljob', 'amtLoans100k_per_totaljob', 'amtLoans250k_per_totaljob']\n",
    "money_columns += ['amtLoansToSmallest_per_totaljob', 'amtLoans_per_totaljob', 'amtWorkingLoans_per_totaljob']\n",
    "money_columns += ['amtLoans1_per_sbjob', 'amtLoans100k_per_sbjob', 'amtLoans250k_per_sbjob']\n",
    "money_columns += ['amtLoansToSmallest_per_sbjob', 'amtLoans_per_sbjob', 'amtWorkingLoans_per_sbjob']\n",
    "money_columns += ['mfi', 'median_home_value', 'mfi_err', 'median_home_value_err']\n",
    "\n",
    "\n",
    "\n",
    "adjusted_money_df = pandas.DataFrame(data=None, columns=['GEOID']+money_columns).set_index('GEOID')\n",
    "\n",
    "for thisyear in years:\n",
    "    thefactor = inflator_df.loc[thisyear]['thefactor']\n",
    "    adjusted_money_df_i = data_gdf.xs(thisyear, level='year')[money_columns].apply(lambda x: x*thefactor)\n",
    "    adjusted_money_df_i = adjusted_money_df_i.assign(year = thisyear).reset_index()\n",
    "    adjusted_money_df = pandas.concat((adjusted_money_df, adjusted_money_df_i), axis=0)\n",
    "\n",
    "for x in money_columns:\n",
    "    adjusted_money_df = adjusted_money_df.rename(columns={x: x+'_adj'})\n",
    "# adjusted_money_df = adjusted_money_df.reset.set_index(['GEOID', 'year'])\n",
    "    \n",
    "# # data_gdf = pandas.concat((data_gdf, adjusted_money_df), axis=1)\n",
    "#.set_index(['GEOID', 'year'])\n",
    "adjusted_money_df = adjusted_money_df.set_index(['GEOID', 'year'])\n",
    "data_gdf = pandas.concat((data_gdf,  adjusted_money_df), axis=1)\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('{0:}: Kept {1:,.0f} tract-years in {2:,.0f} minutes {3:.0f} seconds!'.format(city, len(data_gdf), np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "#data_gdf.reset_index().groupby(['GEOID', 'year']).size().sort_values()\n",
    "#adjusted_money_df.xs(2012, level='year')\n",
    "#adjusted_money_df.reset_index().groupby(['GEOID', 'year']).size().sort_values(ascending=False)\n",
    "\n",
    "#data_gdf.reset_index().groupby(['GEOID', 'year']).size().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add community statistical areas (Baltimore only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "if (city == 'Baltimore'):\n",
    "    tract_to_csa_df = pandas.read_csv(code_lookup_dir+'census_tract_to_neighborhood.csv')\n",
    "    tract_to_csa_df.loc[:, 'GEOID10'] = tract_to_csa_df['GEOID10'].apply(lambda x: '14000US'+str(x))\n",
    "    data_gdf = data_gdf.reset_index().merge(tract_to_csa_df[['GEOID10','TRACTCE10','NAME10','CSA2010']], left_on='GEOID', right_on='GEOID10').set_index(['GEOID', 'year'])\n",
    "\n",
    "# print('backing up...')\n",
    "# data_gdf_bk = data_gdf\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get only the columns we need, in the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "# print('getting from backup...')\n",
    "# data_gdf = data_gdf_bk\n",
    "print('renaming variables describing this tract\\'s state...')\n",
    "data_gdf = data_gdf.rename(columns = {'STATE': 'state_acs5', 'state': 'state_loans', 'STATEFP': 'state'})\n",
    "data_gdf = data_gdf.assign(city_name = city)\n",
    "\n",
    "print('selecting column order...')\n",
    "new_columns = []\n",
    "#new_columns += ['state', 'county', 'census_tract']\n",
    "new_columns += ['city_name', 'msa'] # city name, metropolitan statistical area (MSA) number\n",
    "new_columns += ['loan_type', 'action_taken_type'] # loan type (always 4 for business) and action taken (always 1 for origination)\n",
    "new_columns += ['income_group_code'] # income group code (1 to 14 or 101 to 105)\n",
    "new_columns += ['income_group', 'cra_level'] # income group and CRA level (human-readable)\n",
    "new_columns += ['nLoans1','amtLoans1','nLoans100k','amtLoans100k','nLoans250k','amtLoans250k','nLoansToSmallest','amtLoansToSmallest'] # number and amount of loans (directly from CRA data)\n",
    "new_columns += ['nLoans', 'amtLoans', 'avgSmallLoan', 'nWorkingLoans', 'amtWorkingLoans'] # calculated total and working loans\n",
    "new_columns += ['total_jobs', 'sb_jobs'] # total and small business jobs\n",
    "\n",
    "# all jobs columns under original names\n",
    "new_columns += ['C000','CA01','CA02','CA03','CE01','CE02','CE03']\n",
    "new_columns += ['CNS01','CNS02','CNS03','CNS04','CNS05','CNS06','CNS07','CNS08','CNS09','CNS10','CNS11','CNS12','CNS13','CNS14','CNS15','CNS16','CNS17','CNS18','CNS19','CNS20']\n",
    "new_columns += ['CR01','CR02','CR03','CR04','CR05','CR07','CT01','CT02','CD01','CD02','CD03','CD04','CS01','CS02']\n",
    "new_columns += ['CFA01','CFA02','CFA03','CFA04','CFA05','CFS01','CFS02','CFS03','CFS04','CFS05']\n",
    "\n",
    "# loans per job\n",
    "new_columns += ['nLoans1_per_totaljob','amtLoans1_per_totaljob','nLoans100k_per_totaljob','amtLoans100k_per_totaljob']\n",
    "new_columns += ['nLoans250k_per_totaljob','amtLoans250k_per_totaljob','nLoansToSmallest_per_totaljob','amtLoansToSmallest_per_totaljob']\n",
    "new_columns += ['nLoans_per_totaljob','amtLoans_per_totaljob','nWorkingLoans_per_totaljob','amtWorkingLoans_per_totaljob']\n",
    "new_columns += ['nLoans1_per_sbjob','amtLoans1_per_sbjob','nLoans100k_per_sbjob','amtLoans100k_per_sbjob','nLoans250k_per_sbjob','amtLoans250k_per_sbjob']\n",
    "new_columns += ['nLoansToSmallest_per_sbjob','amtLoansToSmallest_per_sbjob','nLoans_per_sbjob','amtLoans_per_sbjob','nWorkingLoans_per_sbjob','amtWorkingLoans_per_sbjob']\n",
    "\n",
    "# census count estimates\n",
    "new_columns += ['pop_total','pop_by_race_total','pop_white','pop_black']\n",
    "new_columns += ['total_households','white_householders','black_householders','female_householders']\n",
    "new_columns += ['total_housing_units','occupied_housing_units','vacant_housing_units']\n",
    "new_columns += ['educated','unemployed_16plus','poverty_past_12_months','mfi'] # high school graduates (ages 25+), unemployed, in poverty, MFI, travel time to work\n",
    "new_columns += ['owner_occ_housing_units','median_home_value','year_built', 'median_year_built'] # home\n",
    "new_columns += ['pop_25plus','labor_force_16plus','poverty_status_known']\n",
    "new_columns += ['household_type_family', 'household_type_nonfamily','household_type_total', 'tenure_rent', 'tenure_total']\n",
    "\n",
    "\n",
    "# census count errors\n",
    "new_columns += ['pop_total_err','pop_by_race_total_err','pop_white_err','pop_black_err']\n",
    "new_columns += ['total_households_err','white_householders_err','black_householders_err','female_householders_err']\n",
    "new_columns += ['total_housing_units_err','occupied_housing_units_err','vacant_housing_units_err']\n",
    "new_columns += ['educated_serr','unemployed_16plus_err','poverty_past_12_months_err','mfi_err'] # high school graduates (ages 25+), unemployed, in poverty, MFI, travel time to work\n",
    "new_columns += ['owner_occ_housing_units_err','median_home_value_err', 'median_year_built_err'] # home\n",
    "new_columns += ['pop_25plus_err','labor_force_16plus_err','poverty_status_known_err']\n",
    "new_columns += ['household_type_family_err', 'household_type_nonfamily_err', 'household_type_total_err', 'tenure_rent_err', 'tenure_total_err']\n",
    "\n",
    "# census percentages and their errors\n",
    "new_columns += ['pct_white','pct_black','pct_white_householders','pct_black_householders','pct_female_householders']\n",
    "new_columns += ['peducated','pct_unemployed','pct_poverty','pct_vacant', 'pct_household_family', 'pct_rent']\n",
    "new_columns += ['pct_white_serr','pct_black_serr','pct_white_householders_serr','pct_black_householders_serr','pct_female_householders_serr']\n",
    "new_columns += ['peducated_serr','pct_unemployed_serr','pct_poverty_serr','pct_vacant_serr', 'pct_household_family_serr', 'pct_rent_serr']\n",
    "\n",
    "# # inflation-adjusted money values\n",
    "new_columns += ['amtLoans1_adj','amtLoans100k_adj','amtLoans250k_adj','amtLoansToSmallest_adj']\n",
    "new_columns += ['avgSmallLoan_adj','amtLoans_adj','amtWorkingLoans_adj']\n",
    "new_columns += ['amtLoans1_per_totaljob_adj','amtLoans100k_per_totaljob_adj','amtLoans250k_per_totaljob_adj','amtLoansToSmallest_per_totaljob_adj']\n",
    "new_columns += ['amtLoans_per_totaljob_adj','amtWorkingLoans_per_totaljob_adj']\n",
    "new_columns += ['amtLoans1_per_sbjob_adj','amtLoans100k_per_sbjob_adj','amtLoans250k_per_sbjob_adj','amtLoansToSmallest_per_sbjob_adj','amtLoans_per_sbjob_adj','amtWorkingLoans_per_sbjob_adj']\n",
    "new_columns += ['mfi_adj','median_home_value_adj','mfi_err_adj','median_home_value_err_adj']\n",
    "\n",
    "## geographic information about the census tract\n",
    "#data_gdf = data_gdf.rename(columns={'STATEFP': 'state_tractdata'})\n",
    "new_columns += ['state', 'county', 'census_tract', 'NAME', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON']\n",
    "\n",
    "## more info from loans data\n",
    "#data_gdf = data_gdf.rename(columns =  {'Geography Name_loans_jobs': 'geography_name_loans_jobs'})\n",
    "new_columns += ['state_loans', 'split_county_indicator', 'population_classification', 'split_county_indicator', 'population_classification']#, 'geography_name_loans_jobs']\n",
    "\n",
    "## ACS5 raw census data under original variable names\n",
    "new_columns += [x for x in data_gdf.columns if (x not in new_columns) and (x[0] == 'B')]\n",
    "## ACS5 tract metadata\n",
    "#data_gdf = data_gdf.rename(columns =  {'Geography Name_acs5': 'geography_name_acs5'})\n",
    "new_columns += ['FILEID', 'FILETYPE', 'STUSAB', 'CHARITER', 'SEQUENCE', 'LOGRECNO', 'state_acs5',  'Name']#, 'geography_name_acs5']\n",
    "\n",
    "if (city == 'Baltimore'):\n",
    "    new_columns += ['GEOID10', 'TRACTCE10', 'NAME10', 'CSA2010']\n",
    "\n",
    "# # as traditional, geometry goes last\n",
    "# new_columns += ['geometry']\n",
    "\n",
    "# check that we got them all\n",
    "#pprint([x for x in data_gdf.columns if x not in new_columns])\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done!')\n",
    "#print('Done in {0:,.0f} minutes {1:,.0f} seconds!'.format(np.floor((e-s)/60), np.floor((e-s)%60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_gdf[new_columns[0:173]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('outputting without shapefile data...')\n",
    "s = time.time()\n",
    "city_name_for_output = city.replace(\".\",\"\").replace(\" \",\"_\").lower()\n",
    "outfilename_no_geo = '{0:}_{1:.0f}_{2:.0f}_no_geo.csv'.format(city_name_for_output, earliest_year, latest_year)\n",
    "\n",
    "data_gdf[new_columns].to_csv(output_data_dir+outfilename_no_geo)\n",
    "\n",
    "print('\\tWrote {0:,.0f} tract-years by {1:,.0f} columns (without geometry data) to:\\n\\t\\t{2:}'.format(\n",
    "    data_gdf[new_columns].shape[0], data_gdf[new_columns].shape[1], output_data_dir+outfilename_no_geo\n",
    "     ))\n",
    "print('\\n')\n",
    "\n",
    "print('outputting shapefile data separately...')\n",
    "outfilename_shapefiles_geo = '{0:}_{1:.0f}_{2:.0f}_shapefiles.shp'.format(city_name_for_output, earliest_year, latest_year)\n",
    "shapefile_writer_gdf = geopandas.GeoDataFrame(data=data_gdf['geometry'], columns=['geometry'], crs=data_gdf.crs, geometry='geometry')\n",
    "shapefile_writer_gdf.to_file(output_data_dir+outfilename_shapefiles_geo)\n",
    "\n",
    "print('\\tWrote {0:,.0f} tract-years by {1:,.0f} columns (SHAPEFILE DATA ONLY) to \\n\\t{2:}'.format(shapefile_writer_gdf.shape[0], shapefile_writer_gdf.shape[1], output_data_dir+outfilename_shapefiles_geo))\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "print('Done in {0:,.0f} minutes {1:,.0f} seconds!'.format(np.floor((e-s)/60), np.floor((e-s)%60)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "q = output_data_dir+'{0:}_{1:.0f}_{2:.0f}_no_geo.csv'.format(city_name_for_output, earliest_year, latest_year)\n",
    "qdf = pandas.read_csv(q)\n",
    "qdf = qdf.set_index(['GEOID', 'year'], drop=True)\n",
    "\n",
    "\n",
    "#r = '/home/idies/workspace/Storage/raddick/Baltimore/community_reinvestment_act/final_data/baltimore_2010_2019_shapefiles.shp'\n",
    "r = output_data_dir+'{0:}_{1:.0f}_{2:.0f}_shapefiles.shp'.format(city_name_for_output, earliest_year, latest_year)\n",
    "rgdf = geopandas.read_file(r)\n",
    "rgdf = rgdf.set_index(['GEOID', 'year'], drop=True)\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(data=pandas.concat((qdf, rgdf), axis=1), columns=pandas.concat((qdf, rgdf), axis=1).columns, crs=rgdf.crs, geometry='geometry')\n",
    "\n",
    "e = time.time()\n",
    "g = g + (e-s)\n",
    "\n",
    "print('DONE! Total time: {0:,.0f} minutes {1:,.0f} seconds!'.format(np.floor(g/60), np.floor(g%60)))\n",
    "\n",
    "gdf.plot(column='amtLoans1_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_loans_all_df = agg_loans_all_df.assign(\n",
    "#     GEOID = agg_loans_all_df.apply(lambda row: '14000US{0:02d}{1:03d}{2:04d}{3:02d}'.format(\n",
    "#         row['state'], \n",
    "#         row['county'], \n",
    "#         int(str(row['census_tract'])[0:str(row['census_tract']).find('.')]),\n",
    "#         int(str(row['census_tract'])[str(row['census_tract']).find('.')+1:])\n",
    "#     ), axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
